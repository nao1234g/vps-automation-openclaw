{
  "env": {
    "CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS": "1"
  },
  "permissions": {
    "allow": [
      "Bash(ls:*)",
      "Bash(wc:*)",
      "Bash(du:*)",
      "Bash(docker compose:*)",
      "Bash(ssh:*)",
      "Bash(curl:*)",
      "Bash(chmod:*)",
      "Bash(ssh-keygen:*)",
      "Bash(for user in appuser deploy admin openclaw nao)",
      "Bash(do echo \"Trying $user...\")",
      "Bash(done)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(git push:*)",
      "WebFetch(domain:163.44.124.123)",
      "Bash(ping:*)",
      "Bash(taskkill:*)",
      "Bash(scp:*)",
      "Bash(python3 -c \" import sys, json data = json.load\\(sys.stdin\\) gw = data.get\\(''gateway'', {}\\) # Remove auth section if ''auth'' in gw: del gw[''auth''] data[''gateway''] = gw print\\(json.dumps\\(data, indent=2, ensure_ascii=False\\)\\) \")",
      "WebFetch(domain:github.com)",
      "WebFetch(domain:docs.openclaw.ai)",
      "WebFetch(domain:medium.com)",
      "WebFetch(domain:o-mega.ai)",
      "WebFetch(domain:macaron.im)",
      "WebFetch(domain:raw.githubusercontent.com)",
      "WebFetch(domain:deepwiki.com)",
      "WebFetch(domain:openclawwiki.org)",
      "WebFetch(domain:www.answeroverflow.com)",
      "WebFetch(domain:www.npmjs.com)",
      "Bash(dir \"C:\\\\Users\\\\user\\\\.claude\")",
      "Bash(gh api:*)",
      "Bash(gh auth status:*)",
      "WebFetch(domain:support.claude.com)",
      "WebFetch(domain:news.ycombinator.com)",
      "WebFetch(domain:gist.github.com)",
      "Bash(claude auth status:*)",
      "Bash(dir:*)",
      "WebFetch(domain:whitepaper.virtuals.io)",
      "WebFetch(domain:community.n8n.io)",
      "WebFetch(domain:iam.slys.dev)",
      "Bash(rsync:*)",
      "WebFetch(domain:api.github.com)",
      "Bash(python:*)",
      "Bash(findstr:*)",
      "Bash(powershell -Command \"Select-String -Path ''c:\\\\Users\\\\user\\\\.claude\\\\projects\\\\c--Users-user-OneDrive--------vps-automation-openclaw\\\\*.jsonl'' -Pattern ''xai-i67c[a-zA-Z0-9_-]+F0rxvM8H'' | Select-Object -First 1 -ExpandProperty Line | Select-String -Pattern ''xai-[a-zA-Z0-9_-]+'' -AllMatches | ForEach-Object { $_Matches } | Select-Object -First 1 -ExpandProperty Value\")",
      "Bash(powershell -Command \"$content = Get-Content ''c:\\\\Users\\\\user\\\\.claude\\\\projects\\\\c--Users-user-OneDrive--------vps-automation-openclaw\\\\a12ea597-4f13-4bc3-971a-d483e84067aa.jsonl'' -Raw; if \\($content -match ''\\(xai-i67c[a-zA-Z0-9_-]+F0rxvM8H\\)''\\) { $matches[1] } else { ''Not found'' }\")",
      "Bash(pip install:*)",
      "Bash(node -e:*)",
      "WebFetch(domain:twitterapi.io)",
      "WebFetch(domain:docs.x.ai)",
      "Bash(bash:*)",
      "WebFetch(domain:community.latenode.com)",
      "WebFetch(domain:docs.python-telegram-bot.org)",
      "Bash(powershell -Command \"Get-Process ssh -ErrorAction SilentlyContinue | Stop-Process -Force; Start-Sleep -Seconds 2\")",
      "Bash(start ssh -L 3389:localhost:3389 -N root@163.44.124.123)",
      "Bash(timeout /t 3 /nobreak)",
      "Bash(powershell -Command \"Start-Sleep -Seconds 3; Get-NetTCPConnection -LocalPort 3389 -State Listen -ErrorAction SilentlyContinue | Select-Object LocalAddress, LocalPort, State\")",
      "Bash(powershell -Command \"Get-NetTCPConnection -LocalPort 3389 -ErrorAction SilentlyContinue | Format-Table LocalAddress, LocalPort, State, OwningProcess; Get-Process -Id \\(Get-NetTCPConnection -LocalPort 3389 -ErrorAction SilentlyContinue\\).OwningProcess -ErrorAction SilentlyContinue | Format-Table ProcessName, Id\")",
      "Bash(powershell.exe -Command \"Get-Process ssh -ErrorAction SilentlyContinue | Stop-Process -Force\")",
      "Bash(start:*)",
      "Bash(powershell.exe -Command \"Start-Sleep -Seconds 2; Start-Process ''C:\\\\Users\\\\user\\\\Desktop\\\\vps-xrdp.rdp''\")",
      "Bash(powershell.exe -Command \"Stop-Process -Name mstsc -Force -ErrorAction SilentlyContinue\")",
      "Bash(powershell.exe -Command \"mstsc.exe\")",
      "Bash(powershell.exe -Command \"Stop-Process -Name mstsc -Force -ErrorAction SilentlyContinue; Start-Sleep 1; mstsc.exe /v:127.0.0.1:3389\")",
      "Bash(powershell.exe -Command \"mstsc.exe /v:127.0.0.1:3389\")",
      "Bash(powershell.exe -Command \"Get-Process ssh | Select-Object Id,ProcessName,StartTime,CommandLine | Format-List\")",
      "Bash(powershell.exe:*)",
      "Read(//tmp/**)",
      "Bash(python3:*)",
      "Bash(py:*)",
      "Bash(/tmp/x-intel-upload/scripts/x_actions.sh << 'XEOF'\n#!/usr/bin/env bash\nset -e\n\n###############################################################################\n# x_actions.sh - X/Twitter Browser Automation via xdotool\n#\n# Usage:\n#   ./x_actions.sh follow @username\n#   ./x_actions.sh search \"keyword\"\n#   ./x_actions.sh retweet \"https://x.com/user/status/123\"\n#\n# Prerequisites: xdotool, scrot, xclip, imagemagick, tesseract-ocr\n# Chrome must be running on DISPLAY=:10 logged into @aisaintel\n###############################################################################\n\nexport DISPLAY=:10\n\nBASE_DIR=\"/opt/shared/x-intelligence\"\nLOG_FILE=\"${BASE_DIR}/logs/actions.log\"\nSCREENSHOT_DIR=\"${BASE_DIR}/data/screenshots\"\n\nmkdir -p \"${SCREENSHOT_DIR}\" \"$\\(dirname \"${LOG_FILE}\"\\)\"\n\n# ---------------------------------------------------------------------------\n# Helpers\n# ---------------------------------------------------------------------------\n\nlog\\(\\) {\n    local level=\"$1\"; shift\n    local msg=\"$*\"\n    local ts\n    ts=\"$\\(date '+%Y-%m-%d %H:%M:%S'\\)\"\n    echo \"[${ts}] [${level}] ${msg}\" | tee -a \"${LOG_FILE}\"\n}\n\ntake_screenshot\\(\\) {\n    local label=\"$1\"\n    local ts\n    ts=\"$\\(date '+%Y%m%d_%H%M%S'\\)\"\n    local path=\"${SCREENSHOT_DIR}/${ts}_${label}.png\"\n    scrot -D \"${DISPLAY}\" \"${path}\" 2>/dev/null || scrot \"${path}\" 2>/dev/null || true\n    log \"INFO\" \"Screenshot saved: ${path}\"\n    echo \"${path}\"\n}\n\nnavigate_to_url\\(\\) {\n    local url=\"$1\"\n    log \"INFO\" \"Navigating to: ${url}\"\n\n    # Focus address bar\n    xdotool key ctrl+l\n    sleep 0.5\n\n    # Clear current URL\n    xdotool key ctrl+a\n    sleep 0.2\n\n    # Type URL via clipboard for reliability \\(handles special chars\\)\n    echo -n \"${url}\" | xclip -selection clipboard\n    xdotool key ctrl+v\n    sleep 0.3\n    xdotool key Return\n    sleep 5\n\n    log \"INFO\" \"Navigation complete, waited 5s for page load\"\n}\n\n# ---------------------------------------------------------------------------\n# Actions\n# ---------------------------------------------------------------------------\n\naction_follow\\(\\) {\n    local username=\"$1\"\n    # Strip leading @ if present\n    username=\"${username#@}\"\n\n    if [[ -z \"${username}\" ]]; then\n        log \"ERROR\" \"follow: username is required\"\n        exit 1\n    fi\n\n    log \"INFO\" \"=== FOLLOW @${username} ===\"\n\n    navigate_to_url \"https://x.com/${username}\"\n\n    # Extra wait for profile page to fully render\n    sleep 2\n\n    # Take a screenshot before clicking\n    take_screenshot \"follow_before_${username}\"\n\n    # Attempt OCR to locate the Follow button\n    local ss_path\n    ss_path=\"$\\(take_screenshot \"follow_ocr_${username}\"\\)\"\n\n    local follow_coords=\"\"\n    if command -v tesseract &>/dev/null; then\n        local ocr_img=\"/tmp/ocr_follow_${username}.png\"\n        convert \"${ss_path}\" -colorspace Gray \"${ocr_img}\" 2>/dev/null || true\n\n        local ocr_tsv=\"/tmp/ocr_follow_${username}\"\n        tesseract \"${ocr_img}\" \"${ocr_tsv}\" -l eng tsv 2>/dev/null || true\n\n        if [[ -f \"${ocr_tsv}.tsv\" ]]; then\n            follow_coords=$\\(awk -F'\\\\t' '{ if \\($12 == \"Follow\"\\) { x = $7 + \\($9 / 2\\); y = $8 + \\($10 / 2\\); print int\\(x\\), int\\(y\\); exit } }' \"${ocr_tsv}.tsv\" 2>/dev/null || true\\)\n            rm -f \"${ocr_img}\" \"${ocr_tsv}.tsv\" 2>/dev/null || true\n        fi\n    fi\n\n    if [[ -n \"${follow_coords}\" ]]; then\n        local fx fy\n        fx=\"$\\(echo \"${follow_coords}\" | awk '{print $1}'\\)\"\n        fy=\"$\\(echo \"${follow_coords}\" | awk '{print $2}'\\)\"\n        log \"INFO\" \"Found Follow button via OCR at \\(${fx}, ${fy}\\)\"\n        xdotool mousemove \"${fx}\" \"${fy}\"\n        sleep 0.3\n        xdotool click 1\n    else\n        log \"INFO\" \"OCR did not find Follow button, using fallback position \\(1050, 420\\)\"\n        xdotool mousemove 1050 420\n        sleep 0.3\n        xdotool click 1\n    fi\n\n    sleep 2\n\n    take_screenshot \"follow_after_${username}\"\n    log \"INFO\" \"=== FOLLOW @${username} COMPLETE ===\"\n}\n\naction_search\\(\\) {\n    local keyword=\"$1\"\n\n    if [[ -z \"${keyword}\" ]]; then\n        log \"ERROR\" \"search: keyword is required\"\n        exit 1\n    fi\n\n    log \"INFO\" \"=== SEARCH: ${keyword} ===\"\n\n    local encoded\n    encoded=\"$\\(python3 -c \"import urllib.parse, sys; print\\(urllib.parse.quote\\(sys.argv[1]\\)\\)\" \"${keyword}\" 2>/dev/null || echo \"${keyword}\"\\)\"\n\n    navigate_to_url \"https://x.com/search?q=${encoded}&f=top\"\n\n    sleep 3\n\n    take_screenshot \"search_${keyword// /_}\"\n    log \"INFO\" \"=== SEARCH COMPLETE ===\"\n}\n\naction_retweet\\(\\) {\n    local url=\"$1\"\n\n    if [[ -z \"${url}\" ]]; then\n        log \"ERROR\" \"retweet: tweet URL is required\"\n        exit 1\n    fi\n\n    log \"INFO\" \"=== RETWEET: ${url} ===\"\n\n    navigate_to_url \"${url}\"\n\n    sleep 3\n\n    take_screenshot \"retweet_before\"\n\n    local retweet_x=640\n    local retweet_y=555\n\n    local ss_path\n    ss_path=\"$\\(take_screenshot \"retweet_ocr\"\\)\"\n\n    if command -v tesseract &>/dev/null; then\n        local ocr_img=\"/tmp/ocr_rt.png\"\n        convert \"${ss_path}\" -colorspace Gray \"${ocr_img}\" 2>/dev/null || true\n        local ocr_tsv=\"/tmp/ocr_rt\"\n        tesseract \"${ocr_img}\" \"${ocr_tsv}\" -l eng tsv 2>/dev/null || true\n\n        if [[ -f \"${ocr_tsv}.tsv\" ]]; then\n            local rt_coords\n            rt_coords=$\\(awk -F'\\\\t' '{ if \\($12 ~ /^[Rr]epost/ || $12 ~ /^[Rr]etweet/\\) { x = $7 + \\($9 / 2\\); y = $8 + \\($10 / 2\\); print int\\(x\\), int\\(y\\); exit } }' \"${ocr_tsv}.tsv\" 2>/dev/null || true\\)\n\n            if [[ -n \"${rt_coords}\" ]]; then\n                retweet_x=\"$\\(echo \"${rt_coords}\" | awk '{print $1}'\\)\"\n                retweet_y=\"$\\(echo \"${rt_coords}\" | awk '{print $2}'\\)\"\n                log \"INFO\" \"Found retweet area via OCR at \\(${retweet_x}, ${retweet_y}\\)\"\n            fi\n            rm -f \"${ocr_img}\" \"${ocr_tsv}.tsv\" 2>/dev/null || true\n        fi\n    fi\n\n    log \"INFO\" \"Clicking retweet button at \\(${retweet_x}, ${retweet_y}\\)\"\n    xdotool mousemove \"${retweet_x}\" \"${retweet_y}\"\n    sleep 0.5\n    xdotool click 1\n    sleep 2\n\n    # Popup menu with \"Repost\" -- typically right below the button\n    local repost_y=$\\(\\(retweet_y + 50\\)\\)\n    log \"INFO\" \"Clicking Repost in menu at \\(${retweet_x}, ${repost_y}\\)\"\n    xdotool mousemove \"${retweet_x}\" \"${repost_y}\"\n    sleep 0.3\n    xdotool click 1\n    sleep 2\n\n    take_screenshot \"retweet_after\"\n    log \"INFO\" \"=== RETWEET COMPLETE ===\"\n}\n\n# ---------------------------------------------------------------------------\n# Main\n# ---------------------------------------------------------------------------\n\nif [[ $# -lt 2 ]]; then\n    echo \"Usage: $0 <action> <argument>\"\n    echo \"  $0 follow @username\"\n    echo \"  $0 search \\\\\"keyword\\\\\"\"\n    echo \"  $0 retweet \\\\\"https://x.com/.../status/...\\\\\"\"\n    exit 1\nfi\n\nACTION=\"$1\"\nARG=\"$2\"\n\ncase \"${ACTION}\" in\n    follow\\)\n        action_follow \"${ARG}\"\n        ;;\n    search\\)\n        action_search \"${ARG}\"\n        ;;\n    retweet\\)\n        action_retweet \"${ARG}\"\n        ;;\n    *\\)\n        log \"ERROR\" \"Unknown action: ${ACTION}\"\n        echo \"Unknown action: ${ACTION}. Supported: follow, search, retweet\"\n        exit 1\n        ;;\nesac\nXEOF)",
      "Bash(/tmp/x-intel-upload/scripts/execute_actions.sh:*)",
      "Bash(/tmp/x-intel-upload/scripts/write_curate_on_vps.py << 'WEOF'\n#!/usr/bin/env python3\n\"\"\"Writer script - creates curate.py on the VPS filesystem.\"\"\"\nimport os\n\ntarget = \"/opt/shared/x-intelligence/scripts/curate.py\"\n\ncode = '''#!/usr/bin/env python3\n# curate.py - X Intelligence Curation Engine\n#\n# Reads discovery JSON files, uses xAI Grok API to score/prioritize\n# accounts to follow, tweets to retweet, content for Neo articles.\n#\n# Usage:\n#     python3 curate.py\n#     XAI_API_KEY=... python3 curate.py\n\nimport json\nimport os\nimport sys\nimport glob\nimport logging\nfrom datetime import datetime, timezone, timedelta\nfrom pathlib import Path\n\ntry:\n    import requests\nexcept ImportError:\n    import subprocess\n    subprocess.check_call\\([sys.executable, \"-m\", \"pip\", \"install\", \"requests\"]\\)\n    import requests\n\n# Configuration\nBASE_DIR = Path\\(\"/opt/shared/x-intelligence\"\\)\nDISCOVERY_DIR = BASE_DIR / \"data\" / \"discovery\"\nCURATION_DIR = BASE_DIR / \"data\" / \"curation\"\nLOGS_DIR = BASE_DIR / \"logs\"\nDASHBOARD_PATH = BASE_DIR / \"dashboard.md\"\n\nXAI_API_KEY = os.environ.get\\(\n    \"XAI_API_KEY\",\n    \"REDACTED_XAI_KEY\",\n\\)\nXAI_API_URL = \"https://api.x.ai/v1/chat/completions\"\nXAI_MODEL = \"grok-4-1-fast-non-reasoning\"\n\nJST = timezone\\(timedelta\\(hours=9\\)\\)\nTODAY = datetime.now\\(JST\\).strftime\\(\"%Y-%m-%d\"\\)\n\nCURATION_DIR.mkdir\\(parents=True, exist_ok=True\\)\nLOGS_DIR.mkdir\\(parents=True, exist_ok=True\\)\n\nlogging.basicConfig\\(\n    level=logging.INFO,\n    format=\"[%\\(asctime\\)s] [%\\(levelname\\)s] %\\(message\\)s\",\n    handlers=[\n        logging.StreamHandler\\(\\),\n        logging.FileHandler\\(LOGS_DIR / \"curate.log\"\\),\n    ],\n\\)\nlog = logging.getLogger\\(\"curate\"\\)\n\n\ndef load_today_discoveries\\(\\):\n    \"\"\"Load all discovery JSON files from today.\"\"\"\n    pattern = str\\(DISCOVERY_DIR / \\(TODAY + \"*.json\"\\)\\)\n    files = sorted\\(glob.glob\\(pattern\\)\\)\n    if not files:\n        pattern = str\\(DISCOVERY_DIR / \"*.json\"\\)\n        files = sorted\\(glob.glob\\(pattern\\)\\)\n        if files:\n            log.info\\(\"No files matching today \\(%s\\), using all %d discovery files\", TODAY, len\\(files\\)\\)\n    discoveries = []\n    for fpath in files:\n        try:\n            with open\\(fpath, \"r\", encoding=\"utf-8\"\\) as f:\n                data = json.load\\(f\\)\n                if isinstance\\(data, list\\):\n                    discoveries.extend\\(data\\)\n                else:\n                    discoveries.append\\(data\\)\n            count = len\\(data\\) if isinstance\\(data, list\\) else 1\n            log.info\\(\"Loaded discovery file: %s \\(%d items\\)\", fpath, count\\)\n        except \\(json.JSONDecodeError, IOError\\) as e:\n            log.warning\\(\"Failed to load %s: %s\", fpath, e\\)\n    return discoveries\n\n\ndef call_grok\\(prompt, max_tokens=2000\\):\n    \"\"\"Call xAI Grok API and return the response text.\"\"\"\n    headers = {\n        \"Authorization\": \"Bearer \" + XAI_API_KEY,\n        \"Content-Type\": \"application/json\",\n    }\n    payload = {\n        \"model\": XAI_MODEL,\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \\(\n                    \"You are a crypto/Web3 intelligence analyst for AISA \"\n                    \"\\(Asia Intelligence Signal Agent\\). You curate X/Twitter content \"\n                    \"about Asian crypto markets, regulations, DeFi, and macro trends. \"\n                    \"You must respond ONLY with valid JSON, no markdown fences.\"\n                \\),\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        \"max_tokens\": max_tokens,\n        \"temperature\": 0.3,\n    }\n    try:\n        resp = requests.post\\(XAI_API_URL, headers=headers, json=payload, timeout=60\\)\n        resp.raise_for_status\\(\\)\n        data = resp.json\\(\\)\n        return data[\"choices\"][0][\"message\"][\"content\"].strip\\(\\)\n    except requests.RequestException as e:\n        log.error\\(\"Grok API call failed: %s\", e\\)\n        if hasattr\\(e, \"response\"\\) and e.response is not None:\n            log.error\\(\"Response body: %s\", e.response.text[:500]\\)\n        raise\n\n\ndef parse_json_response\\(text\\):\n    \"\"\"Parse JSON from Grok response, stripping markdown fences if present.\"\"\"\n    text = text.strip\\(\\)\n    if text.startswith\\(\"```\"\\):\n        text_lines = text.split\\(\"\\\\\\\\n\"\\)\n        text_lines = [l for l in text_lines if not l.strip\\(\\).startswith\\(\"```\"\\)]\n        text = \"\\\\\\\\n\".join\\(text_lines\\)\n    return json.loads\\(text\\)\n\n\nCURATION_PROMPT_TEMPLATE = \"\"\"Analyze the following X/Twitter discovery data and produce a curation report.\n\nDISCOVERY DATA:\n{discovery_text}\n\nINSTRUCTIONS:\n1. Score each discovered account 1-10 for relevance to Asian crypto intelligence.\n   - Include accounts scoring 8+ in \"follow_accounts\" \\(return @handle strings\\).\n   - Prioritize: regulatory bodies, major exchanges, analysts, protocol teams.\n\n2. Score each discovered tweet/post 1-10 for retweet worthiness.\n   - Include tweets scoring 9+ in \"retweet_urls\" \\(return URL strings\\).\n   - Must be genuinely valuable: breaking news, deep analysis, regulatory updates.\n   - Do NOT retweet price predictions, shilling, or low-quality content.\n\n3. Identify topics suitable for Neo long-form article writing.\n   - Group related discoveries into article topics.\n   - Include source URLs and a brief topic description.\n\nRespond with ONLY valid JSON \\(no markdown fences\\):\n{json_template}\n\"\"\"\n\n\ndef curate\\(discoveries\\):\n    \"\"\"Send discoveries to Grok for scoring and curation.\"\"\"\n    if not discoveries:\n        log.warning\\(\"No discoveries to curate. Generating empty curation.\"\\)\n        return {\"date\": TODAY, \"follow_accounts\": [], \"retweet_urls\": [], \"article_material\": []}\n    truncated = discoveries[:30]\n    discovery_text = json.dumps\\(truncated, indent=2, ensure_ascii=False, default=str\\)\n    json_template = json.dumps\\({\n        \"date\": TODAY,\n        \"follow_accounts\": [\"@handle1\", \"@handle2\"],\n        \"retweet_urls\": [\"https://x.com/...\"],\n        \"article_material\": [{\"topic\": \"Topic\", \"summary\": \"Brief desc\", \"sources\": [\"url1\"]}]\n    }, indent=4\\)\n    prompt = CURATION_PROMPT_TEMPLATE.format\\(discovery_text=discovery_text, json_template=json_template\\)\n    log.info\\(\"Sending %d discoveries to Grok for curation...\", len\\(truncated\\)\\)\n    raw_response = call_grok\\(prompt\\)\n    log.info\\(\"Grok response received, parsing...\"\\)\n    try:\n        result = parse_json_response\\(raw_response\\)\n    except json.JSONDecodeError as e:\n        log.error\\(\"Failed to parse Grok response as JSON: %s\", e\\)\n        log.error\\(\"Raw response: %s\", raw_response[:500]\\)\n        result = {\"date\": TODAY, \"follow_accounts\": [], \"retweet_urls\": [], \"article_material\": [], \"_raw_response\": raw_response[:1000]}\n    result.setdefault\\(\"date\", TODAY\\)\n    result.setdefault\\(\"follow_accounts\", []\\)\n    result.setdefault\\(\"retweet_urls\", []\\)\n    result.setdefault\\(\"article_material\", []\\)\n    return result\n\n\ndef save_curation\\(curation\\):\n    \"\"\"Save the curation result to a JSON file.\"\"\"\n    output_path = CURATION_DIR / \\(TODAY + \"_approved.json\"\\)\n    with open\\(output_path, \"w\", encoding=\"utf-8\"\\) as f:\n        json.dump\\(curation, f, indent=2, ensure_ascii=False\\)\n    log.info\\(\"Curation saved to: %s\", output_path\\)\n    return output_path\n\n\ndef generate_dashboard\\(curation\\):\n    \"\"\"Generate a markdown dashboard for Neo to read.\"\"\"\n    follows = curation.get\\(\"follow_accounts\", []\\)\n    retweets = curation.get\\(\"retweet_urls\", []\\)\n    articles = curation.get\\(\"article_material\", []\\)\n    now_str = datetime.now\\(JST\\).strftime\\(\"%Y-%m-%d %H:%M:%S JST\"\\)\n    md = []\n    md.append\\(\"# X Intelligence Dashboard\"\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"**Date**: \" + TODAY\\)\n    md.append\\(\"**Generated**: \" + now_str\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"---\"\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"## Accounts to Follow \\(\" + str\\(len\\(follows\\)\\) + \"\\)\"\\)\n    md.append\\(\"\"\\)\n    if follows:\n        for handle in follows:\n            h = handle.lstrip\\(\"@\"\\)\n            md.append\\(\"- [ ] \" + handle + \" -- https://x.com/\" + h\\)\n    else:\n        md.append\\(\"_No accounts recommended today._\"\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"## Tweets to Repost \\(\" + str\\(len\\(retweets\\)\\) + \"\\)\"\\)\n    md.append\\(\"\"\\)\n    if retweets:\n        for url in retweets:\n            md.append\\(\"- [ ] \" + url\\)\n    else:\n        md.append\\(\"_No tweets recommended for repost today._\"\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"## Article Material \\(\" + str\\(len\\(articles\\)\\) + \"\\)\"\\)\n    md.append\\(\"\"\\)\n    if articles:\n        for i, item in enumerate\\(articles, 1\\):\n            topic = item.get\\(\"topic\", \"Untitled\"\\)\n            summary = item.get\\(\"summary\", \"\"\\)\n            sources = item.get\\(\"sources\", []\\)\n            md.append\\(\"### \" + str\\(i\\) + \". \" + topic\\)\n            if summary:\n                md.append\\(\"\"\\)\n                md.append\\(summary\\)\n            if sources:\n                md.append\\(\"\"\\)\n                md.append\\(\"**Sources**:\"\\)\n                for src in sources:\n                    md.append\\(\"- \" + src\\)\n            md.append\\(\"\"\\)\n    else:\n        md.append\\(\"_No article material identified today._\"\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"---\"\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"_Auto-generated by X Intelligence Curation Engine_\"\\)\n    with open\\(DASHBOARD_PATH, \"w\", encoding=\"utf-8\"\\) as f:\n        f.write\\(\"\\\\\\\\n\".join\\(md\\)\\)\n    log.info\\(\"Dashboard written to: %s\", DASHBOARD_PATH\\)\n\n\ndef main\\(\\):\n    log.info\\(\"=== X Intelligence Curation -- %s ===\", TODAY\\)\n    discoveries = load_today_discoveries\\(\\)\n    log.info\\(\"Loaded %d total discovery items\", len\\(discoveries\\)\\)\n    curation = curate\\(discoveries\\)\n    follows = len\\(curation.get\\(\"follow_accounts\", []\\)\\)\n    retweets = len\\(curation.get\\(\"retweet_urls\", []\\)\\)\n    articles = len\\(curation.get\\(\"article_material\", []\\)\\)\n    log.info\\(\"Curation result: %d follows, %d retweets, %d article topics\", follows, retweets, articles\\)\n    save_curation\\(curation\\)\n    generate_dashboard\\(curation\\)\n    log.info\\(\"=== Curation complete ===\"\\)\n\n\nif __name__ == \"__main__\":\n    main\\(\\)\n'''\n\nos.makedirs\\(os.path.dirname\\(target\\), exist_ok=True\\)\nwith open\\(target, \"w\"\\) as f:\n    f.write\\(code\\)\nos.chmod\\(target, 0o755\\)\nprint\\(f\"Written {target}, size: {os.path.getsize\\(target\\)} bytes\"\\)\nWEOF)",
      "Bash(/tmp/x-intel-upload/scripts/write_curate_on_vps.py:*)",
      "Bash(/tmp/curate_writer.py << 'PYWRITEREOF'\n#!/usr/bin/env python3\n\"\"\"Writer that creates curate.py - run this on the VPS.\"\"\"\nimport os, textwrap\n\ntarget = \"/opt/shared/x-intelligence/scripts/curate.py\"\nos.makedirs\\(os.path.dirname\\(target\\), exist_ok=True\\)\n\ncode = textwrap.dedent\\(\"\"\"\\\\\n    #!/usr/bin/env python3\n    # curate.py - X Intelligence Curation Engine\n    #\n    # Reads discovery JSON files, uses xAI Grok API to score/prioritize\n    # accounts to follow, tweets to retweet, content for Neo articles.\n    #\n    # Usage:\n    #     python3 curate.py\n    #     XAI_API_KEY=... python3 curate.py\n\n    import json\n    import os\n    import sys\n    import glob\n    import logging\n    from datetime import datetime, timezone, timedelta\n    from pathlib import Path\n\n    try:\n        import requests\n    except ImportError:\n        import subprocess\n        subprocess.check_call\\([sys.executable, \"-m\", \"pip\", \"install\", \"requests\"]\\)\n        import requests\n\n    # Configuration\n    BASE_DIR = Path\\(\"/opt/shared/x-intelligence\"\\)\n    DISCOVERY_DIR = BASE_DIR / \"data\" / \"discovery\"\n    CURATION_DIR = BASE_DIR / \"data\" / \"curation\"\n    LOGS_DIR = BASE_DIR / \"logs\"\n    DASHBOARD_PATH = BASE_DIR / \"dashboard.md\"\n\n    XAI_API_KEY = os.environ.get\\(\n        \"XAI_API_KEY\",\n        \"REDACTED_XAI_KEY\",\n    \\)\n    XAI_API_URL = \"https://api.x.ai/v1/chat/completions\"\n    XAI_MODEL = \"grok-4-1-fast-non-reasoning\"\n\n    JST = timezone\\(timedelta\\(hours=9\\)\\)\n    TODAY = datetime.now\\(JST\\).strftime\\(\"%Y-%m-%d\"\\)\n\n    CURATION_DIR.mkdir\\(parents=True, exist_ok=True\\)\n    LOGS_DIR.mkdir\\(parents=True, exist_ok=True\\)\n\n    logging.basicConfig\\(\n        level=logging.INFO,\n        format=\"[%\\(asctime\\)s] [%\\(levelname\\)s] %\\(message\\)s\",\n        handlers=[\n            logging.StreamHandler\\(\\),\n            logging.FileHandler\\(LOGS_DIR / \"curate.log\"\\),\n        ],\n    \\)\n    log = logging.getLogger\\(\"curate\"\\)\n\n\n    def load_today_discoveries\\(\\):\n        pattern = str\\(DISCOVERY_DIR / \\(TODAY + \"*.json\"\\)\\)\n        files = sorted\\(glob.glob\\(pattern\\)\\)\n        if not files:\n            pattern = str\\(DISCOVERY_DIR / \"*.json\"\\)\n            files = sorted\\(glob.glob\\(pattern\\)\\)\n            if files:\n                log.info\\(\"No files matching today \\(%s\\), using all %d discovery files\", TODAY, len\\(files\\)\\)\n        discoveries = []\n        for fpath in files:\n            try:\n                with open\\(fpath, \"r\", encoding=\"utf-8\"\\) as f:\n                    data = json.load\\(f\\)\n                    if isinstance\\(data, list\\):\n                        discoveries.extend\\(data\\)\n                    else:\n                        discoveries.append\\(data\\)\n                count = len\\(data\\) if isinstance\\(data, list\\) else 1\n                log.info\\(\"Loaded discovery file: %s \\(%d items\\)\", fpath, count\\)\n            except \\(json.JSONDecodeError, IOError\\) as e:\n                log.warning\\(\"Failed to load %s: %s\", fpath, e\\)\n        return discoveries\n\n\n    def call_grok\\(prompt, max_tokens=2000\\):\n        headers = {\n            \"Authorization\": \"Bearer \" + XAI_API_KEY,\n            \"Content-Type\": \"application/json\",\n        }\n        payload = {\n            \"model\": XAI_MODEL,\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \\(\n                        \"You are a crypto/Web3 intelligence analyst for AISA \"\n                        \"\\(Asia Intelligence Signal Agent\\). You curate X/Twitter content \"\n                        \"about Asian crypto markets, regulations, DeFi, and macro trends. \"\n                        \"You must respond ONLY with valid JSON, no markdown fences.\"\n                    \\),\n                },\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            \"max_tokens\": max_tokens,\n            \"temperature\": 0.3,\n        }\n        try:\n            resp = requests.post\\(XAI_API_URL, headers=headers, json=payload, timeout=60\\)\n            resp.raise_for_status\\(\\)\n            data = resp.json\\(\\)\n            return data[\"choices\"][0][\"message\"][\"content\"].strip\\(\\)\n        except requests.RequestException as e:\n            log.error\\(\"Grok API call failed: %s\", e\\)\n            if hasattr\\(e, \"response\"\\) and e.response is not None:\n                log.error\\(\"Response body: %s\", e.response.text[:500]\\)\n            raise\n\n\n    def parse_json_response\\(text\\):\n        text = text.strip\\(\\)\n        if text.startswith\\(\"```\"\\):\n            text_lines = text.split\\(chr\\(10\\)\\)\n            text_lines = [l for l in text_lines if not l.strip\\(\\).startswith\\(\"```\"\\)]\n            text = chr\\(10\\).join\\(text_lines\\)\n        return json.loads\\(text\\)\n\n\n    CURATION_PROMPT = \\(\n        \"Analyze the following X/Twitter discovery data and produce a curation report.\\\\\\\\n\"\n        \"\\\\\\\\n\"\n        \"DISCOVERY DATA:\\\\\\\\n\"\n        \"{discovery_text}\\\\\\\\n\"\n        \"\\\\\\\\n\"\n        \"INSTRUCTIONS:\\\\\\\\n\"\n        \"1. Score each discovered account 1-10 for relevance to Asian crypto intelligence.\\\\\\\\n\"\n        \"   - Include accounts scoring 8+ in follow_accounts \\(return @handle strings\\).\\\\\\\\n\"\n        \"   - Prioritize: regulatory bodies, major exchanges, analysts, protocol teams.\\\\\\\\n\"\n        \"\\\\\\\\n\"\n        \"2. Score each discovered tweet/post 1-10 for retweet worthiness.\\\\\\\\n\"\n        \"   - Include tweets scoring 9+ in retweet_urls \\(return URL strings\\).\\\\\\\\n\"\n        \"   - Must be genuinely valuable: breaking news, deep analysis, regulatory updates.\\\\\\\\n\"\n        \"   - Do NOT retweet price predictions, shilling, or low-quality content.\\\\\\\\n\"\n        \"\\\\\\\\n\"\n        \"3. Identify topics suitable for Neo long-form article writing.\\\\\\\\n\"\n        \"   - Group related discoveries into article topics.\\\\\\\\n\"\n        \"   - Include source URLs and a brief topic description.\\\\\\\\n\"\n        \"\\\\\\\\n\"\n        \"Respond with ONLY valid JSON \\(no markdown fences\\):\\\\\\\\n\"\n        \"{json_template}\\\\\\\\n\"\n    \\)\n\n\n    def curate\\(discoveries\\):\n        if not discoveries:\n            log.warning\\(\"No discoveries to curate. Generating empty curation.\"\\)\n            return {\"date\": TODAY, \"follow_accounts\": [], \"retweet_urls\": [], \"article_material\": []}\n        truncated = discoveries[:30]\n        discovery_text = json.dumps\\(truncated, indent=2, ensure_ascii=False, default=str\\)\n        json_template = json.dumps\\({\n            \"date\": TODAY,\n            \"follow_accounts\": [\"@handle1\", \"@handle2\"],\n            \"retweet_urls\": [\"https://x.com/...\"],\n            \"article_material\": [{\"topic\": \"Topic\", \"summary\": \"Brief desc\", \"sources\": [\"url1\"]}]\n        }, indent=4\\)\n        prompt = CURATION_PROMPT.format\\(discovery_text=discovery_text, json_template=json_template\\)\n        log.info\\(\"Sending %d discoveries to Grok for curation...\", len\\(truncated\\)\\)\n        raw_response = call_grok\\(prompt\\)\n        log.info\\(\"Grok response received, parsing...\"\\)\n        try:\n            result = parse_json_response\\(raw_response\\)\n        except json.JSONDecodeError as e:\n            log.error\\(\"Failed to parse Grok response as JSON: %s\", e\\)\n            log.error\\(\"Raw response: %s\", raw_response[:500]\\)\n            result = {\n                \"date\": TODAY, \"follow_accounts\": [], \"retweet_urls\": [],\n                \"article_material\": [], \"_raw_response\": raw_response[:1000]\n            }\n        result.setdefault\\(\"date\", TODAY\\)\n        result.setdefault\\(\"follow_accounts\", []\\)\n        result.setdefault\\(\"retweet_urls\", []\\)\n        result.setdefault\\(\"article_material\", []\\)\n        return result\n\n\n    def save_curation\\(curation\\):\n        output_path = CURATION_DIR / \\(TODAY + \"_approved.json\"\\)\n        with open\\(output_path, \"w\", encoding=\"utf-8\"\\) as f:\n            json.dump\\(curation, f, indent=2, ensure_ascii=False\\)\n        log.info\\(\"Curation saved to: %s\", output_path\\)\n        return output_path\n\n\n    def generate_dashboard\\(curation\\):\n        follows = curation.get\\(\"follow_accounts\", []\\)\n        retweets = curation.get\\(\"retweet_urls\", []\\)\n        articles = curation.get\\(\"article_material\", []\\)\n        now_str = datetime.now\\(JST\\).strftime\\(\"%Y-%m-%d %H:%M:%S JST\"\\)\n        md = []\n        md.append\\(\"# X Intelligence Dashboard\"\\)\n        md.append\\(\"\"\\)\n        md.append\\(\"**Date**: \" + TODAY\\)\n        md.append\\(\"**Generated**: \" + now_str\\)\n        md.append\\(\"\"\\)\n        md.append\\(\"---\"\\)\n        md.append\\(\"\"\\)\n        md.append\\(\"## Accounts to Follow \\(\" + str\\(len\\(follows\\)\\) + \"\\)\"\\)\n        md.append\\(\"\"\\)\n        if follows:\n            for handle in follows:\n                h = handle.lstrip\\(\"@\"\\)\n                md.append\\(\"- [ ] \" + handle + \" -- https://x.com/\" + h\\)\n        else:\n            md.append\\(\"_No accounts recommended today._\"\\)\n        md.append\\(\"\"\\)\n        md.append\\(\"## Tweets to Repost \\(\" + str\\(len\\(retweets\\)\\) + \"\\)\"\\)\n        md.append\\(\"\"\\)\n        if retweets:\n            for url in retweets:\n                md.append\\(\"- [ ] \" + url\\)\n        else:\n            md.append\\(\"_No tweets recommended for repost today._\"\\)\n        md.append\\(\"\"\\)\n        md.append\\(\"## Article Material \\(\" + str\\(len\\(articles\\)\\) + \"\\)\"\\)\n        md.append\\(\"\"\\)\n        if articles:\n            for i, item in enumerate\\(articles, 1\\):\n                topic = item.get\\(\"topic\", \"Untitled\"\\)\n                summary = item.get\\(\"summary\", \"\"\\)\n                sources = item.get\\(\"sources\", []\\)\n                md.append\\(\"### \" + str\\(i\\) + \". \" + topic\\)\n                if summary:\n                    md.append\\(\"\"\\)\n                    md.append\\(summary\\)\n                if sources:\n                    md.append\\(\"\"\\)\n                    md.append\\(\"**Sources**:\"\\)\n                    for src in sources:\n                        md.append\\(\"- \" + src\\)\n                md.append\\(\"\"\\)\n        else:\n            md.append\\(\"_No article material identified today._\"\\)\n        md.append\\(\"\"\\)\n        md.append\\(\"---\"\\)\n        md.append\\(\"\"\\)\n        md.append\\(\"_Auto-generated by X Intelligence Curation Engine_\"\\)\n        with open\\(DASHBOARD_PATH, \"w\", encoding=\"utf-8\"\\) as f:\n            f.write\\(chr\\(10\\).join\\(md\\)\\)\n        log.info\\(\"Dashboard written to: %s\", DASHBOARD_PATH\\)\n\n\n    def main\\(\\):\n        log.info\\(\"=== X Intelligence Curation -- %s ===\", TODAY\\)\n        discoveries = load_today_discoveries\\(\\)\n        log.info\\(\"Loaded %d total discovery items\", len\\(discoveries\\)\\)\n        curation = curate\\(discoveries\\)\n        follows = len\\(curation.get\\(\"follow_accounts\", []\\)\\)\n        retweets = len\\(curation.get\\(\"retweet_urls\", []\\)\\)\n        articles = len\\(curation.get\\(\"article_material\", []\\)\\)\n        log.info\\(\"Curation result: %d follows, %d retweets, %d article topics\", follows, retweets, articles\\)\n        save_curation\\(curation\\)\n        generate_dashboard\\(curation\\)\n        log.info\\(\"=== Curation complete ===\"\\)\n\n\n    if __name__ == \"__main__\":\n        main\\(\\)\n\"\"\"\\)\n\nwith open\\(target, \"w\"\\) as f:\n    f.write\\(code\\)\nos.chmod\\(target, 0o755\\)\nprint\\(f\"Written {target}, size: {os.path.getsize\\(target\\)} bytes\"\\)\nPYWRITEREOF)",
      "Bash(/tmp/curate_raw.py << 'RAWEOF'\n#!/usr/bin/env python3\n# curate.py - X Intelligence Curation Engine\n#\n# Reads discovery JSON files, uses xAI Grok API to score/prioritize\n# accounts to follow, tweets to retweet, content for Neo articles.\n#\n# Usage:\n#     python3 curate.py\n#     XAI_API_KEY=... python3 curate.py\n\nimport json\nimport os\nimport sys\nimport glob\nimport logging\nfrom datetime import datetime, timezone, timedelta\nfrom pathlib import Path\n\ntry:\n    import requests\nexcept ImportError:\n    import subprocess\n    subprocess.check_call\\([sys.executable, \"-m\", \"pip\", \"install\", \"requests\"]\\)\n    import requests\n\n# Configuration\nBASE_DIR = Path\\(\"/opt/shared/x-intelligence\"\\)\nDISCOVERY_DIR = BASE_DIR / \"data\" / \"discovery\"\nCURATION_DIR = BASE_DIR / \"data\" / \"curation\"\nLOGS_DIR = BASE_DIR / \"logs\"\nDASHBOARD_PATH = BASE_DIR / \"dashboard.md\"\n\nXAI_API_KEY = os.environ.get\\(\n    \"XAI_API_KEY\",\n    \"REDACTED_XAI_KEY\",\n\\)\nXAI_API_URL = \"https://api.x.ai/v1/chat/completions\"\nXAI_MODEL = \"grok-4-1-fast-non-reasoning\"\n\nJST = timezone\\(timedelta\\(hours=9\\)\\)\nTODAY = datetime.now\\(JST\\).strftime\\(\"%Y-%m-%d\"\\)\n\nCURATION_DIR.mkdir\\(parents=True, exist_ok=True\\)\nLOGS_DIR.mkdir\\(parents=True, exist_ok=True\\)\n\nlogging.basicConfig\\(\n    level=logging.INFO,\n    format=\"[%\\(asctime\\)s] [%\\(levelname\\)s] %\\(message\\)s\",\n    handlers=[\n        logging.StreamHandler\\(\\),\n        logging.FileHandler\\(LOGS_DIR / \"curate.log\"\\),\n    ],\n\\)\nlog = logging.getLogger\\(\"curate\"\\)\n\n\ndef load_today_discoveries\\(\\):\n    pattern = str\\(DISCOVERY_DIR / \\(TODAY + \"*.json\"\\)\\)\n    files = sorted\\(glob.glob\\(pattern\\)\\)\n    if not files:\n        pattern = str\\(DISCOVERY_DIR / \"*.json\"\\)\n        files = sorted\\(glob.glob\\(pattern\\)\\)\n        if files:\n            log.info\\(\"No files matching today \\(%s\\), using all %d discovery files\", TODAY, len\\(files\\)\\)\n    discoveries = []\n    for fpath in files:\n        try:\n            with open\\(fpath, \"r\", encoding=\"utf-8\"\\) as f:\n                data = json.load\\(f\\)\n                if isinstance\\(data, list\\):\n                    discoveries.extend\\(data\\)\n                else:\n                    discoveries.append\\(data\\)\n            count = len\\(data\\) if isinstance\\(data, list\\) else 1\n            log.info\\(\"Loaded discovery file: %s \\(%d items\\)\", fpath, count\\)\n        except \\(json.JSONDecodeError, IOError\\) as e:\n            log.warning\\(\"Failed to load %s: %s\", fpath, e\\)\n    return discoveries\n\n\ndef call_grok\\(prompt, max_tokens=2000\\):\n    headers = {\n        \"Authorization\": \"Bearer \" + XAI_API_KEY,\n        \"Content-Type\": \"application/json\",\n    }\n    payload = {\n        \"model\": XAI_MODEL,\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \\(\n                    \"You are a crypto/Web3 intelligence analyst for AISA \"\n                    \"\\(Asia Intelligence Signal Agent\\). You curate X/Twitter content \"\n                    \"about Asian crypto markets, regulations, DeFi, and macro trends. \"\n                    \"You must respond ONLY with valid JSON, no markdown fences.\"\n                \\),\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        \"max_tokens\": max_tokens,\n        \"temperature\": 0.3,\n    }\n    try:\n        resp = requests.post\\(XAI_API_URL, headers=headers, json=payload, timeout=60\\)\n        resp.raise_for_status\\(\\)\n        data = resp.json\\(\\)\n        return data[\"choices\"][0][\"message\"][\"content\"].strip\\(\\)\n    except requests.RequestException as e:\n        log.error\\(\"Grok API call failed: %s\", e\\)\n        if hasattr\\(e, \"response\"\\) and e.response is not None:\n            log.error\\(\"Response body: %s\", e.response.text[:500]\\)\n        raise\n\n\ndef parse_json_response\\(text\\):\n    text = text.strip\\(\\)\n    fence = chr\\(96\\) * 3\n    if text.startswith\\(fence\\):\n        text_lines = text.split\\(\"\\\\n\"\\)\n        text_lines = [l for l in text_lines if not l.strip\\(\\).startswith\\(fence\\)]\n        text = \"\\\\n\".join\\(text_lines\\)\n    return json.loads\\(text\\)\n\n\nCURATION_PROMPT = \\(\n    \"Analyze the following X/Twitter discovery data and produce a curation report.\\\\n\"\n    \"\\\\n\"\n    \"DISCOVERY DATA:\\\\n\"\n    \"{discovery_text}\\\\n\"\n    \"\\\\n\"\n    \"INSTRUCTIONS:\\\\n\"\n    \"1. Score each discovered account 1-10 for relevance to Asian crypto intelligence.\\\\n\"\n    \"   - Include accounts scoring 8+ in follow_accounts \\(return @handle strings\\).\\\\n\"\n    \"   - Prioritize: regulatory bodies, major exchanges, analysts, protocol teams.\\\\n\"\n    \"\\\\n\"\n    \"2. Score each discovered tweet/post 1-10 for retweet worthiness.\\\\n\"\n    \"   - Include tweets scoring 9+ in retweet_urls \\(return URL strings\\).\\\\n\"\n    \"   - Must be genuinely valuable: breaking news, deep analysis, regulatory updates.\\\\n\"\n    \"   - Do NOT retweet price predictions, shilling, or low-quality content.\\\\n\"\n    \"\\\\n\"\n    \"3. Identify topics suitable for Neo long-form article writing.\\\\n\"\n    \"   - Group related discoveries into article topics.\\\\n\"\n    \"   - Include source URLs and a brief topic description.\\\\n\"\n    \"\\\\n\"\n    \"Respond with ONLY valid JSON \\(no markdown fences\\):\\\\n\"\n    \"{json_template}\\\\n\"\n\\)\n\n\ndef curate\\(discoveries\\):\n    if not discoveries:\n        log.warning\\(\"No discoveries to curate. Generating empty curation.\"\\)\n        return {\"date\": TODAY, \"follow_accounts\": [], \"retweet_urls\": [], \"article_material\": []}\n    truncated = discoveries[:30]\n    discovery_text = json.dumps\\(truncated, indent=2, ensure_ascii=False, default=str\\)\n    json_template = json.dumps\\({\n        \"date\": TODAY,\n        \"follow_accounts\": [\"@handle1\", \"@handle2\"],\n        \"retweet_urls\": [\"https://x.com/...\"],\n        \"article_material\": [{\"topic\": \"Topic\", \"summary\": \"Brief desc\", \"sources\": [\"url1\"]}]\n    }, indent=4\\)\n    prompt = CURATION_PROMPT.format\\(discovery_text=discovery_text, json_template=json_template\\)\n    log.info\\(\"Sending %d discoveries to Grok for curation...\", len\\(truncated\\)\\)\n    raw_response = call_grok\\(prompt\\)\n    log.info\\(\"Grok response received, parsing...\"\\)\n    try:\n        result = parse_json_response\\(raw_response\\)\n    except json.JSONDecodeError as e:\n        log.error\\(\"Failed to parse Grok response as JSON: %s\", e\\)\n        log.error\\(\"Raw response: %s\", raw_response[:500]\\)\n        result = {\n            \"date\": TODAY, \"follow_accounts\": [], \"retweet_urls\": [],\n            \"article_material\": [], \"_raw_response\": raw_response[:1000]\n        }\n    result.setdefault\\(\"date\", TODAY\\)\n    result.setdefault\\(\"follow_accounts\", []\\)\n    result.setdefault\\(\"retweet_urls\", []\\)\n    result.setdefault\\(\"article_material\", []\\)\n    return result\n\n\ndef save_curation\\(curation\\):\n    output_path = CURATION_DIR / \\(TODAY + \"_approved.json\"\\)\n    with open\\(output_path, \"w\", encoding=\"utf-8\"\\) as f:\n        json.dump\\(curation, f, indent=2, ensure_ascii=False\\)\n    log.info\\(\"Curation saved to: %s\", output_path\\)\n    return output_path\n\n\ndef generate_dashboard\\(curation\\):\n    follows = curation.get\\(\"follow_accounts\", []\\)\n    retweets = curation.get\\(\"retweet_urls\", []\\)\n    articles = curation.get\\(\"article_material\", []\\)\n    now_str = datetime.now\\(JST\\).strftime\\(\"%Y-%m-%d %H:%M:%S JST\"\\)\n    md = []\n    md.append\\(\"# X Intelligence Dashboard\"\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"**Date**: \" + TODAY\\)\n    md.append\\(\"**Generated**: \" + now_str\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"---\"\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"## Accounts to Follow \\(\" + str\\(len\\(follows\\)\\) + \"\\)\"\\)\n    md.append\\(\"\"\\)\n    if follows:\n        for handle in follows:\n            h = handle.lstrip\\(\"@\"\\)\n            md.append\\(\"- [ ] \" + handle + \" -- https://x.com/\" + h\\)\n    else:\n        md.append\\(\"_No accounts recommended today._\"\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"## Tweets to Repost \\(\" + str\\(len\\(retweets\\)\\) + \"\\)\"\\)\n    md.append\\(\"\"\\)\n    if retweets:\n        for url in retweets:\n            md.append\\(\"- [ ] \" + url\\)\n    else:\n        md.append\\(\"_No tweets recommended for repost today._\"\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"## Article Material \\(\" + str\\(len\\(articles\\)\\) + \"\\)\"\\)\n    md.append\\(\"\"\\)\n    if articles:\n        for i, item in enumerate\\(articles, 1\\):\n            topic = item.get\\(\"topic\", \"Untitled\"\\)\n            summary = item.get\\(\"summary\", \"\"\\)\n            sources = item.get\\(\"sources\", []\\)\n            md.append\\(\"### \" + str\\(i\\) + \". \" + topic\\)\n            if summary:\n                md.append\\(\"\"\\)\n                md.append\\(summary\\)\n            if sources:\n                md.append\\(\"\"\\)\n                md.append\\(\"**Sources**:\"\\)\n                for src in sources:\n                    md.append\\(\"- \" + src\\)\n            md.append\\(\"\"\\)\n    else:\n        md.append\\(\"_No article material identified today._\"\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"---\"\\)\n    md.append\\(\"\"\\)\n    md.append\\(\"_Auto-generated by X Intelligence Curation Engine_\"\\)\n    with open\\(DASHBOARD_PATH, \"w\", encoding=\"utf-8\"\\) as f:\n        f.write\\(\"\\\\n\".join\\(md\\)\\)\n    log.info\\(\"Dashboard written to: %s\", DASHBOARD_PATH\\)\n\n\ndef main\\(\\):\n    log.info\\(\"=== X Intelligence Curation -- %s ===\", TODAY\\)\n    discoveries = load_today_discoveries\\(\\)\n    log.info\\(\"Loaded %d total discovery items\", len\\(discoveries\\)\\)\n    curation = curate\\(discoveries\\)\n    follows = len\\(curation.get\\(\"follow_accounts\", []\\)\\)\n    retweets = len\\(curation.get\\(\"retweet_urls\", []\\)\\)\n    articles = len\\(curation.get\\(\"article_material\", []\\)\\)\n    log.info\\(\"Curation result: %d follows, %d retweets, %d article topics\", follows, retweets, articles\\)\n    save_curation\\(curation\\)\n    generate_dashboard\\(curation\\)\n    log.info\\(\"=== Curation complete ===\"\\)\n\n\nif __name__ == \"__main__\":\n    main\\(\\)\nRAWEOF)",
      "Bash(/tmp/test_heredoc.txt << 'EOF123'\nline1\nline2\nchr\\(96\\) * 3\nEOF123)",
      "Bash(/tmp/test2.txt << 'TESTEOF'\nfence = chr\\(96\\) * 3\nif text.startswith\\(fence\\):\n    text_lines = text.split\\(\"\\\\n\"\\)\nTESTEOF)",
      "Bash(/tmp/test3.txt << 'RAWEOF'\nline1\nif __name__ == \"__main__\":\n    main\\(\\)\nRAWEOF)",
      "Bash(/tmp/curate_p1.py << 'P1EOF'\n#!/usr/bin/env python3\n# curate.py - X Intelligence Curation Engine\n#\n# Reads discovery JSON files, uses xAI Grok API to score/prioritize\n# accounts to follow, tweets to retweet, content for Neo articles.\n\nimport json\nimport os\nimport sys\nimport glob\nimport logging\nfrom datetime import datetime, timezone, timedelta\nfrom pathlib import Path\n\ntry:\n    import requests\nexcept ImportError:\n    import subprocess\n    subprocess.check_call\\([sys.executable, \"-m\", \"pip\", \"install\", \"requests\"]\\)\n    import requests\n\nBASE_DIR = Path\\(\"/opt/shared/x-intelligence\"\\)\nDISCOVERY_DIR = BASE_DIR / \"data\" / \"discovery\"\nCURATION_DIR = BASE_DIR / \"data\" / \"curation\"\nLOGS_DIR = BASE_DIR / \"logs\"\nDASHBOARD_PATH = BASE_DIR / \"dashboard.md\"\n\nXAI_API_KEY = os.environ.get\\(\n    \"XAI_API_KEY\",\n    \"REDACTED_XAI_KEY\",\n\\)\nXAI_API_URL = \"https://api.x.ai/v1/chat/completions\"\nXAI_MODEL = \"grok-4-1-fast-non-reasoning\"\n\nJST = timezone\\(timedelta\\(hours=9\\)\\)\nTODAY = datetime.now\\(JST\\).strftime\\(\"%Y-%m-%d\"\\)\n\nCURATION_DIR.mkdir\\(parents=True, exist_ok=True\\)\nLOGS_DIR.mkdir\\(parents=True, exist_ok=True\\)\n\nlogging.basicConfig\\(\n    level=logging.INFO,\n    format=\"[%\\(asctime\\)s] [%\\(levelname\\)s] %\\(message\\)s\",\n    handlers=[\n        logging.StreamHandler\\(\\),\n        logging.FileHandler\\(LOGS_DIR / \"curate.log\"\\),\n    ],\n\\)\nlog = logging.getLogger\\(\"curate\"\\)\nP1EOF)",
      "Bash(/tmp/curate_p2.py << 'P2EOF'\n\n\ndef load_today_discoveries\\(\\):\n    pattern = str\\(DISCOVERY_DIR / \\(TODAY + \"*.json\"\\)\\)\n    files = sorted\\(glob.glob\\(pattern\\)\\)\n    if not files:\n        pattern = str\\(DISCOVERY_DIR / \"*.json\"\\)\n        files = sorted\\(glob.glob\\(pattern\\)\\)\n        if files:\n            log.info\\(\"No files matching today \\(%s\\), using all %d discovery files\", TODAY, len\\(files\\)\\)\n    discoveries = []\n    for fpath in files:\n        try:\n            with open\\(fpath, \"r\", encoding=\"utf-8\"\\) as f:\n                data = json.load\\(f\\)\n                if isinstance\\(data, list\\):\n                    discoveries.extend\\(data\\)\n                else:\n                    discoveries.append\\(data\\)\n            count = len\\(data\\) if isinstance\\(data, list\\) else 1\n            log.info\\(\"Loaded discovery file: %s \\(%d items\\)\", fpath, count\\)\n        except \\(json.JSONDecodeError, IOError\\) as e:\n            log.warning\\(\"Failed to load %s: %s\", fpath, e\\)\n    return discoveries\n\n\ndef call_grok\\(prompt, max_tokens=2000\\):\n    headers = {\n        \"Authorization\": \"Bearer \" + XAI_API_KEY,\n        \"Content-Type\": \"application/json\",\n    }\n    payload = {\n        \"model\": XAI_MODEL,\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \\(\n                    \"You are a crypto/Web3 intelligence analyst for AISA \"\n                    \"\\(Asia Intelligence Signal Agent\\). You curate X/Twitter content \"\n                    \"about Asian crypto markets, regulations, DeFi, and macro trends. \"\n                    \"You must respond ONLY with valid JSON, no markdown fences.\"\n                \\),\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        \"max_tokens\": max_tokens,\n        \"temperature\": 0.3,\n    }\n    try:\n        resp = requests.post\\(XAI_API_URL, headers=headers, json=payload, timeout=60\\)\n        resp.raise_for_status\\(\\)\n        data = resp.json\\(\\)\n        return data[\"choices\"][0][\"message\"][\"content\"].strip\\(\\)\n    except requests.RequestException as e:\n        log.error\\(\"Grok API call failed: %s\", e\\)\n        if hasattr\\(e, \"response\"\\) and e.response is not None:\n            log.error\\(\"Response body: %s\", e.response.text[:500]\\)\n        raise\n\n\ndef parse_json_response\\(text\\):\n    text = text.strip\\(\\)\n    fence = chr\\(96\\) * 3\n    if text.startswith\\(fence\\):\n        text_lines = text.split\\(\"\\\\n\"\\)\n        text_lines = [l for l in text_lines if not l.strip\\(\\).startswith\\(fence\\)]\n        text = \"\\\\n\".join\\(text_lines\\)\n    return json.loads\\(text\\)\nP2EOF)",
      "Bash(/tmp/curate_p3.py:*)",
      "Bash(/tmp/curate_p4.py:*)",
      "WebFetch(domain:www.theunwindai.com)",
      "WebFetch(domain:claudefa.st)",
      "WebFetch(domain:sammyjankis.com)",
      "WebFetch(domain:novaorigin26.github.io)",
      "WebFetch(domain:www.productcompass.pm)",
      "Bash(del \"c:\\\\Users\\\\user\\\\OneDrive\\\\\\\\vps-automation-openclaw\\\\tmp_loop.py\" \"c:\\\\Users\\\\user\\\\OneDrive\\\\\\\\vps-automation-openclaw\\\\tmp_watchdog.sh\" \"c:\\\\Users\\\\user\\\\OneDrive\\\\\\\\vps-automation-openclaw\\\\tmp_memory_gen.py\" \"c:\\\\Users\\\\user\\\\OneDrive\\\\\\\\vps-automation-openclaw\\\\tmp_xintel_loop.service\")",
      "Bash(git reset:*)",
      "Bash(CLAUDE_PROJECT_DIR=\".\" bash:*)"
    ]
  },
  "hooks": {
    "SessionStart": [
      {
        "matcher": "",
        "hooks": [
          {
            "type": "command",
            "command": "bash \"$CLAUDE_PROJECT_DIR/.claude/hooks/session-start.sh\"",
            "timeout": 10
          }
        ]
      }
    ],
    "PreToolUse": [
      {
        "matcher": "Edit|Write|WebSearch|WebFetch|Read",
        "hooks": [
          {
            "type": "command",
            "command": "bash \"$CLAUDE_PROJECT_DIR/.claude/hooks/research-gate.sh\"",
            "timeout": 5
          }
        ]
      }
    ],
    "PostToolUse": [
      {
        "matcher": "WebSearch|WebFetch",
        "hooks": [
          {
            "type": "command",
            "command": "bash \"$CLAUDE_PROJECT_DIR/.claude/hooks/research-reward.sh\"",
            "timeout": 5
          }
        ]
      }
    ],
    "PostToolUseFailure": [
      {
        "matcher": "",
        "hooks": [
          {
            "type": "command",
            "command": "bash \"$CLAUDE_PROJECT_DIR/.claude/hooks/error-tracker.sh\"",
            "timeout": 5
          }
        ]
      }
    ],
    "SessionEnd": [
      {
        "matcher": "",
        "hooks": [
          {
            "type": "command",
            "command": "bash \"$CLAUDE_PROJECT_DIR/.claude/hooks/session-end.sh\"",
            "timeout": 10
          }
        ]
      }
    ]
  }
}
