#!/usr/bin/env python3
"""
neo_article_writer.py â€” Breaking Pipeline Phase 2: NEOã«è¨˜äº‹ç”Ÿæˆã‚’æŒ‡ç¤º

breaking_queue.json ã® pending ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–ã‚Šå‡ºã—ã€
å…ƒè¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è£œå¼·ã—ã€
NEO-ONE/TWO ã« Telethon çµŒç”±ã§è¨˜äº‹åŸ·ç­†ã‚’æŒ‡ç¤ºã™ã‚‹ã€‚

ä½¿ã„æ–¹:
  python3 neo_article_writer.py              # 1ä»¶å‡¦ç†
  python3 neo_article_writer.py --batch 5    # 5ä»¶ã¾ã¨ã‚ã¦æŒ‡ç¤º
  python3 neo_article_writer.py --dry-run    # æŒ‡ç¤ºå†…å®¹ã‚’ç¢ºèªã®ã¿
  python3 neo_article_writer.py --bot neo2   # NEO-TWOã«æŒ‡ç¤º

cron: */5 * * * * source /opt/cron-env.sh && python3 /opt/shared/scripts/neo_article_writer.py

ãƒ•ãƒ­ãƒ¼:
  breaking_queue.json (status=pending)
    â†’ å…ƒè¨˜äº‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ï¼ˆURLå–å¾—ï¼‰
    â†’ NEOã« Telethon ã§è¨˜äº‹åŸ·ç­†æŒ‡ç¤º
    â†’ status ã‚’ "writing" ã«å¤‰æ›´
    â†’ NEOãŒè¨˜äº‹å®Œæˆå¾Œã« breaking_pipeline_helper.py ã‚’å®Ÿè¡Œ
    â†’ status ãŒ "article_ready" ã«å¤‰ã‚ã‚‹
    â†’ x_quote_repost.py ãŒå¼•ç”¨ãƒªãƒã‚¹ãƒˆ
"""

import asyncio
import json
import os
import sys
import subprocess
import argparse
from datetime import datetime, timezone, timedelta

QUEUE_FILE = "/opt/shared/scripts/breaking_queue.json"
SEND_SCRIPT = "/opt/shared/scripts/send-to-neo.py"
WRITING_TIMEOUT_MIN = 60  # writingçŠ¶æ…‹ã§60åˆ†ä»¥ä¸ŠçµŒéã—ãŸã‚‰å†é€ä¿¡

# ã‚«ãƒ†ã‚´ãƒª â†’ æ—¥æœ¬èªã‚¿ã‚¯ã‚½ãƒãƒŸãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°
CAT_TO_GENRE = {
    "ç·åˆ": "ç¤¾ä¼š",
    "çµŒæ¸ˆ": "çµŒæ¸ˆãƒ»è²¿æ˜“",
    "é‡‘è": "é‡‘èãƒ»å¸‚å ´",
    "æš—å·è³‡ç”£": "æš—å·è³‡ç”£",
    "AI": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼",
    "ãƒ†ãƒƒã‚¯": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼",
    "æ”¿æ²»": "ã‚¬ãƒãƒŠãƒ³ã‚¹ãƒ»æ³•",
    "åœ°æ”¿å­¦": "åœ°æ”¿å­¦ãƒ»å®‰å…¨ä¿éšœ",
    "å›½éš›": "åœ°æ”¿å­¦ãƒ»å®‰å…¨ä¿éšœ",
    "é€Ÿå ±": "ç¤¾ä¼š",
    "ã‚¨ãƒãƒ«ã‚®ãƒ¼": "ã‚¨ãƒãƒ«ã‚®ãƒ¼",
}


def load_queue():
    if os.path.exists(QUEUE_FILE):
        with open(QUEUE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []


def save_queue(queue):
    with open(QUEUE_FILE, "w", encoding="utf-8") as f:
        json.dump(queue, f, ensure_ascii=False, indent=2)


def get_pending_items(queue):
    """pendingçŠ¶æ…‹ã®ã‚¢ã‚¤ãƒ†ãƒ ï¼ˆã‚¹ã‚³ã‚¢é †ï¼‰"""
    pending = [q for q in queue if q.get("status") == "pending"]
    pending.sort(key=lambda x: (x.get("score", 0), x.get("likes", 0)), reverse=True)
    return pending


def get_stuck_writing(queue):
    """writingçŠ¶æ…‹ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸã‚¢ã‚¤ãƒ†ãƒ """
    now = datetime.now(timezone.utc)
    stuck = []
    for q in queue:
        if q.get("status") == "writing":
            started = q.get("writing_started_at", "")
            if started:
                try:
                    start_dt = datetime.fromisoformat(started)
                    if (now - start_dt) > timedelta(minutes=WRITING_TIMEOUT_MIN):
                        stuck.append(q)
                except (ValueError, TypeError):
                    stuck.append(q)
            else:
                stuck.append(q)
    return stuck


def extract_url(url_item):
    """article_urlsã®è¦ç´ ã‹ã‚‰URLæ–‡å­—åˆ—ã‚’å®‰å…¨ã«å–ã‚Šå‡ºã™"""
    if isinstance(url_item, str):
        return url_item
    if isinstance(url_item, dict):
        return url_item.get("url", url_item.get("expanded_url", ""))
    if isinstance(url_item, (list, tuple)) and len(url_item) > 0:
        return str(url_item[0])
    return str(url_item) if url_item else ""


def scrape_article_url(url, timeout=15):
    """å…ƒè¨˜äº‹ã®URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆï¼‰"""
    if not url:
        return ""
    try:
        from curl_cffi import requests as cffi_requests
        from bs4 import BeautifulSoup

        resp = cffi_requests.get(url, impersonate="chrome", timeout=timeout, allow_redirects=True)
        if resp.status_code != 200:
            return ""

        soup = BeautifulSoup(resp.text, "lxml")

        # ä¸è¦ãªè¦ç´ ã‚’å‰Šé™¤
        for tag in soup.find_all(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()

        # articleè¦ç´ å„ªå…ˆã€ãªã‘ã‚Œã°body
        article = soup.find("article")
        if article:
            text = article.get_text(separator="\n", strip=True)
        else:
            text = soup.get_text(separator="\n", strip=True)

        # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(text) > 5000:
            text = text[:5000] + "\n...(truncated)"

        return text.strip()
    except Exception as e:
        print(f"    ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—å¤±æ•—({url[:50]}): {e}")
        return ""


def _load_json(path):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®‰å…¨ã«èª­ã¿è¾¼ã‚€"""
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def find_previous_article(item):
    """åŒã˜ãƒˆãƒ”ãƒƒã‚¯ï¼ˆåŠ›å­¦Ã—ã‚¸ãƒ£ãƒ³ãƒ«ï¼‰ã®ç›´è¿‘ã®å‰å›è¨˜äº‹ã‚’æ¤œç´¢ã€‚

    Returns:
        dict or None: {
            "article_id", "title", "url", "published_at",
            "bottom_line", "scenarios", "dynamics_tags", "chain_count"
        }
    """
    cat = item.get("cat", "")
    genre = CAT_TO_GENRE.get(cat, "")
    if not genre:
        return None

    idx = _load_json("/opt/shared/nowpattern_article_index.json")
    db = _load_json("/opt/shared/scripts/prediction_db.json")

    # article_index ã®ã‚¸ãƒ£ãƒ³ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§åŒã‚¸ãƒ£ãƒ³ãƒ«è¨˜äº‹ã‚’å–å¾—
    genre_article_ids = idx.get("genre_index", {}).get(genre, [])
    if not genre_article_ids:
        return None

    # è¨˜äº‹ã‚’æ—¥ä»˜é™é †ã§ã‚½ãƒ¼ãƒˆï¼ˆæœ€æ–°ãŒå…ˆé ­ï¼‰
    articles = idx.get("articles", [])
    genre_articles = []
    for a in articles:
        if a.get("article_id") in genre_article_ids:
            genre_articles.append(a)
    genre_articles.sort(key=lambda a: a.get("published_at", ""), reverse=True)

    if not genre_articles:
        return None

    # æœ€æ–°ã®è¨˜äº‹ã‚’ã€Œå‰å›ã€ã¨ã—ã¦è¿”ã™
    prev = genre_articles[0]

    # prediction_dbã‹ã‚‰åŒè¨˜äº‹ã®ã‚·ãƒŠãƒªã‚ªã‚’æ¤œç´¢
    prev_scenarios = []
    for p in db.get("predictions", []):
        if p.get("article_id") == prev.get("article_id"):
            prev_scenarios = p.get("scenarios", [])
            break

    # ãƒã‚§ãƒ¼ãƒ³ã‚«ã‚¦ãƒ³ãƒˆ: åŒã‚¸ãƒ£ãƒ³ãƒ«ã®è¨˜äº‹æ•° + 1ï¼ˆä»Šå›ã®è¨˜äº‹ï¼‰
    chain_count = len(genre_articles) + 1

    return {
        "article_id": prev.get("article_id", ""),
        "title": prev.get("title_ja", "") or prev.get("title_en", ""),
        "url": prev.get("url", ""),
        "published_at": prev.get("published_at", "")[:10],
        "bottom_line": prev.get("bottom_line", ""),
        "scenarios": prev_scenarios,
        "dynamics_tags": prev.get("dynamics_tags", []),
        "chain_count": chain_count,
    }


def build_delta_context(item):
    """NEOã«æ¸¡ã™Deltaå·®åˆ†æƒ…å ±ã®ãƒ†ã‚­ã‚¹ãƒˆã¨æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã€‚

    Returns:
        tuple: (delta_text: str, delta_data: dict or None)
        - delta_text: NEOã¸ã®æŒ‡ç¤ºã«å«ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
        - delta_data: article_builderã«æ¸¡ã™æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
    """
    prev = find_previous_article(item)
    if not prev:
        return "", None

    # å‰å›è¨˜äº‹ã®ã‚·ãƒŠãƒªã‚ªãŒç©ºã®å ´åˆ
    if not prev.get("scenarios"):
        delta_text = (
            f"ã€Delta â€” å‰å›è¨˜äº‹ã¨ã®å·®åˆ†ã€‘\n"
            f"å‰å›: {prev['title'][:60]} ({prev['published_at']})\n"
            f"URL: {prev['url']}\n"
            f"â†’ å‰å›ã®è¨˜äº‹ã¨æ¯”è¼ƒã—ã¦ã€Œä½•ãŒå¤‰ã‚ã£ãŸã‹ã€ã‚’å…·ä½“çš„ã«åˆ†æã—ã¦ãã ã•ã„ã€‚\n"
            f"â†’ delta_data ã® delta_reason ã«ã€Œãªãœå¤‰ã‚ã£ãŸã‹ã€ã‚’1-2æ–‡ã§æ›¸ã„ã¦ãã ã•ã„ã€‚\n"
            f"â†’ ã“ã‚Œã¯ã“ã®ãƒˆãƒ”ãƒƒã‚¯{prev['chain_count']}å›ç›®ã®åˆ†æã§ã™ã€‚"
        )
        delta_data = {
            "prev_article_title": prev["title"][:60],
            "prev_article_url": prev["url"],
            "prev_article_date": prev["published_at"],
            "prev_scenarios": [],
            "current_scenarios": [],  # NEOãŒè¨˜äº‹åŸ·ç­†æ™‚ã«åŸ‹ã‚ã‚‹
            "delta_reason": "",  # NEOãŒè¨˜äº‹åŸ·ç­†æ™‚ã«åŸ‹ã‚ã‚‹
            "chain_count": prev["chain_count"],
        }
        return delta_text, delta_data

    # å‰å›ã‚·ãƒŠãƒªã‚ªã‚ã‚Šã®å ´åˆ: æ§‹é€ åŒ–ã•ã‚ŒãŸå·®åˆ†æƒ…å ±ã‚’æä¾›
    scenarios_str = ", ".join(
        f"{s.get('label', '')}({s.get('probability', 0)})"
        for s in prev["scenarios"]
    )
    bottom_line_str = f"å‰å›ã®BOTTOM LINE: {prev['bottom_line']}" if prev.get("bottom_line") else ""

    delta_text = (
        f"ã€Delta â€” å‰å›è¨˜äº‹ã¨ã®å·®åˆ†ï¼ˆé‡è¦: å¿…ãšå·®åˆ†ã‚’åˆ†æã™ã‚‹ã“ã¨ï¼‰ã€‘\n"
        f"å‰å›: {prev['title'][:60]} ({prev['published_at']})\n"
        f"URL: {prev['url']}\n"
        f"å‰å›ã‚·ãƒŠãƒªã‚ª: {scenarios_str}\n"
        f"{bottom_line_str}\n"
        f"\n"
        f"â†’ å‰å›ã®ã‚·ãƒŠãƒªã‚ªç¢ºç‡ã¨æ¯”è¼ƒã—ã¦ã€ä»Šå›ã®åˆ†æã§ç¢ºç‡ãŒã©ã†å¤‰åŒ–ã—ãŸã‹æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚\n"
        f"â†’ delta_data ã® current_scenarios ã«ä»Šå›ã®ç¢ºç‡ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚\n"
        f"â†’ delta_data ã® delta_reason ã«ã€Œãªãœç¢ºç‡ãŒå¤‰ã‚ã£ãŸã‹ã€ã‚’1-2æ–‡ã§æ›¸ã„ã¦ãã ã•ã„ã€‚\n"
        f"â†’ ã“ã‚Œã¯ã“ã®ãƒˆãƒ”ãƒƒã‚¯{prev['chain_count']}å›ç›®ã®åˆ†æã§ã™ã€‚"
    )
    delta_data = {
        "prev_article_title": prev["title"][:60],
        "prev_article_url": prev["url"],
        "prev_article_date": prev["published_at"],
        "prev_scenarios": [
            {"label": s.get("label", ""), "probability": s.get("probability", "")}
            for s in prev["scenarios"]
        ],
        "current_scenarios": [],  # NEOãŒè¨˜äº‹åŸ·ç­†æ™‚ã«åŸ‹ã‚ã‚‹
        "delta_reason": "",  # NEOãŒè¨˜äº‹åŸ·ç­†æ™‚ã«åŸ‹ã‚ã‚‹
        "chain_count": prev["chain_count"],
    }

    return delta_text, delta_data


def get_flywheel_context(item):
    """ãƒ•ãƒ©ã‚¤ãƒ›ã‚¤ãƒ¼ãƒ«: åŒã˜ã‚«ãƒ†ã‚´ãƒªã®éå»äºˆæ¸¬ã¨è¨˜äº‹ã‚’å–å¾—ã—ã¦NEOã«æä¾›"""
    context_parts = []
    cat = item.get("cat", "")
    genre = CAT_TO_GENRE.get(cat, "")

    # 0. Deltaï¼ˆå·®åˆ†ï¼‰æƒ…å ±
    delta_text, _delta_data = build_delta_context(item)
    if delta_text:
        context_parts.append(delta_text)

    # 1. prediction_db ã‹ã‚‰åŒã‚«ãƒ†ã‚´ãƒªã®openäºˆæ¸¬ã‚’æ¤œç´¢
    try:
        db = _load_json("/opt/shared/scripts/prediction_db.json")
        related_preds = []
        for p in db.get("predictions", []):
            if p.get("status") == "open":
                p_genre = p.get("genre_tags", "")
                p_dynamics = p.get("dynamics_tags", "")
                if genre and (genre in p_genre or cat.lower() in p_dynamics.lower()):
                    related_preds.append(p)
        if related_preds:
            context_parts.append("\nã€éå»ã®é–¢é€£äºˆæ¸¬ï¼ˆãƒ•ãƒ©ã‚¤ãƒ›ã‚¤ãƒ¼ãƒ«å‚ç…§ï¼‰ã€‘")
            for rp in related_preds[:3]:
                scenarios_str = ", ".join(
                    f"{s.get('label','')}({s.get('probability',0)})"
                    for s in rp.get("scenarios", [])
                )
                context_parts.append(
                    f"- {rp['prediction_id']}: {rp.get('article_title','')[:60]}\n"
                    f"  ã‚·ãƒŠãƒªã‚ª: {scenarios_str}\n"
                    f"  ãƒˆãƒªã‚¬ãƒ¼: {rp.get('open_loop_trigger','')[:80]}"
                )
            context_parts.append("â†’ ä¸Šè¨˜ã®äºˆæ¸¬ã‚’è¸ã¾ãˆã¦ã€æ–°ã—ã„è¨˜äº‹ã®åˆ†æã«æ´»ã‹ã—ã¦ãã ã•ã„ã€‚")
    except Exception:
        pass

    # 2. article_index ã‹ã‚‰åŒã‚¸ãƒ£ãƒ³ãƒ«ã®éå»è¨˜äº‹ã‚’æ¤œç´¢
    try:
        idx = _load_json("/opt/shared/nowpattern_article_index.json")
        genre_articles = idx.get("genre_index", {}).get(genre, [])
        if genre_articles:
            context_parts.append(f"\nã€åŒã‚¸ãƒ£ãƒ³ãƒ«({genre})ã®éå»è¨˜äº‹: {len(genre_articles)}ä»¶ã€‘")
            all_articles = idx.get("articles", [])
            for aid in genre_articles[-3:]:
                for a in all_articles:
                    if a.get("article_id") == aid:
                        context_parts.append(f"- {aid}: {a.get('title_ja','')[:50]} ({a.get('url','')})")
                        break
    except Exception:
        pass

    return "\n".join(context_parts)


def build_neo_instruction(item, scraped_text="", past_context=""):
    """NEOã¸ã®è¨˜äº‹åŸ·ç­†æŒ‡ç¤ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰"""
    tweet_id = item.get("tweet_id", "")
    account = item.get("account", "?")
    lang = item.get("lang", "ja")
    cat = item.get("cat", "ç·åˆ")
    text = item.get("text", "")
    tweet_url = item.get("tweet_url", "")
    article_urls = item.get("article_urls", [])
    likes = item.get("likes", 0)
    retweets = item.get("retweets", 0)
    genre = CAT_TO_GENRE.get(cat, "ç¤¾ä¼š")

    # è¨˜äº‹è¨€èªã®æ±ºå®š
    article_lang = "ja"  # nowpattern.comã®ãƒ¡ã‚¤ãƒ³è¨€èª
    if lang == "en":
        article_lang = "ja"  # è‹±èªã‚½ãƒ¼ã‚¹ã§ã‚‚æ—¥æœ¬èªè¨˜äº‹ã‚’å„ªå…ˆ

    instruction = f"""â–  ãƒŸãƒƒã‚·ãƒ§ãƒ³: Nowpattern Breaking Pipeline â€” Now Reportè¨˜äº‹ã‚’åŸ·ç­†ã—ã¦Ghostã«æŠ•ç¨¿

ã€ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã€‘
- tweet_id: {tweet_id}
- tweet_url: {tweet_url}
- ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ: @{account}
- è¨€èª: {lang}
- ã‚«ãƒ†ã‚´ãƒª: {cat}
- ã„ã„ã­: {likes} / RT: {retweets}
- ãƒ†ã‚­ã‚¹ãƒˆ:
{text}
"""

    if article_urls:
        instruction += f"\nã€å…ƒè¨˜äº‹URLã€‘\n"
        for url_item in article_urls:
            url_str = extract_url(url_item)
            if url_str:
                instruction += f"- {url_str}\n"

    if scraped_text:
        instruction += f"\nã€å…ƒè¨˜äº‹æœ¬æ–‡ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—æ¸ˆã¿ï¼‰ã€‘\n{scraped_text}\n"

    if past_context:
        instruction += f"\n{past_context}\n"

    instruction += f"""
ã€åŸ·ç­†è¦ä»¶ â€” Nowpattern v5.0 Delta Formatã€‘
1. ä¸Šè¨˜ãƒ„ã‚¤ãƒ¼ãƒˆå†…å®¹ã‚’åˆ†æã—ã€Now Reportï¼ˆ1,500-2,500èªï¼‰ã‚’æ—¥æœ¬èªã§åŸ·ç­†
2. Deep Pattern v4.0ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¾“ã†:
   - BOTTOM LINE: è¨˜äº‹ã®æ ¸å¿ƒã‚’1æ–‡ã§ï¼ˆèª­è€…ãŒ3ç§’ã§ç†è§£ï¼‰+ ãƒ‘ã‚¿ãƒ¼ãƒ³å + åŸºæœ¬ã‚·ãƒŠãƒªã‚ª + æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆ
   - ä½•ãŒèµ·ããŸã‹ï¼ˆäº‹å®Ÿã®è¦ç´„ã€300-400èªï¼‰
   - ãªãœé‡è¦ã‹ï¼ˆæ§‹é€ çš„æ„å‘³ã€300-400èªï¼‰
   - Between the Lines: å…¬å¼ç™ºè¡¨ãŒã€Œè¨€ã£ã¦ã„ãªã„ã“ã¨ã€ã‚’1æ®µè½ã§ã€‚è£ã®åŠ›å­¦ã€éš ã•ã‚ŒãŸæ„å›³ã‚’åˆ†æ
   - ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ­£ä½“ï¼ˆåŠ›å­¦åˆ†æã€400-500èªï¼‰â€” æ®µè½å†…ã®æœ€é‡è¦ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å¤ªå­—(strong)ã§å¼·èª¿
   - ä¸»è¦ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆåˆ©å®³é–¢ä¿‚è€…ã€150-200èªï¼‰
   - ä»Šå¾Œã®å±•æœ›ï¼ˆ3ã‚·ãƒŠãƒªã‚ª+ç¢ºç‡ã€200-300èªï¼‰
   - Open Loop: æ¬¡ã®ãƒˆãƒªã‚¬ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆ+å…·ä½“çš„æ—¥ä»˜ã€ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¿½è·¡ãƒ†ãƒ¼ãƒ
3. 5è¦ç´ ãƒã‚§ãƒƒã‚¯: æ­´å²ãƒ»åˆ©å®³ãƒ»è«–ç†ãƒ»ã‚·ãƒŠãƒªã‚ªãƒ»ç¤ºå”†
4. ã‚¸ãƒ£ãƒ³ãƒ«ã‚¿ã‚°: {genre}
5. åŠ›å­¦ã‚¿ã‚°: åˆ†æå†…å®¹ã‹ã‚‰æœ€é©ãªåŠ›å­¦ã‚¿ã‚°ã‚’1-2å€‹é¸æŠ
6. ãƒˆãƒ¼ãƒ³: Matt Levineçš„ãªä¼šè©±å£èª¿ã€‚èª­è€…ã®çŸ¥æ€§ã‚’å°Šé‡ã—ã¤ã¤å°‚é–€ç”¨èªã‚’ã‹ã¿ç •ãã€‚ä»®å®šå¯¾è©±OKã€‚

ã€v5.0 ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆå¿…ãšåŸ‹ã‚ã‚‹ã“ã¨ï¼‰ã€‘
- bottom_line: è¨˜äº‹ã®æ ¸å¿ƒã‚’1æ–‡ã§
- bottom_line_pattern: åŠ›å­¦ãƒ‘ã‚¿ãƒ¼ãƒ³åã®è¦ç´„
- bottom_line_scenario: åŸºæœ¬ã‚·ãƒŠãƒªã‚ªã®ä¸€æ–‡è¦ç´„
- bottom_line_watch: æ¬¡ã®æ³¨ç›®ã‚¤ãƒ™ãƒ³ãƒˆ+æ—¥ä»˜
- between_the_lines: å ±é“ãŒè¨€ã£ã¦ã„ãªã„æœ¬å½“ã®è©±ï¼ˆ1æ®µè½ï¼‰
- open_loop_trigger: æ¬¡ã®ãƒˆãƒªã‚¬ãƒ¼+å…·ä½“çš„æ—¥ä»˜
- open_loop_series: ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¬¡ã®è¿½è·¡ãƒ†ãƒ¼ãƒ
- å„dynamics_sectionsã®analysiså†…ã§æœ€é‡è¦ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’HTML <strong>ã‚¿ã‚°ã§å¤ªå­—ã«

ã€v5.0 Deltaï¼ˆå·®åˆ†ï¼‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã€‘
- delta_data: å‰å›è¨˜äº‹ã¨ã®å·®åˆ†æƒ…å ±ï¼ˆä¸Šè¨˜ã®ã€Deltaã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åŸºã¥ã„ã¦åŸ‹ã‚ã‚‹ï¼‰
  - current_scenarios: ä»Šå›ã®3ã‚·ãƒŠãƒªã‚ªç¢ºç‡ï¼ˆå‰å›ã¨ã®æ¯”è¼ƒç”¨ï¼‰
  - delta_reason: ã€Œãªãœç¢ºç‡ãŒå¤‰ã‚ã£ãŸã‹ã€ã‚’1-2æ–‡ã§
  â€» å‰å›è¨˜äº‹ãŒãªã„å ´åˆã¯ delta_data ã‚’ç©ºã«ã™ã‚‹ï¼ˆchain_count: 1ï¼‰
  â€» å‰å›è¨˜äº‹ãŒã‚ã‚‹å ´åˆã€å‰å›ã®ç¢ºç‡ã¨ä»Šå›ã®ç¢ºç‡ã‚’æ¯”è¼ƒã—ã¦å¤‰åŒ–é‡ã‚’æ˜ç¤ºã™ã‚‹

ã€å‡ºåŠ›æ‰‹é †ã€‘
1. JSONãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚€:
   cat /opt/shared/scripts/breaking_article_template.json

2. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¨˜äº‹å†…å®¹ã§åŸ‹ã‚ã¦ã€ä»¥ä¸‹ã®ãƒ‘ã‚¹ã«ä¿å­˜:
   /tmp/article_{tweet_id}.json

   - tweet_id ã¯ "{tweet_id}" ã‚’ä½¿ç”¨
   - language ã¯ "{article_lang}"
   - genre_tags ã¯ "{genre}"
   - source_urls ã®URLã¯ "{tweet_url}"
   - x_comment ã¯200å­—ä»¥å†…ã®å¼•ç”¨ãƒªãƒã‚¹ãƒˆç”¨ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆè£èª­ã¿å‹: å¥½å¥‡å¿ƒâ†’åˆ†æâ†’ãƒªãƒ³ã‚¯â†’å•ã„ã‹ã‘ï¼‰

3. GhostæŠ•ç¨¿ã‚’å®Ÿè¡Œ:
   python3 /opt/shared/scripts/breaking_pipeline_helper.py /tmp/article_{tweet_id}.json

ã“ã‚Œã«ã‚ˆã‚ŠGhostã¸ã®æŠ•ç¨¿ã¨breaking_queue.jsonã®æ›´æ–°ãŒè‡ªå‹•ã§è¡Œã‚ã‚Œã¾ã™ã€‚
å®Œäº†å¾Œã€è¨˜äº‹URLã‚’å ±å‘Šã—ã¦ãã ã•ã„ã€‚"""

    return instruction


def send_to_neo(bot, message, dry_run=False):
    """TelethonçµŒç”±ã§NEOã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡"""
    if dry_run:
        print(f"  [DRY-RUN] {bot}ã¸ã®æŒ‡ç¤º:")
        print(f"  {message[:200]}...")
        return True

    try:
        result = subprocess.run(
            ["python3", SEND_SCRIPT, "--bot", bot, "--msg", message],
            capture_output=True, text=True, timeout=30
        )
        if result.returncode == 0:
            print(f"  âœ… {bot}ã«æŒ‡ç¤ºé€ä¿¡å®Œäº†")
            return True
        else:
            print(f"  âŒ é€ä¿¡å¤±æ•—: {result.stderr}")
            return False
    except Exception as e:
        print(f"  âŒ é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def run_writer(batch_size=1, bot="neo1", dry_run=False):
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†: ã‚­ãƒ¥ãƒ¼ã‹ã‚‰pendingã‚’å–ã‚Šå‡ºã—ã¦NEOã«æŒ‡ç¤º"""
    queue = load_queue()

    # ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹é›†è¨ˆ
    status_counts = {}
    for q in queue:
        s = q.get("status", "unknown")
        status_counts[s] = status_counts.get(s, 0) + 1

    print(f"ğŸ“‹ ã‚­ãƒ¥ãƒ¼çŠ¶æ³: {len(queue)} ä»¶")
    for s, c in sorted(status_counts.items()):
        print(f"   {s}: {c}")

    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãŸ writing ã‚¢ã‚¤ãƒ†ãƒ ã‚’ pending ã«æˆ»ã™
    stuck = get_stuck_writing(queue)
    if stuck:
        print(f"\nâš ï¸ {len(stuck)} ä»¶ãŒ writing ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{WRITING_TIMEOUT_MIN}åˆ†è¶…éï¼‰â†’ pending ã«æˆ»ã—ã¾ã™")
        for item in stuck:
            item["status"] = "pending"
            item["writing_timeout_count"] = item.get("writing_timeout_count", 0) + 1
        if not dry_run:
            save_queue(queue)

    # pending ã‚¢ã‚¤ãƒ†ãƒ å–å¾—
    pending = get_pending_items(queue)
    if not pending:
        print("\nå‡¦ç†å¯¾è±¡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    print(f"\nğŸ“ å‡¦ç†å¯¾è±¡: {len(pending)} ä»¶ã®ã†ã¡ {min(batch_size, len(pending))} ä»¶ã‚’å‡¦ç†")

    # writingä¸­ã®ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’ç¢ºèªï¼ˆåŒæ™‚ã«å‡¦ç†ã—ã™ããªã„ï¼‰
    writing_count = sum(1 for q in queue if q.get("status") == "writing")
    max_concurrent = 3  # åŒæ™‚writingä¸Šé™
    available_slots = max(0, max_concurrent - writing_count)

    if available_slots == 0:
        print(f"\nâ³ ç¾åœ¨ {writing_count} ä»¶ãŒå‡¦ç†ä¸­ï¼ˆä¸Šé™{max_concurrent}ï¼‰ã€‚å®Œäº†ã‚’å¾…ã¡ã¾ã™ã€‚")
        return

    actual_batch = min(batch_size, len(pending), available_slots)
    sent_count = 0

    # NEO-ONE / NEO-TWO ã®äº¤äº’ä½¿ç”¨
    bots = ["neo1", "neo2"]

    for i, item in enumerate(pending[:actual_batch]):
        tweet_id = item.get("tweet_id", "?")
        account = item.get("account", "?")
        text_preview = item.get("text", "")[:80]

        print(f"\n--- [{i+1}/{actual_batch}] @{account}: {text_preview}...")

        # å…ƒè¨˜äº‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ï¼ˆãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆï¼‰
        scraped_text = ""
        article_urls = item.get("article_urls", [])
        if article_urls:
            first_url = extract_url(article_urls[0])
            if first_url:
                print(f"  ğŸ“° å…ƒè¨˜äº‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ä¸­: {first_url[:60]}...")
                scraped_text = scrape_article_url(first_url)
                if scraped_text:
                    print(f"  â†’ {len(scraped_text)} æ–‡å­—å–å¾—")
                else:
                    print(f"  â†’ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—å¤±æ•—ï¼ˆãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§åŸ·ç­†ï¼‰")

        # ãƒ•ãƒ©ã‚¤ãƒ›ã‚¤ãƒ¼ãƒ«: åŒã˜åŠ›å­¦/ã‚¸ãƒ£ãƒ³ãƒ«ã®éå»äºˆæ¸¬ã‚’å–å¾—ã—ã¦NEOã«æä¾›
        past_context = get_flywheel_context(item)

        # NEOæŒ‡ç¤ºæ§‹ç¯‰
        instruction = build_neo_instruction(item, scraped_text, past_context)

        # é€ä¿¡å…ˆ: æŒ‡å®šãŒã‚ã‚Œã°ãã‚Œã€ãªã‘ã‚Œã°äº¤äº’
        target_bot = bot if bot != "auto" else bots[i % len(bots)]

        # é€ä¿¡
        success = send_to_neo(target_bot, instruction, dry_run=dry_run)

        if success and not dry_run:
            item["status"] = "writing"
            item["writing_started_at"] = datetime.now(timezone.utc).isoformat()
            item["assigned_to"] = target_bot
            sent_count += 1

    if not dry_run:
        save_queue(queue)

    print(f"\n=== å®Œäº†: {sent_count} ä»¶ã®è¨˜äº‹åŸ·ç­†æŒ‡ç¤ºã‚’é€ä¿¡ ===")


def main():
    parser = argparse.ArgumentParser(description="NEOè¨˜äº‹ç”Ÿæˆãƒˆãƒªã‚¬ãƒ¼ï¼ˆBreaking Pipeline Phase 2ï¼‰")
    parser.add_argument("--batch", type=int, default=1, help="ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ä»¶æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰")
    parser.add_argument("--bot", default="neo1", choices=["neo1", "neo2", "auto"],
                        help="æŒ‡ç¤ºå…ˆï¼ˆneo1/neo2/auto=äº¤äº’ï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="é€ä¿¡ã›ãšã«æŒ‡ç¤ºå†…å®¹ã‚’ç¢ºèªã®ã¿")
    args = parser.parse_args()

    run_writer(batch_size=args.batch, bot=args.bot, dry_run=args.dry_run)


if __name__ == "__main__":
    main()
