#!/usr/bin/env python3
"""
X ãƒ„ã‚¤ãƒ¼ãƒˆåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ â€” Nowpattern Breaking Pipeline èµ·ç‚¹

30+ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®æœ€æ–°ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å·¡å›ã—ã€
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° + ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆé–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ã—ã¦
breaking_queue.json ã«ä¿å­˜ã™ã‚‹ã€‚

ä½¿ã„æ–¹:
  python3 x_tweet_collector.py              # é€šå¸¸å®Ÿè¡Œ
  python3 x_tweet_collector.py --dry-run    # åé›†ã®ã¿ã€ã‚­ãƒ¥ãƒ¼æ›¸ãè¾¼ã¿ãªã—
  python3 x_tweet_collector.py --max 50     # æœ€å¤§50ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 

cron: */10 * * * * source /opt/cron-env.sh && python3 /opt/shared/scripts/x_tweet_collector.py
"""

import asyncio
import json
import os
import sys
import time
import argparse
from datetime import datetime, timezone, timedelta

COOKIES_FILE = "/opt/.x-cookies.json"
QUEUE_FILE = "/opt/shared/scripts/breaking_queue.json"
POSTED_FILE = "/opt/shared/scripts/breaking_posted.json"
MAX_QUEUE = 100
REQUEST_DELAY = 20  # ç§’ï¼ˆ50 req/15min åˆ¶é™ã‚’å®ˆã‚‹ï¼‰

# =====================================================================
# å·¡å›å¯¾è±¡ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
# =====================================================================
MEDIA_ACCOUNTS = [
    # ===== æ—¥æœ¬èªãƒ¡ãƒ‡ã‚£ã‚¢ =====
    {"account": "nhk_news", "lang": "ja", "cat": "ç·åˆ"},
    {"account": "coin_post", "lang": "ja", "cat": "æš—å·è³‡ç”£"},
    {"account": "JpCointelegraph", "lang": "ja", "cat": "æš—å·è³‡ç”£"},
    {"account": "Nikkei", "lang": "ja", "cat": "çµŒæ¸ˆ"},
    {"account": "YahooNewsTopics", "lang": "ja", "cat": "ç·åˆ"},
    # ===== è‹±èª â€” ç·åˆãƒ‹ãƒ¥ãƒ¼ã‚¹ =====
    {"account": "Reuters", "lang": "en", "cat": "ç·åˆ"},
    {"account": "BBCWorld", "lang": "en", "cat": "ç·åˆ"},
    {"account": "AJEnglish", "lang": "en", "cat": "å›½éš›"},
    {"account": "guardian", "lang": "en", "cat": "ç·åˆ"},
    {"account": "WSJ", "lang": "en", "cat": "çµŒæ¸ˆ"},
    {"account": "AP", "lang": "en", "cat": "ç·åˆ"},
    {"account": "TIME", "lang": "en", "cat": "ç·åˆ"},
    # ===== è‹±èª â€” æ”¿æ²»ãƒ»åœ°æ”¿å­¦ =====
    {"account": "thehill", "lang": "en", "cat": "æ”¿æ²»"},
    {"account": "ForeignPolicy", "lang": "en", "cat": "åœ°æ”¿å­¦"},
    # ===== è‹±èª â€” ãƒ†ãƒƒã‚¯ãƒ»AI =====
    {"account": "techreview", "lang": "en", "cat": "AI"},
    {"account": "TechCrunch", "lang": "en", "cat": "AI"},
    # ===== è‹±èª â€” æš—å·è³‡ç”£ =====
    {"account": "CoinDesk", "lang": "en", "cat": "æš—å·è³‡ç”£"},
    {"account": "Cointelegraph", "lang": "en", "cat": "æš—å·è³‡ç”£"},
    {"account": "WatcherGuru", "lang": "en", "cat": "æš—å·è³‡ç”£"},
    # ===== è‹±èª â€” çµŒæ¸ˆ =====
    {"account": "CNBC", "lang": "en", "cat": "çµŒæ¸ˆ"},
    {"account": "FT", "lang": "en", "cat": "çµŒæ¸ˆ"},
    # ===== é€Ÿå ±ç³» =====
    {"account": "MarioNawfal", "lang": "en", "cat": "é€Ÿå ±"},
]

# =====================================================================
# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
# =====================================================================
KEYWORDS = {
    "AI": ["AI", "artificial intelligence", "LLM", "AGI", "ChatGPT", "Claude",
           "Gemini", "OpenAI", "Anthropic", "æ©Ÿæ¢°å­¦ç¿’", "ç”ŸæˆAI", "äººå·¥çŸ¥èƒ½",
           "neural network", "deep learning", "GPT", "automation"],
    "æš—å·è³‡ç”£": ["bitcoin", "BTC", "crypto", "blockchain", "Ethereum", "ETH",
                "stablecoin", "DeFi", "NFT", "Web3", "CBDC", "ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³",
                "æš—å·è³‡ç”£", "ä»®æƒ³é€šè²¨", "ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚³ã‚¤ãƒ³"],
    "åœ°æ”¿å­¦": ["geopolitics", "military", "Taiwan", "Ukraine", "sanctions",
              "NATO", "nuclear", "missile", "åœ°æ”¿å­¦", "å®‰å…¨ä¿éšœ", "è»äº‹",
              "åˆ¶è£", "ç´›äº‰", "æˆ¦äº‰"],
    "æ”¿æ²»": ["politics", "election", "president", "Trump", "congress",
            "legislation", "æ”¿æ²»", "é¸æŒ™", "æ”¿åºœ", "å¤§çµ±é ˜", "é¦–ç›¸"],
    "çµŒæ¸ˆ": ["economy", "inflation", "interest rate", "Fed", "GDP",
            "recession", "tariff", "çµŒæ¸ˆ", "é‡‘è", "ã‚¤ãƒ³ãƒ•ãƒ¬", "é‡‘åˆ©",
            "é–¢ç¨", "åŠå°ä½“"],
}

# ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆé–¾å€¤ï¼ˆæœ€ä½ã„ã„ã­æ•°ï¼‰
MIN_LIKES = 10


def score_tweet(text, cat):
    """ãƒ„ã‚¤ãƒ¼ãƒˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    score = 0
    text_lower = text.lower()

    for category, words in KEYWORDS.items():
        for word in words:
            if word.lower() in text_lower:
                score += 2 if category == cat else 1

    return score


def load_queue():
    """æ—¢å­˜ã‚­ãƒ¥ãƒ¼ã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(QUEUE_FILE):
        with open(QUEUE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []


def load_posted():
    """æŠ•ç¨¿æ¸ˆã¿ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(POSTED_FILE):
        with open(POSTED_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []


def save_queue(queue):
    """ã‚­ãƒ¥ãƒ¼ã‚’ä¿å­˜"""
    with open(QUEUE_FILE, "w", encoding="utf-8") as f:
        json.dump(queue, f, ensure_ascii=False, indent=2)


def is_duplicate(tweet_id, queue, posted):
    """é‡è¤‡ãƒã‚§ãƒƒã‚¯"""
    existing_ids = {item.get("tweet_id") for item in queue}
    posted_ids = {item.get("tweet_id") for item in posted}
    return tweet_id in existing_ids or tweet_id in posted_ids


async def collect_tweets(dry_run=False, max_queue=MAX_QUEUE):
    """ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆã‚’åé›†"""
    try:
        from twikit import Client
    except ImportError:
        print("ERROR: twikit ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install twikit ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    client = Client("en-US")

    if not os.path.exists(COOKIES_FILE):
        print(f"ERROR: Cookie ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {COOKIES_FILE}")
        sys.exit(1)

    client.load_cookies(COOKIES_FILE)
    print(f"ğŸ“‚ Cookie èª­ã¿è¾¼ã¿å®Œäº†")

    queue = load_queue()
    posted = load_posted()
    pending_count = len([q for q in queue if q.get("status") == "pending"])
    print(f"ğŸ“‹ ç¾åœ¨ã®ã‚­ãƒ¥ãƒ¼: {len(queue)} ä»¶ï¼ˆã†ã¡ pending: {pending_count}ï¼‰")

    candidates = []
    errors = 0

    for i, media in enumerate(MEDIA_ACCOUNTS):
        account = media["account"]
        lang = media["lang"]
        cat = media["cat"]

        print(f"  [{i+1}/{len(MEDIA_ACCOUNTS)}] @{account} ã‚’æ¤œç´¢ä¸­...")

        try:
            tweets = await client.search_tweet(
                f"from:{account}",
                "Latest",
                count=20
            )

            for tweet in tweets:
                tweet_id = str(tweet.id)

                if is_duplicate(tweet_id, queue, posted):
                    continue

                likes = getattr(tweet, "favorite_count", 0) or 0
                if likes < MIN_LIKES:
                    continue

                text = getattr(tweet, "text", "") or ""
                score = score_tweet(text, cat)

                # URLã‚’æŠ½å‡º
                urls = []
                if hasattr(tweet, "urls") and tweet.urls:
                    urls = [u for u in tweet.urls if u]

                screen_name = getattr(tweet.user, "screen_name", account) if hasattr(tweet, "user") else account
                tweet_url = f"https://x.com/{screen_name}/status/{tweet_id}"

                candidates.append({
                    "tweet_id": tweet_id,
                    "tweet_url": tweet_url,
                    "account": account,
                    "lang": lang,
                    "cat": cat,
                    "text": text[:500],
                    "likes": likes,
                    "retweets": getattr(tweet, "retweet_count", 0) or 0,
                    "replies": getattr(tweet, "reply_count", 0) or 0,
                    "score": score,
                    "article_urls": urls[:3],
                    "collected_at": datetime.now(timezone.utc).isoformat(),
                    "status": "pending",
                })

            print(f"    -> {len(tweets) if tweets else 0} ãƒ„ã‚¤ãƒ¼ãƒˆå–å¾—")

        except Exception as e:
            print(f"    -> ERROR: {e}")
            errors += 1

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å®ˆã‚‹
        if i < len(MEDIA_ACCOUNTS) - 1:
            await asyncio.sleep(REQUEST_DELAY)

    # ã‚¹ã‚³ã‚¢ + ã„ã„ã­æ•°ã§ã‚½ãƒ¼ãƒˆï¼ˆé«˜ã„é †ï¼‰
    candidates.sort(key=lambda x: (x["score"], x["likes"]), reverse=True)

    # ä¸Šé™ã¾ã§åˆ‡ã‚Šè©°ã‚
    slots_available = max_queue - pending_count
    new_items = candidates[:max(0, slots_available)]

    print()
    print(f"=== åé›†çµæœ ===")
    print(f"  å·¡å›ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ: {len(MEDIA_ACCOUNTS)}")
    print(f"  å€™è£œãƒ„ã‚¤ãƒ¼ãƒˆ: {len(candidates)}")
    print(f"  ã‚­ãƒ¥ãƒ¼è¿½åŠ : {len(new_items)}")
    print(f"  ã‚¨ãƒ©ãƒ¼: {errors}")

    if new_items:
        print()
        print("--- è¿½åŠ ã™ã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆï¼ˆä¸Šä½10ä»¶ï¼‰ ---")
        for item in new_items[:10]:
            print(f"  [{item['score']}ç‚¹ / {item['likes']}â¤ï¸] @{item['account']}: {item['text'][:80]}...")

    if not dry_run and new_items:
        queue.extend(new_items)
        save_queue(queue)
        print(f"\nâœ… {len(new_items)} ä»¶ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã—ã¾ã—ãŸ")
    elif dry_run:
        print(f"\nğŸ” dry-run ãƒ¢ãƒ¼ãƒ‰: ã‚­ãƒ¥ãƒ¼ã¸ã®æ›¸ãè¾¼ã¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")

    return new_items


def main():
    parser = argparse.ArgumentParser(description="X ãƒ„ã‚¤ãƒ¼ãƒˆåé›†ï¼ˆNowpattern Breaking Pipelineï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="åé›†ã®ã¿ã€ã‚­ãƒ¥ãƒ¼æ›¸ãè¾¼ã¿ãªã—")
    parser.add_argument("--max", type=int, default=MAX_QUEUE, help=f"æœ€å¤§ã‚­ãƒ¥ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {MAX_QUEUE}ï¼‰")
    args = parser.parse_args()

    asyncio.run(collect_tweets(dry_run=args.dry_run, max_queue=args.max))


if __name__ == "__main__":
    main()
