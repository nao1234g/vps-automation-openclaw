#!/usr/bin/env python3
"""
breaking_pipeline_helper.py â€” NEOãŒç”Ÿæˆã—ãŸè¨˜äº‹JSONã‚’Ghostã«æŠ•ç¨¿ + ã‚­ãƒ¥ãƒ¼æ›´æ–°

NEOãŒè¨˜äº‹ã‚’åˆ†æãƒ»åŸ·ç­†ã—ãŸå¾Œã€ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å‘¼ã³å‡ºã—ã¦:
1. è¨˜äº‹JSONã‹ã‚‰HTMLç”Ÿæˆï¼ˆnowpattern_article_builder.pyï¼‰
2. Ghost CMSã«æŠ•ç¨¿ï¼ˆnowpattern_publisher.pyï¼‰
3. breaking_queue.jsonã‚’æ›´æ–°ï¼ˆstatus â†’ article_readyï¼‰
4. Xå¼•ç”¨ãƒªãƒã‚¹ãƒˆç”¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜

ä½¿ã„æ–¹ï¼ˆNEOãŒå®Ÿè¡Œï¼‰:
  python3 breaking_pipeline_helper.py /tmp/article_12345.json
  python3 breaking_pipeline_helper.py /tmp/article_12345.json --dry-run

JSONå…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
{
  "tweet_id": "123456789",
  "title": "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«",
  "language": "ja",
  "why_it_matters": "ãªãœé‡è¦ã‹ï¼ˆ2-3æ–‡ï¼‰",
  "facts": [["ãƒ©ãƒ™ãƒ«", "å†…å®¹"], ...],
  "big_picture_history": "æ­´å²çš„èƒŒæ™¯ï¼ˆæ®µè½ãƒ†ã‚­ã‚¹ãƒˆï¼‰",
  "stakeholder_map": [["ã‚¢ã‚¯ã‚¿ãƒ¼", "å»ºå‰", "æœ¬éŸ³", "å¾—ã‚‹ã‚‚ã®", "ãƒªã‚¹ã‚¯"], ...],
  "data_points": [["æ•°å­—", "æ„å‘³"], ...],
  "dynamics_tags": "åŠ›å­¦ã‚¿ã‚° Ã— åŠ›å­¦ã‚¿ã‚°2",
  "dynamics_summary": "åŠ›å­¦ã®è¦ç´„",
  "dynamics_sections": [{"tag": "...", "subheader": "...", "lead": "...", "analysis": "..."}],
  "dynamics_intersection": "åŠ›å­¦ã®äº¤å·®åˆ†æ",
  "pattern_history": [{"year": 2020, "title": "...", "content": "...", "similarity": "..."}],
  "history_pattern_summary": "ãƒ‘ã‚¿ãƒ¼ãƒ³å²ã¾ã¨ã‚",
  "scenarios": [["æ¥½è¦³", "30%", "å†…å®¹", "ç¤ºå”†"], ["åŸºæœ¬", "50%", "...", "..."], ["æ‚²è¦³", "20%", "...", "..."]],
  "triggers": [["ãƒˆãƒªã‚¬ãƒ¼", "æ™‚æœŸ"], ...],
  "genre_tags": "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼",
  "event_tags": "è¦åˆ¶ãƒ»æ³•æ”¹æ­£",
  "source_urls": [["åå‰", "URL"], ...],
  "x_comment": "å¼•ç”¨ãƒªãƒã‚¹ãƒˆç”¨ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ200å­—ä»¥å†…ï¼‰"
}
"""

import json
import os
import sys
import argparse
from datetime import datetime, timezone

QUEUE_FILE = "/opt/shared/scripts/breaking_queue.json"
SCRIPTS_DIR = "/opt/shared/scripts"

# Ghostè¨­å®š
GHOST_URL = "https://nowpattern.com"


def load_env():
    """cron-env.sh ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€"""
    env_file = "/opt/cron-env.sh"
    if os.path.exists(env_file):
        with open(env_file, "r") as f:
            for line in f:
                line = line.strip()
                if line.startswith("export ") and "=" in line:
                    key_val = line[7:]  # remove "export "
                    key, val = key_val.split("=", 1)
                    val = val.strip('"').strip("'")
                    os.environ[key] = val


def load_queue():
    if os.path.exists(QUEUE_FILE):
        with open(QUEUE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []


def save_queue(queue):
    with open(QUEUE_FILE, "w", encoding="utf-8") as f:
        json.dump(queue, f, ensure_ascii=False, indent=2)


def build_html(article_data):
    """nowpattern_article_builder.py ã‚’ä½¿ã£ã¦HTMLç”Ÿæˆ"""
    sys.path.insert(0, SCRIPTS_DIR)
    from nowpattern_article_builder import build_deep_pattern_html

    # dynamics_sections ã«quotesãŒç„¡ã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿½åŠ 
    for section in article_data.get("dynamics_sections", []):
        if "quotes" not in section:
            section["quotes"] = []

    html = build_deep_pattern_html(
        title=article_data["title"],
        why_it_matters=article_data["why_it_matters"],
        facts=[tuple(f) for f in article_data["facts"]],
        big_picture_history=article_data.get("big_picture_history", ""),
        stakeholder_map=[tuple(s) for s in article_data.get("stakeholder_map", [])],
        data_points=[tuple(d) for d in article_data.get("data_points", [])],
        dynamics_tags=article_data.get("dynamics_tags", ""),
        dynamics_summary=article_data.get("dynamics_summary", ""),
        dynamics_sections=article_data.get("dynamics_sections", []),
        dynamics_intersection=article_data.get("dynamics_intersection", ""),
        pattern_history=article_data.get("pattern_history", []),
        history_pattern_summary=article_data.get("history_pattern_summary", ""),
        scenarios=[tuple(s) for s in article_data.get("scenarios", [])],
        triggers=[tuple(t) for t in article_data.get("triggers", [])],
        genre_tags=article_data.get("genre_tags", ""),
        event_tags=article_data.get("event_tags", ""),
        source_urls=[tuple(s) for s in article_data.get("source_urls", [])],
        related_articles=article_data.get("related_articles", []),
        diagram_html=article_data.get("diagram_html", ""),
        language=article_data.get("language", "ja"),
        # v4.0 Flywheel additions
        bottom_line=article_data.get("bottom_line", ""),
        bottom_line_pattern=article_data.get("bottom_line_pattern", ""),
        bottom_line_scenario=article_data.get("bottom_line_scenario", ""),
        bottom_line_watch=article_data.get("bottom_line_watch", ""),
        between_the_lines=article_data.get("between_the_lines", ""),
        open_loop_trigger=article_data.get("open_loop_trigger", ""),
        open_loop_series=article_data.get("open_loop_series", ""),
        prediction_id=article_data.get("prediction_id", ""),
    )
    return html


def publish_to_ghost(article_data, html, dry_run=False):
    """Ghost CMSã«æŠ•ç¨¿

    v3.0å¤‰æ›´ç‚¹:
    - ã‚¿ã‚°ã¯taxonomy.jsonã«åŸºã¥ã„ã¦STRICTãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    - ä¸æ­£ã‚¿ã‚°ãŒã‚ã‚Œã°æŠ•ç¨¿ã‚’ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆValueErrorã‚’é€å‡ºï¼‰
    - å®Ÿéš›ã®Ghost APIãƒ¬ã‚¹ãƒãƒ³ã‚¹URLã‚’ä½¿ç”¨ï¼ˆslug truncationé˜²æ­¢ï¼‰
    """
    sys.path.insert(0, SCRIPTS_DIR)
    from nowpattern_publisher import publish_deep_pattern, generate_article_id

    admin_api_key = os.environ.get("NOWPATTERN_GHOST_ADMIN_API_KEY", "")
    if not admin_api_key:
        print("ERROR: NOWPATTERN_GHOST_ADMIN_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return None

    article_id = generate_article_id("deep_pattern")
    lang = article_data.get("language", "ja")

    # ã‚¿ã‚°ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ï¼ˆåŒºåˆ‡ã‚Šæ–‡å­—: "/" or "Ã—"ï¼‰
    genre_list = [t.strip() for t in article_data.get("genre_tags", "").split("/") if t.strip()]
    event_list = [t.strip() for t in article_data.get("event_tags", "").split("/") if t.strip()]
    dynamics_list = [t.strip() for t in article_data.get("dynamics_tags", "").replace("Ã—", "/").split("/") if t.strip()]

    if dry_run:
        print(f"  [DRY-RUN] GhostæŠ•ç¨¿: {article_data['title']}")
        print(f"  [DRY-RUN] Article ID: {article_id}")
        print(f"  [DRY-RUN] Tags: {genre_list + event_list + dynamics_list}")
        return {
            "url": f"https://nowpattern.com/dry-run-{article_id}/",
            "article_id": article_id,
        }

    # publish_deep_pattern()ãŒSTRICTãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã€
    # ä¸æ­£ã‚¿ã‚°ãŒã‚ã‚Œã°ValueErrorã§æŠ•ç¨¿ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã™ã‚‹
    result = publish_deep_pattern(
        article_id=article_id,
        title=article_data["title"],
        html=html,
        genre_tags=genre_list,
        event_tags=event_list,
        dynamics_tags=dynamics_list,
        dynamics_tags_en=[],
        source_urls=[u[1] if isinstance(u, (list, tuple)) else u for u in article_data.get("source_urls", [])],
        related_article_ids=[],
        pattern_history_cases=article_data.get("pattern_history", []),
        word_count_ja=len(html) // 3,  # rough estimate
        title_en=article_data["title"] if lang == "en" else "",
        ghost_url=GHOST_URL,
        admin_api_key=admin_api_key,
        status="published",
        index_path="/opt/shared/scripts/nowpattern_article_index.json",
    )

    return result


def update_queue(tweet_id, ghost_url, x_comment, dry_run=False):
    """breaking_queue.json ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ article_ready ã«æ›´æ–°"""
    queue = load_queue()
    updated = False

    for item in queue:
        if item.get("tweet_id") == tweet_id:
            item["status"] = "article_ready"
            item["ghost_url"] = ghost_url
            item["x_comment"] = x_comment
            item["article_completed_at"] = datetime.now(timezone.utc).isoformat()
            updated = True
            break

    if updated and not dry_run:
        save_queue(queue)
        print(f"  âœ… ã‚­ãƒ¥ãƒ¼æ›´æ–°: tweet_id={tweet_id} â†’ article_ready")
    elif not updated:
        print(f"  âš ï¸ tweet_id={tweet_id} ãŒã‚­ãƒ¥ãƒ¼ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    return updated


def main():
    parser = argparse.ArgumentParser(description="Breaking Pipeline Helper â€” è¨˜äº‹JSON â†’ GhostæŠ•ç¨¿ + ã‚­ãƒ¥ãƒ¼æ›´æ–°")
    parser.add_argument("json_file", help="NEOãŒç”Ÿæˆã—ãŸè¨˜äº‹JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--dry-run", action="store_true", help="æŠ•ç¨¿ã›ãšã«ç¢ºèªã®ã¿")
    args = parser.parse_args()

    # ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
    load_env()

    # JSONèª­ã¿è¾¼ã¿
    if not os.path.exists(args.json_file):
        print(f"ERROR: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.json_file}")
        sys.exit(1)

    with open(args.json_file, "r", encoding="utf-8") as f:
        article_data = json.load(f)

    tweet_id = article_data.get("tweet_id", "")
    title = article_data.get("title", "ç„¡é¡Œ")
    x_comment = article_data.get("x_comment", "")

    print(f"ğŸ“ è¨˜äº‹å‡¦ç†é–‹å§‹: {title}")
    print(f"   tweet_id: {tweet_id}")

    # Step 0: v4.0 ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå…¬é–‹ã‚²ãƒ¼ãƒˆï¼‰
    print("  Step 0: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³...")
    try:
        sys.path.insert(0, SCRIPTS_DIR)
        from article_validator import validate_article
        is_valid, errors, warnings = validate_article(article_data)
        if not is_valid:
            print(f"  ğŸš« ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ä¸åˆæ ¼ â€” å…¬é–‹ãƒ–ãƒ­ãƒƒã‚¯")
            print(f"     ã‚¨ãƒ©ãƒ¼: {'; '.join(errors)}")
            print(f"     â†’ ARTICLE_FORMAT_SPEC.md ã‚’å‚ç…§ã—ã¦v4.0ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
            sys.exit(1)
        print(f"  â†’ ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³åˆæ ¼ âœ…")
    except ImportError:
        print(f"  âš ï¸ article_validator.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ ã‚¹ã‚­ãƒƒãƒ—ï¼‰")

    # Step 1: HTMLç”Ÿæˆ
    print("  Step 1: HTMLç”Ÿæˆä¸­...")
    try:
        html = build_html(article_data)
        print(f"  â†’ HTMLç”Ÿæˆå®Œäº†ï¼ˆ{len(html):,} bytesï¼‰")
    except Exception as e:
        print(f"  âŒ HTMLç”Ÿæˆå¤±æ•—: {e}")
        sys.exit(1)

    # Step 2: GhostæŠ•ç¨¿
    print("  Step 2: GhostæŠ•ç¨¿ä¸­...")
    try:
        result = publish_to_ghost(article_data, html, dry_run=args.dry_run)
        if result:
            ghost_url = result.get("url", "")
            print(f"  â†’ GhostæŠ•ç¨¿å®Œäº†: {ghost_url}")
        else:
            print("  âŒ GhostæŠ•ç¨¿å¤±æ•—")
            sys.exit(1)
    except Exception as e:
        print(f"  âŒ GhostæŠ•ç¨¿å¤±æ•—: {e}")
        sys.exit(1)

    # Step 3: ã‚­ãƒ¥ãƒ¼æ›´æ–°
    print("  Step 3: ã‚­ãƒ¥ãƒ¼æ›´æ–°ä¸­...")
    update_queue(tweet_id, ghost_url, x_comment, dry_run=args.dry_run)

    # Step 4: äºˆæ¸¬è¿½è·¡DBã«è¨˜éŒ²ï¼ˆv4.0 Flywheelï¼‰
    print("  Step 4: äºˆæ¸¬è¿½è·¡DBè¨˜éŒ²ä¸­...")
    try:
        from prediction_tracker import record_prediction, update_ghost_url
        prediction_id = record_prediction(args.json_file)
        if prediction_id and ghost_url:
            update_ghost_url(prediction_id, ghost_url)
            # è¨˜äº‹JSONã«prediction_idã‚’æ›¸ãæˆ»ã—ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«åæ˜ ç”¨ï¼‰
            article_data["prediction_id"] = prediction_id
        print(f"  â†’ äºˆæ¸¬è¨˜éŒ²å®Œäº†: {prediction_id}")
    except Exception as e:
        print(f"  âš ï¸ äºˆæ¸¬è¨˜éŒ²å¤±æ•—ï¼ˆè¨˜äº‹æŠ•ç¨¿ã«ã¯å½±éŸ¿ãªã—ï¼‰: {e}")

    print(f"\n=== å®Œäº† ===")
    print(f"  è¨˜äº‹: {title}")
    print(f"  URL: {ghost_url}")
    if article_data.get("prediction_id"):
        print(f"  Prediction ID: {article_data['prediction_id']}")
    print(f"  Xå¼•ç”¨ã‚³ãƒ¡ãƒ³ãƒˆ: {x_comment[:80]}...")


if __name__ == "__main__":
    main()
