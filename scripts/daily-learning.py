#!/usr/bin/env python3
"""
Hey Loop Intelligence System v3

4x daily intelligence gathering focused on BOTH infrastructure AND revenue.
Sends Telegram reports with article URLs, summaries, and monetization proposals.
Dynamically discovers new "information stars" â€” people making money with AI.

Data sources:
  1. Reddit (JSON API) â€” infrastructure + revenue subreddits
  2. Hacker News (Firebase API) â€” tech + business keywords
  3. GitHub (REST API) â€” dependency tracking + AI builder repos
  4. Gemini + Google Search grounding â€” deep research + dynamic discovery
  5. Grok/xAI (Chat API) â€” X/Twitter real-time intelligence

Schedule: 4x daily (every 6 hours)
  Run 0: 00:00 JST â€” Night scan (global markets, overnight news)
  Run 1: 06:00 JST â€” Morning briefing (main daily report)
  Run 2: 12:00 JST â€” Midday update (trending topics)
  Run 3: 18:00 JST â€” Evening review (summary + action items)

Usage:
  python3 daily-learning.py              # Auto-detect run based on JST hour
  python3 daily-learning.py --run 0      # Force specific run (0-3)
  python3 daily-learning.py --force      # Skip duplicate check
"""

import json
import os
import re
import sys
import time
import urllib.request
import urllib.error
from datetime import datetime, timezone, timedelta

# =============================================================================
# Config
# =============================================================================
LEARNING_DIR = "/opt/shared/learning"
WISDOM_FILE = "/opt/shared/AGENT_WISDOM.md"
GEMINI_API_URL = (
    "https://generativelanguage.googleapis.com/v1beta/models/"
    "gemini-2.5-flash:generateContent"
)
GROK_API_URL = "https://api.x.ai/v1/chat/completions"
TELEGRAM_API_URL = "https://api.telegram.org/bot{token}/sendMessage"

JST = timezone(timedelta(hours=9))

RUN_LABELS = {
    0: "Night Scan",
    1: "Morning Briefing",
    2: "Midday Update",
    3: "Evening Review",
}

# --- Subreddits (infrastructure + revenue) ---
INFRA_SUBREDDITS = [
    "selfhosted", "n8n", "docker", "PostgreSQL",
    "LocalLLaMA", "MachineLearning", "ClaudeAI",
    "Automate", "webdev", "netsec",
]

REVENUE_SUBREDDITS = [
    "AI_Agents", "SideProject", "EntrepreneurRideAlong",
    "passive_income", "newsletters", "SaaS",
    "indiehackers", "Entrepreneur", "startups",
    "juststart",
]

ALL_SUBREDDITS = INFRA_SUBREDDITS + REVENUE_SUBREDDITS

# --- GitHub repos (infrastructure + revenue/AI builders) ---
INFRA_GITHUB_REPOS = [
    "open-claw/open-claw",
    "n8n-io/n8n",
    "docker/compose",
    "langchain-ai/langchain",
    "anthropics/anthropic-sdk-python",
]

REVENUE_GITHUB_REPOS = [
    "joaomdmoura/crewAI",
    "langgenius/dify",
    "significant-gravitas/AutoGPT",
    "assafelovic/gpt-researcher",
    "Mintplex-Labs/anything-llm",
]

ALL_GITHUB_REPOS = INFRA_GITHUB_REPOS + REVENUE_GITHUB_REPOS

# --- Hacker News keywords (tech + business) ---
HN_KEYWORDS = [
    # Infrastructure
    "ai agent", "llm", "docker", "n8n", "postgres",
    "telegram bot", "self-hosted", "automation", "gemini",
    "claude", "grok", "vector database", "rag", "mcp", "open source",
    # Revenue / Business
    "newsletter", "content pipeline", "revenue", "startup",
    "saas", "monetize", "passive income", "indie hacker",
    "side project", "ai business", "pricing", "mrr",
    "subscription", "creator economy", "ai tool",
]

# --- Deep research topics (14 = ~3.5 day full rotation at 4x/day) ---
DEEP_TOPICS = [
    # Infrastructure (7)
    {
        "area": "AI Agent Architecture",
        "category": "infra",
        "search_query": (
            "multi-agent AI system architecture 2026 best practices coordination"
        ),
    },
    {
        "area": "Docker Security",
        "category": "infra",
        "search_query": (
            "Docker container security hardening 2026 non-root CVE"
        ),
    },
    {
        "area": "N8N Advanced Patterns",
        "category": "infra",
        "search_query": (
            "n8n workflow automation advanced patterns error handling 2026"
        ),
    },
    {
        "area": "LLM Cost Optimization",
        "category": "infra",
        "search_query": (
            "LLM API cost optimization prompt caching 2026 Gemini Claude"
        ),
    },
    {
        "area": "Content Automation Pipeline",
        "category": "infra",
        "search_query": (
            "AI newsletter automation pipeline Substack multilingual 2026"
        ),
    },
    {
        "area": "PostgreSQL Performance",
        "category": "infra",
        "search_query": (
            "PostgreSQL 16 17 performance tuning indexing 2026"
        ),
    },
    {
        "area": "Telegram Bot Best Practices",
        "category": "infra",
        "search_query": (
            "Telegram bot development python 2026 best practices webhook"
        ),
    },
    # Revenue (7)
    {
        "area": "AI Newsletter Revenue",
        "category": "revenue",
        "search_query": (
            "AI newsletter business revenue model 2026 Substack subscription "
            "earnings The Rundown Superhuman AI"
        ),
    },
    {
        "area": "AI Automation Agencies",
        "category": "revenue",
        "search_query": (
            "AI automation agency business model pricing clients 2026 "
            "revenue case study"
        ),
    },
    {
        "area": "AI SaaS Products",
        "category": "revenue",
        "search_query": (
            "AI SaaS product launch revenue MRR 2026 indie maker solo "
            "developer bootstrapped"
        ),
    },
    {
        "area": "Content Monetization Strategies",
        "category": "revenue",
        "search_query": (
            "AI content monetization multilingual newsletter sponsorship "
            "affiliate 2026 Asia"
        ),
    },
    {
        "area": "AI Builder Case Studies",
        "category": "revenue",
        "search_query": (
            "AI builder making money case study 2026 solo developer revenue "
            "journey transparent"
        ),
    },
    {
        "area": "Multilingual AI Content Business",
        "category": "revenue",
        "search_query": (
            "multilingual AI content creation translation business Asia "
            "Japan Korea 2026"
        ),
    },
    {
        "area": "AI Agent Marketplace",
        "category": "revenue",
        "search_query": (
            "AI agent marketplace selling bots automation service 2026 "
            "pricing gig economy"
        ),
    },
]

# --- X Watchlist: ç‰¹å®šã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’æ¯æ—¥ç›£è¦– (ãƒ•ã‚©ãƒ­ãƒ¼ä¸è¦ã€from:username ã§å–å¾—) ---
# ãƒ™ãƒ¼ã‚¹ãƒªã‚¹ãƒˆï¼ˆ50äººï¼‰+ å‹•çš„ç™ºè¦‹ãƒªã‚¹ãƒˆï¼ˆ/opt/shared/watchlist_dynamic.jsonï¼‰
# GrokãŒæ¯æœæ–°ã—ã„é«˜ã‚·ã‚°ãƒŠãƒ«ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ç™ºè¦‹ã—ã¦è‡ªå‹•è¿½åŠ ã™ã‚‹
X_WATCHLIST = {
    # â”€â”€â”€ æ—¥æœ¬èªAI/ãƒ†ãƒƒã‚¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "jp_ai_tech": [
        "issei_y",       # å±±æœ¬ä¸€æˆ / ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°CEO / è‡ªå‹•é‹è»¢
        "shaneguML",     # Shane Gu / Google DeepMind / Gemini
        "shanegJP",      # ã‚·ã‚§ã‚¤ãƒ³ãƒ»ã‚°ã‚¦ / Google DeepMind JP
        "kudotomoaki",   # å·¥è—¤æ™ºæ˜­ / JAPAN AI CEO
        "daiu_ko",       # Daiu Ko / Kudan CEO / ãƒ•ã‚£ã‚¸ã‚«ãƒ«AI
        "KudanNews",     # Kudanå…¬å¼
        "nishiohirokazu",# è¥¿å°¾æ³°å’Œ / Cybozu Labs / æŠ€è¡“ãƒ»AIç ”ç©¶
        "shi3z",         # æ¸…æ°´äº® / AIç ”ç©¶è€…ãƒ»èµ·æ¥­å®¶
        "yusuke_arclamp",# å‹ä¿£å“²ç”Ÿ / AIãƒ“ã‚¸ãƒã‚¹
    ],
    # â”€â”€â”€ æ—¥æœ¬èªãƒã‚¯ãƒ­/çµŒæ¸ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "jp_macro": [
        "yurumazu",      # ã‚¨ãƒŸãƒ³ãƒ»ãƒ¦ãƒ«ãƒã‚º / ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ãƒˆãƒ©ãƒ†ã‚¸ã‚¹ãƒˆ
        "goto_finance",  # å¾Œè—¤é”ä¹Ÿ / å…ƒæ—¥çµŒè¨˜è€… / çµŒæ¸ˆãƒ»æŠ•è³‡
        "ryuichirot",    # ç«¹ä¸‹éš†ä¸€éƒ / TBS Bloomberg PIVOT
        "kenkusunoki",   # æ¥ æœ¨å»º / ç«¶äº‰æˆ¦ç•¥
        "tanakayu6",     # ç”°ä¸­å®‡ / å›½éš›ãƒ‹ãƒ¥ãƒ¼ã‚¹ç‹¬ç«‹è§£èª¬
        "hidetomitanaka",# ç”°ä¸­ç§€è‡£ / ä¸Šæ­¦å¤§å­¦ / çµŒæ¸ˆæ”¿ç­–
    ],
    # â”€â”€â”€ ã‚°ãƒ­ãƒ¼ãƒãƒ«AIãƒˆãƒƒãƒ— (CEO/å‰µæ¥­è€…/æ„æ€æ±ºå®šå±¤) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "global_ai_leaders": [
        "sama",          # Sam Altman / OpenAI CEO
        "demishassabis", # Demis Hassabis / Google DeepMind CEO
        "satyanadella",  # Satya Nadella / Microsoft CEO
        "elonmusk",      # Elon Musk / xAIãƒ»Tesla
        "gdb",           # Greg Brockman / OpenAI
        "ylecun",        # Yann LeCun / Meta AI Chief Scientist
        "andrewyng",     # Andrew Ng / DeepLearning.AI
        "ilyasut",       # Ilya Sutskever / SSI
        "aidan_gomez",   # Aidan Gomez / Cohere CEO
        "garrytan",      # Garry Tan / YC President
        "paulg",         # Paul Graham / YC
        "naval",         # Naval Ravikant / AngelList
        "balajis",       # Balaji Srinivasan / network state
    ],
    # â”€â”€â”€ ã‚°ãƒ­ãƒ¼ãƒãƒ«AIãƒ“ãƒ«ãƒ€ãƒ¼ (MRRå…¬é–‹ãƒ»ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆç³»ãƒ»é«˜ã‚·ã‚°ãƒŠãƒ«) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "global_ai_builders": [
        "levelsio",      # Pieter Levels / æœˆåå…¬é–‹ / nomad.so
        "emollick",      # Ethan Mollick / Wharton / AIÃ—ãƒ“ã‚¸ãƒã‚¹å®Ÿè¨¼
        "karpathy",      # Andrej Karpathy / ex-OpenAI / æŠ€è¡“è§£èª¬æœ€é«˜å³°
        "steipete",      # Peter Steinberger / OpenClawä½œè€…
        "swyx",          # swyx / AI engineer trends / early signal
        "benedictevans", # Benedict Evans / ãƒ†ãƒƒã‚¯æ§‹é€ åˆ†æ
        "rowancheung",   # Rowan Cheung / AIãƒ„ãƒ¼ãƒ«ãƒ¬ãƒ“ãƒ¥ãƒ¼
        "therundownai",  # The Rundown AI / AIãƒ‹ãƒ¥ãƒ¼ã‚¹é›†ç´„
        "marc_louvion",  # Marc Lou / AI SaaS MRRå…¬é–‹
        "mattshumer_",   # Matt Shumer / AI product builder
        "andrewchen",    # Andrew Chen / a16z GP / growth
        "shreyas",       # Shreyas Doshi / PM / product strategy
        "david_perell",  # David Perell / Writing + AI newsletter
    ],
    # â”€â”€â”€ åœ°æ”¿å­¦ãƒ»ãƒã‚¯ãƒ­ (Nowpatternç›´çµ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    "global_geopolitics": [
        "ianbremmer",    # Ian Bremmer / Eurasia Groupå‰µæ¥­è€…
        "adam_tooze",    # Adam Tooze / Columbia / çµŒæ¸ˆæ­´å²å®¶
        "foreignpolicy", # Foreign Policy å…¬å¼
        "CFR_org",       # Council on Foreign Relations
        "rbrtstr",       # Robin Brooks / å›½éš›çµŒæ¸ˆãƒ»ãƒ‰ãƒ«
    ],
    # â”€â”€â”€ W1ãƒ•ã‚©ãƒ­ãƒ¼ãƒªã‚¹ãƒˆå…¨ä»¶ (ã‚ªãƒ¼ãƒŠãƒ¼ @w105743926 ã®ãƒ•ã‚©ãƒ­ãƒ¼ä¸€è¦§ã‚ˆã‚Š) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆé †ã«è¿½åŠ ä¸­ã€‚ã€Œã©ã‚“ã©ã‚“æ¸¡ã—ã¦ãã€â†’ éšæ™‚è¿½åŠ ã€‚
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãªã„ â€” å…¨ä»¶ç›£è¦–ã—ã¦GrokãŒæ„å‘³ã®ã‚ã‚‹æŠ•ç¨¿ã‚’åˆ¤å®šã™ã‚‹
    "w1_following": [
        # --- ãƒãƒƒãƒ1 (ã‚»ãƒƒã‚·ãƒ§ãƒ³åœ§ç¸®å¾©å…ƒåˆ†) ---
        "nikkeimj",          # æ—¥çµŒMJ / æ¶ˆè²»ãƒ»æµé€šãƒ»ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°
        "irtv2022",          # IR TV / æŠ•è³‡å®¶å‘ã‘æƒ…å ±
        "PeptiDream_Inc",    # PeptiDream / å‰µè–¬ãƒ™ãƒ³ãƒãƒ£ãƒ¼
        "hineken_al",        # ã²ã­ã‘ã‚“ / AI Ã— ãƒ“ã‚¸ãƒã‚¹
        "itandi_noguchi",    # ã‚¤ã‚¿ãƒ³ã‚¸ é‡å£ / ä¸å‹•ç”£DX
        "Q_Portal_",         # Qãƒãƒ¼ã‚¿ãƒ« / é‡å­ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿æƒ…å ±
        "quantinuum_jp",     # Quantinuum Japan
        "Geniee_inc",        # Geniee / DSPãƒ»DXãƒ»AI
        "quantumbizmag",     # Quantum Business Magazine
        "BridgeSalon",       # Bridge Salon / ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—æƒ…å ±
        "JapanStockC",       # Japan Stock Channel
        "nikkei_business",   # æ—¥çµŒãƒ“ã‚¸ãƒã‚¹
        "nikkei_bizdaily",   # æ—¥çµŒãƒ“ã‚¸ãƒã‚¹é›»å­ç‰ˆ
        "ReHacQ",            # ReHacQ / çµŒæ¸ˆãƒ»ãƒ“ã‚¸ãƒã‚¹YouTube
        "sa3i8te7n8",        # å±±ç”°é€²å¤ªéƒ (ãƒ¡ãƒ«ã‚«ãƒªå‰µæ¥­è€…)
        "GoogleDeepMind",    # Google DeepMind å…¬å¼
        "OpenAI",            # OpenAI å…¬å¼
        "cjhiking",          # CJ Hiking / èµ·æ¥­ãƒ»ãƒ†ãƒƒã‚¯
        "commu_blog",        # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ»ãƒ–ãƒ­ã‚°ç³»
        "BiotechMania",      # ãƒã‚¤ã‚ªãƒ†ãƒƒã‚¯ãƒãƒ‹ã‚¢
        "wired_jp",          # WIRED Japan
        "YoichiTakahashi",   # é«˜æ©‹æ´‹ä¸€ / çµŒæ¸ˆå­¦è€…ãƒ»å…ƒè²¡å‹™çœ
        "Kantei_Saigai",     # é¦–ç›¸å®˜é‚¸é˜²ç½
        "kantei_hisai",      # å®˜é‚¸è¢«ç½è€…æ”¯æ´
        "quick_cvrc",        # QUICKã‚³ãƒ¼ãƒãƒ¬ãƒ¼ãƒˆãƒãƒªãƒ¥ãƒ¼ç ”ç©¶ã‚»ãƒ³ã‚¿ãƒ¼
        "kabumatome",        # æ ªã¾ã¨ã‚ / æ ªå¼æŠ•è³‡æƒ…å ±
        "Investment_kabu",   # æŠ•è³‡ Ã— æ ª
        "stockprayer",       # ã‚¹ãƒˆãƒƒã‚¯ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ / æ ªå¼æŠ•è³‡
        "BioFinWizard",      # ãƒã‚¤ã‚ª Ã— é‡‘è
        "joshm",             # Josh Miller / Product Hunté–¢é€£
        "toyamarudasi",      # ã¨ã‚„ã¾ã‚‹å¤§å¿— / æŠ•è³‡ãƒ»æƒ…å ±
        "YassLab",           # YassLab / Railsãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãƒ»æ•™è‚²
        # --- ãƒãƒƒãƒ2 (ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆæä¾›åˆ† 2026-02-23) ---
        "money_eeexit",      # ã‚¿ãƒŠã‚« / å‰¯æ¥­èµ·æ¥­å®¶ / 10å„„Exit / ãƒãƒ¼ã‚±ä¼šç¤¾çµŒå–¶
        "jimmybajimmyba",    # Jimmy Ba / 100x / xAI co-founder @xai @uoft â˜…é«˜ã‚·ã‚°ãƒŠãƒ«
        "kei31ai",           # AIã‘ã„ã™ã‘ / AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼è§£èª¬ / Zennãƒ»note
        "MrinankSharma",     # mrinank / AI researcher
        "1namaiki",          # ãªã¾ã„ããã‚“ / ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç³»
        "rohit4verse",       # Rohit / FullStack + Agentic AI builder
        "L_go_mrk",          # AIé§†å‹•å¡¾ / ã‚¹ãƒ¢ãƒ“ã‚¸ã‚ªãƒ¼ãƒŠãƒ¼ / AIÃ—SaaS 10å€‹ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
        "aiyabai1219",       # AIã‚„ã°ã„ / AIÃ—å‹•ç”»ç·¨é›† / Antigravity Ã— Remotion
        "aiehon_aya",        # å¦–ç²¾ã‚¢ãƒ¼ãƒ¤ã•ã‚“ / AIå‹•ç”»ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ / è‘—æ›¸ã‚ã‚Š
        "unikoukokun",       # ãƒ¦ãƒ‹ã‚³ / AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–‹ç™º / å£²ä¸Š2.5å„„å††ãƒ»æŠ•è³‡6å„„å†† â˜…
        "openworkceo",       # Openwork CEO / The Agent Economy / $OPENWORK
        "MattPRD",           # Matt Schlicht / moltbook / TheoryForgeVC / YC W12
        "ryolu_",            # Ryo Lu / Cursor.aiãƒ»NotionHQãƒ»Stripeå‡ºèº« â˜…é«˜ã‚·ã‚°ãƒŠãƒ«
        "moriyorihayash1",   # æ—æ‹“æµ· / honkomaä»£è¡¨ / æ±å¤§è¾²å­¦éƒ¨ / èµ·æ¥­å®¶
        "shinkaron",         # è¦æ ¼å¤– / ç‹¬ç«‹ãƒ‹ãƒƒãƒå¸‚å ´ / æœˆå¹³å‡5000ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¢— â˜…
        "muhweb",            # muh / äº‹æ¥­å®¶ / Web3/ã‚²ãƒ¼ãƒ HoB / AIã‚µãƒ¼ãƒ“ã‚¹ / æ±äº¬â‡”ã‚¢ã‚¸ã‚¢
        "y_ruo1",            # ã‚†ã‚‹ãŠãã‚“ / AIè‡ªå‹•é‹ç”¨ã§æœˆ120ä¸‡ â˜…
        "jujulife7",         # ã˜ã‚…ã˜ã‚… / ãƒ©ã‚¤ãƒ•ãƒãƒƒã‚«ãƒ¼ / AIãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒ»è‹±èª
        "ck_novasphere",     # ãƒãƒ£ãƒ³ã‚­ãƒ§ãƒ¡ / NovaSphere / æœˆé¡98,000å††AIåºƒå‘Š â˜…
        "4610_hotel",        # ã©ç´ äººãƒ›ãƒ†ãƒ«å†å»ºè¨ˆç”» / 42ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒã‚ºã‚‰ã›ä¸­ â˜…
        "ashtom",            # Thomas Dohmke / EntireHQ / Former CEO @GitHub â˜…é«˜ã‚·ã‚°ãƒŠãƒ«
        "GrowAIHub",         # GrowAIHub / AIãƒ„ãƒ¼ãƒ«ãƒ»Threadsãƒ»Growthã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        "maxjaderberg",      # Max Jaderberg / IsomorphicLabs President / ex-DeepMind â˜…
        # --- ãƒãƒƒãƒ3 (ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆæä¾›åˆ† 2026-02-23 å…¨ä»¶ No.25ã€œ329, é‡è¤‡é™¤ã) ---
        # â”€â”€ AI/ãƒ†ãƒƒã‚¯ æµ·å¤– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "mustafasuleyman",   # Mustafa Suleyman / Microsoft AI CEO / The Coming Waveè‘—è€…
        "miramurati",        # Mira Murati / ThinkingMachines / ex-OpenAI CTO
        "OriolVinyalsML",    # Oriol Vinyals / VP Research GoogleDeepMind / Geminiå…±åŒãƒªãƒ¼ãƒ‰
        "polynoamial",       # Noam Brown / OpenAI / o3ãƒ»o1æ¨è«–ãƒ¢ãƒ‡ãƒ«å…±åŒé–‹ç™º
        "ch402",             # Chris Olah / @AnthropicAI / ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆè§£é‡ˆç ”ç©¶
        "DarioAmodei",       # Dario Amodei / Anthropic CEO
        "VahidK",            # Vahid Kazemi / ex-xAIãƒ»OpenAIãƒ»Appleãƒ»Google
        "giffmana",          # Lucas Beyer / Meta researcher / ex-OpenAI DeepMind
        "arankomatsuzaki",   # Aran Komatsuzaki / GPT-Jãƒ»LAION / AIç ”ç©¶è€…
        "rhythmrg",          # Rhythm Garg / AppliedCompute CTO / ex-OpenAI research
        "VictorTaelin",      # Taelin / Kind / Bend / HVM / Î»Calculus
        "rayhotate",         # Ray Hotate / xAI MTS / Stanford CS / ex-Goldman
        "Hidenori8Tanaka",   # Hidenori Tanaka / Harvard Physics of AI
        "DKokotajlo",        # Daniel Kokotajlo / AI safety
        "bioshok3",          # bioshok / AI Safetyãƒ»Alignmentãƒ»X-Risk / INODS Research
        "Dr_Singularity",    # Dr Singularity / Futurist / AGI/ASI by 2030
        "drfeifei",          # Fei-Fei Li / Stanford CS / WorldLabs CEO / ç©ºé–“AI
        "KelseyTuoc",        # Kelsey Piper / ã€ŒWe're not doomedã€AIæ¥½è¦³ä¸»ç¾©è€…
        "hokazuya",          # Hodachi / RAGOps / EZO LLMs
        "superforecaster",   # Good Judgment / Superforecasting
        "PTetlock",          # Philip Tetlock / Penn / ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ•ã‚©ãƒ¼ã‚­ãƒ£ã‚¹ãƒ†ã‚£ãƒ³ã‚°ç†è«–
        "wfrhatch",          # Warren Hatch / Good Judgment CEO
        "aileenlee",         # Aileen Lee / CowboyVCå‰µæ¥­è€… / coinedã€Œunicornã€
        "nrmehta",           # Nick Mehta / Gainsightå‰µæ¥­è€… / Vistaå£²å´
        "RajanAnandan",      # Rajan Anandan / Peak XV Partners (Sequoia India)
        "benthompson",       # Ben Thompson / Stratecheryè‘—è€…
        "WillHeaven",        # Will Heaven / Dyson Comms / ex-Spectator
        "TEDchris",          # Chris Anderson / TED Head
        "lexfridman",        # Lex Fridman / Podcast / ãƒ­ãƒœãƒƒãƒˆãƒ»äººé–“
        "RayDalio",          # Ray Dalio / Bridgewater å‰µæ¥­è€… / Principlesè‘—è€…
        "SteveMiran",        # Stephen Miran / FRBç†äº‹
        "rakyll",            # Jaana Dogan / Google SWE / APIs platform
        "joshwoodward",      # Josh Woodward / VP @Google @GeminiApp
        "OfficialLoganK",    # Logan Kilpatrick / @GoogleAIStudio / Gemini API
        "tom_doerr",         # Tom DÃ¶rr / GitHub reposãƒ»DSPyãƒ»agents / ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¬ã‚¿ãƒ¼
        "memU_ai",           # memU / agentic memory framework for LLMs
        "narendramodi",      # Narendra Modi / ã‚¤ãƒ³ãƒ‰é¦–ç›¸
        "tim_cook",          # Tim Cook / Apple CEO
        "sundarpichai",      # Sundar Pichai / Google & Alphabet CEO
        "BillGates",         # Bill Gates / Microsoftå…±åŒå‰µæ¥­è€…
        "realDonaldTrump",   # Donald J. Trump / 45th & 47th President
        "POTUS",             # President Donald J. Trump @POTUS å…¬å¼
        "snakajima",         # Satoshi Nakajima / GraphAI / MulmoCast / ãƒ¡ãƒ«ãƒã‚¬
        "BrandonKHill",      # Brandon K. Hill / btrax CEO / æ—¥ç±³ãƒ‡ã‚¶ã‚¤ãƒ³ä¼šç¤¾
        # â”€â”€ AI/ãƒ†ãƒƒã‚¯ å›½å†… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "ymatsuo",           # æ¾å°¾ è±Š / æ±å¤§æ•™æˆ / æ—¥æœ¬DLå”ä¼šç†äº‹é•·
        "Matsuo_Lab",        # æ±äº¬å¤§å­¦ æ¾å°¾ãƒ»å²©æ¾¤ç ”ç©¶å®¤ å…¬å¼
        "ImAI_Eruel",        # ä»Šäº•ç¿”å¤ª / GenesisAI CEO / JAISTå®¢å“¡æ•™æˆ
        "takahiroanno",      # å®‰é‡è²´åš / ãƒãƒ¼ãƒ ã¿ã‚‰ã„å…šé¦–ãƒ»å‚é™¢è­°å“¡ / AIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢èµ·æ¥­å®¶
        "Tebasaki_lab",      # æ‰‹ç¾½å…ˆ / å›½ç”£LLMé–‹ç™º / ZENå¤§å­¦ç‰¹å¾…ç”Ÿ
        "cumulo_autumn",     # ã‚ãå…ˆç”Ÿ / ShizukuAILabs / UCBerkeley PhD / ex-Meta
        "ozaken_AI",         # ãŠã–ã‘ã‚“ / AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ•™ç§‘æ›¸è‘—è€… / AICXå”ä¼šä»£è¡¨
        "kajikent",          # æ¢¶è°·å¥äºº / POSTSä»£è¡¨ / AIäº‹æ¥­ãƒ»ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆé¡§å•
        "snakehakase",       # ã™ã­ãƒ¼ãåšå£« / ãƒ­ãƒ¼ãƒ³ãƒ30å„„ / AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆx ãƒãƒ¼ã‚±
        "AI_masaou",         # ã¾ã•ãŠ / AIé§†å‹•é–‹ç™ºCEO / Web3000ä¸‡äººåˆ©ç”¨ / YouTube1.5ä¸‡
        "commte",            # ã‚³ãƒ ãƒ† / Claude Codeå®Ÿè·µ / izanami.devé‹å–¶
        "akihiro_genai",     # ã‚ãã²ã‚ / AIæ´»ç”¨ãƒ»Codexæƒ…å ±ã‚³ãƒŸãƒ¥1000å / Android Dev
        "santa128bit",       # Shinji Yamada / AI Agent Operator / Software Dev
        "sora19ai",          # ãã‚‰ / AgentSkills / 21æ­³èµ·æ¥­å®¶ / ä»¤å’Œã®è™ALL
        "AI_Studenttt",      # ã‚‹ã‚‹ã‚€ / AIé–‹ç™ºå¤§å­¦ç”Ÿ / Udemy BS / çˆ†é€Ÿé–‹ç™º
        "aoyama_code",       # é’å±± / AIã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ / AIÃ—SNS1000ä¸‡
        "genkai_syatikuu",   # ãƒ«ãƒŠ / AIã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ / AIÃ—SNS1000ä¸‡ / ã‚µã‚¤ãƒ‰FIRE
        "rich_armadillo",    # ã‚ã‚‹ã¾ã˜ã‚ / æ±å¤§AIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ / AIÃ—X880ä¸‡/å¹´
        "develogon0",        # ãƒ‡ãƒ™ãƒ­ã‚´ãƒ³ / AIè‡ªå‹•åŒ–ãƒ„ãƒ¼ãƒ«æœˆ200ä¸‡ / Fãƒ©ãƒ³å’ãƒ‹ãƒ¼ãƒˆ
        "nero_sansei",       # ã­ã‚ / AIæ´»ç”¨ã§æœˆ30ä¸‡æŒ¯ã‚Šè¾¼ã¾ã‚Œã‚‹ / ç¤¾ä¸ãƒ‹ãƒ¼ãƒˆ
        "y_ruo1",            # ã‚†ã‚‹ãŠãã‚“ / AIè‡ªå‹•é‹ç”¨ã§æœˆ120ä¸‡ â˜…ï¼ˆbatch2é‡è¤‡ç¢ºèªç”¨ã«ä¿æŒï¼‰
        "shota7180",         # æœ¨å†…ç¿”å¤§ / SHIFT AIä»£è¡¨ / æ—¥æœ¬æœ€å¤§AIã‚¹ã‚¯ãƒ¼ãƒ«3ä¸‡äºº
        "kawai_design",      # KAWAI / SHIFT AIãƒ‡ã‚¶ã‚¤ãƒ³éƒ¨é•· / AIÃ—ãƒ‡ã‚¶ã‚¤ãƒ³æœ¬è‘—è€…
        "SakuSaku23TOP8",    # ã‚µã‚¯ã‚µã‚¯ / ä¸­å­¦ç”ŸAIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢èµ·æ¥­å®¶ / East Ventures
        "quronekox",         # Quro / æ—¥è˜­ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—CTO / @wnb_community
        "_nogu66",           # nogu / Claude Codeãƒ»Agent SDKå¥½ã / SWE
        "taishiyade",        # Taishi / å€‹äººé–‹ç™ºæœˆ1000ä¸‡ / å…ƒSilicon Valley CTO
        "0317_hiroya",       # Hiroya Iizuka / Leversä»£è¡¨ / ex-CTO ex-åŒ»å¸« / Obsidian
        "qumaiu",            # ç†Šäº•æ‚  / ãƒ©ãƒ³ã‚¹ãƒ†ã‚£ã‚¢CEO / GEAR.indigoé–‹ç™º
        "labelmake",         # Kyohei / pdfme(4Kâ˜…) / å¤–è³‡ITã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
        "Shin_Engineer",     # Shin / YouTuber7ä¸‡äºº / Udemy24ä¸‡äºº / Next.jsæ›¸ç±
        "K8292288065827",    # Lofi boyå·æœ¬ç¿” / BuildKit / 10åˆ†ã§AIã‚µãƒ¼ãƒ“ã‚¹é–‹ç™º
        "azukiazusa9",       # azukiazusa / ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
        "yutakashino",       # Yuta Kashino / BakFoo / ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢èµ·æ¥­å®¶
        "Sikino_Sito",       # å¼ä¹ƒã‚·ãƒˆ / ä½œå®¶ãƒ»ä¸–ç•Œè¦³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆ / izanami Awardså—è³
        "tsuchi_ya_84",      # tsuchi_ya / macOS Native Developer / Solopreneur
        "shotovim",          # æ¾æ¿¤Vimmer / CyberAgent SWE / ObsidianÃ—AI
        "at_sushi_",         # é–€è„‡ æ•¦å¸ / Knowledge Sense CEO / æ±å¤§ / SWEå‹Ÿé›†ä¸­
        "usutaku_channel",   # usutaku / Michikusa CEO / AIç ”ä¿® / #AIæœ¨æ›œä¼š
        "iwashi86",          # iwashi / NTTãƒ‰ã‚³ãƒ¢ç”ŸæˆAIå‘¨ã‚Š / ã‚¨ãƒãƒ³ã‚¸ã‚§ãƒªã‚¹ãƒˆ
        "nwiizo",            # nwiizo / Software Developer
        "Aoi_genai",         # ã‚ãŠã„ / ç”ŸæˆAIç ”ä¿®ãƒ»é–‹ç™º / ä¸Šå ´ä¼æ¥­å–å¼•å¤šæ•° / ReHacQ MC
        "AiAircle34052",     # Aircle / å­¦ç”ŸAIã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ / ä½“é¨“ãƒ™ãƒ¼ã‚¹AIç™ºä¿¡
        "compassinai",       # AIæ™‚ä»£ã®ç¾…é‡ç›¤ / AGIâ†’ASIãƒ»è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç™ºä¿¡
        "yugen_matuni",      # ã¾ã¤ã«ãƒ / ç”ŸæˆAIãˆã°ã‚“ã˜ã‡ã‚Šã™ã¨ / ã‚¨ã‚¯ã‚¹ãƒ—ãƒ©ã‚¶
        "ai_Prompt_1144",    # ä¸ƒé‡Œä¿¡ä¸€ / ç”ŸæˆAIã‚»ãƒŸãƒŠãƒ¼550å›ãƒ»å‚åŠ 35ä¸‡äºº
        "tetumemo",          # ãƒ†ãƒ„ãƒ¡ãƒ¢ / AIå›³è§£Ã—æ¤œè¨¼ / Newsletterãƒ–ãƒ­ã‚¬ãƒ¼
        "proica1",           # ã·ã‚ã„ã‹ / AIå¤±æ¥­æ¯æ—¥æŠ•ç¨¿ / AGIãƒ»ã‚·ãƒ³ã‚®ãƒ¥ãƒ©ãƒªãƒ†ã‚£
        "Tsubame33785667",   # Tsubame / ã‚·ãƒ³ã‚®ãƒ¥ãƒ©ãƒªãƒ†ã‚£ãƒ»ã‚«ãƒ¼ãƒ„ãƒ¯ã‚¤ãƒ«æƒ…å ±
        "ai_lin_creation",   # LIN / æœ€æ–°AIåˆ†ã‹ã‚Šã‚„ã™ãè§£èª¬ / æ—©ç¨²ç”° / Cross AIå…±åŒå‰µæ¥­
        "chatgptair",        # ã‚ã‚‹ã‚‹ / ChatGPT Ã— AIãƒ„ãƒ¼ãƒ« ä¸€ç•ªã‚ã‹ã‚Šã‚„ã™ãç™ºä¿¡
        "suguruKun_ai",      # ã™ãã‚‹ / ChatGPTã‚¬ãƒå‹¢ / AIç ”ä¿®é–‹ç™ºä¼šç¤¾CEO
        "pop_ikeda",         # æ± ç”° æœ‹å¼˜ / ChatGPTæœ€å¼·ã®ä»•äº‹è¡“4ä¸‡éƒ¨
        "masahirochaen",     # ãƒãƒ£ã‚¨ãƒ³ / ãƒ‡ã‚¸ãƒ©ã‚¤ã‚ºCEO / AIæƒ…å ±æœ€é€Ÿç™ºä¿¡ / Geminié¡§å•
        "ctgptlb",           # AGIãƒ©ãƒœ / ChatGPTãƒ»Geminiãƒ»Claudeè§£èª¬
        "The_AGI_WAY",       # ãƒãƒ¤ã‚·ã‚·ãƒ¥ãƒ³ã‚¹ã‚± / ã‚´ãƒ¼ãƒ«ã‚·ãƒ¼ã‚¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ / #PPALä¸»å®°
        "keitaro_aigc",      # ã‘ã„ãŸã‚ã† / AIÃ—GASã§æ¥­å‹™æ”¹å–„ / Notionå¤§ä½¿ / skyworkå¤§ä½¿
        "dify_base",         # Dify Base / AXæƒ…å ±ç™ºä¿¡ãƒ»Difyã‚³ãƒ³ã‚µãƒ«AIé–‹ç™º
        "omluc_ai",          # å²¸ç”°å´‡å² / Omlucä»£è¡¨ / Difyã§ã¯ã˜ã‚ã‚‹ã€œè‘—è€…
        "genspark_japan",    # Genspark æ—¥æœ¬å…¬å¼ / All-in-one AIãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹
        "ai_database",       # AIDB / ç”ŸæˆAIãƒ»è«–æ–‡ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
        "skywork_ai_jp",     # Skywork æ—¥æœ¬å…¬å¼ / AI ã‚ªãƒ•ã‚£ã‚¹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        "d_1d2d",            # d / æµ·å¤–AIæƒ…å ±ã¾ã¨ã‚
        "ManusAI_JP",        # Manus æ—¥æœ¬å…¬å¼ (Meta) / æ±ç”¨AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        "ManusAI",           # Manus å…¬å¼ (Meta) è‹±èªç‰ˆ
        "GlbGPT",            # GlobalGPT / GPT-5ãƒ»Claudeãƒ»Soraãƒ»100+ AI toolsçµ±åˆ
        "arena",             # Arena.ai / LMArena / AIè©•ä¾¡ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
        "deepseek_ai",       # DeepSeek å…¬å¼
        "ChatGPTapp",        # ChatGPT å…¬å¼ @ChatGPTapp
        "OpenAIDevs",        # OpenAI Developers å…¬å¼
        "OpenAINewsroom",    # OpenAI Newsroom å…¬å¼
        "AnthropicAI",       # Anthropic å…¬å¼
        "claudeai",          # Claude å…¬å¼ @claudeai
        "MicrosoftAI",       # Microsoft AI å…¬å¼
        "metaai",            # AI at Meta å…¬å¼
        "openclaw",          # OpenClaw å…¬å¼ @openclaw
        "Remotion",          # Remotion / Make videos programmatically
        "moltbook",          # moltbook / OpenClaw bots & AI agents hang out
        "steipete",          # Peter Steinberger / ClawFather / @openclaw â˜…
        "obsdmd",            # Obsidian å…¬å¼
        # â”€â”€ ãƒ“ã‚¸ãƒã‚¹ãƒ»èµ·æ¥­ å›½å†… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "IHayato",           # ã‚¤ã‚±ãƒãƒ¤ / ãƒ†ãƒ¬ãƒ“ã‚¢ãƒ‹ãƒ¡ãƒ»AIã‚¢ãƒ‹ãƒ¡ä½œè€… / CryptoNinja
        "note_ai_mousigo",   # ã¾ãª / AIÃ—note 8ãƒ¶æœˆã§å£²ä¸Š2000ä¸‡ / ãƒ¡ãƒ³ã‚·ãƒ—300äºº
        "sako_brain",        # ã•ã“ç¤¾é•· / Brainä»£è¡¨ / åˆ©ç”¨è€…33ä¸‡äºº / å¹´å•†10å„„
        "gagarot200",        # ã‚¬ã‚¬ãƒ­ãƒƒãƒˆ / AIÃ—SNSæœˆ100ä¸‡ / ãƒ•ãƒªãƒ¼ãƒ©ãƒ³ã‚¹
        "koala_YouTube99",   # ã‚³ã‚¢ãƒ© / YouTubeç´¯è¨ˆ1.5å„„ / AIÃ—å¤–æ³¨åŒ–
        "Fujin_Metaverse",   # FujinAI / 1é€±é–“ã§AIã§å£²ä¸Š1000ä¸‡ / Opalè¬›åº§1ä½
        "ck_novasphere",     # ãƒãƒ£ãƒ³ã‚­ãƒ§ãƒ¡ / NovaSphere / æœˆé¡98,000å††AIåºƒå‘Š â˜…
        "smobijiman_sss",    # ã‚¹ãƒ¢ãƒ“ã‚¸ã¾ã‚“ / å¸æ³•è©¦é¨“åˆæ ¼â†’äº‹æ¥­å£²å´13å„„
        "bmr_sri",           # BMR ã‚¹ãƒ¢ãƒ¼ãƒ«ãƒ“ã‚¸ãƒã‚¹ç ”ç©¶æ‰€ / æœˆ100ä¸‡ã‚¹ãƒ¢ãƒ¼ãƒ«ãƒ“ã‚¸
        "milbon_",           # ã¿ã‚‹ã¼ã‚“ / å¤–è³‡ã‚³ãƒ³ã‚µãƒ«Ã—å‰¯æ¥­ / æœˆå•†1000ä¸‡
        "career_koumei",     # ã‚­ãƒ£ãƒªã‚¢å­”æ˜ / æ²–ç¸„ / å¹´é–“8å„„ã‚¤ãƒ³ãƒ— / X1å¹´5ä¸‡
        "fladdict",          # æ·±æ´¥ è²´ä¹‹ / THE GUILD / note CXO
        "minowanowa",        # ç®•è¼ªåšä»‹ / å¹»å†¬èˆç·¨é›†è€…ãƒ»ç¤¾é•·
        "Kohaku_NFT",        # ã“ã¯ã / AIç¤¾å“¡å®Ÿè£… / 18æ­³èµ·æ¥­ / Pikaãƒ»Haggsfieldã¨ææº
        "920raian",          # ãƒ©ã‚¤ã‚¢ãƒ³ / æ³•äºº4æœŸç›® / SNSÃ—ãƒˆãƒ¬ãƒ¼ãƒ‰
        "ceo_tommy1",        # ãƒˆãƒŸãƒ¼ / ãƒ‰ãƒã‚¤åœ¨ä½
        "0x__tom",           # Tom / ç”ŸæˆAI2ç¤¾ç›®èµ·æ¥­ / ãƒ‰ãƒã‚¤â†å¤§æ‰‹åºƒå‘Šå£²å´
        "kosuke_agos",       # Kosuke / noimos_AI / ãƒ¡ãƒ‡ã‚£ã‚¢å£²å´â†’150å„„ä¸Šå ´
        "Ryo_Ogawa70",       # å°å·å¶º / ã‚¿ã‚¤ãƒŸãƒ¼ä»£è¡¨å–ç· å½¹ / å°†æ£‹é€£ç›Ÿæ™®åŠæŒ‡å°å“¡
        "ozarnozarn",        # å°æ¾¤éš†ç”Ÿ / BoostCapital VC / JFAç†äº‹
        "tabbata",           # ç”°ç«¯ä¿¡å¤ªéƒ / ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ã‚¹ãƒˆå€‹äººæŠ•è³‡å®¶ / LINEãƒ¤ãƒ•ãƒ¼å…ƒå½¹å“¡
        "densetsufm",        # ä¼èª¬ãƒ©ã‚¸ã‚ª Podcast / ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—æ¥­ç•Œæœ¬éŸ³
        "suan_news",         # SUAN / ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã‚¢ãƒ³ãƒ†ãƒŠ
        "Ptaro_chan",         # ã´ãƒ¼ãŸã‚ / 40ä»£ã€œ / æœ¬æ¥­2100ä¸‡Ã—å‰¯æ¥­1500ä¸‡
        "happyyoshigi",      # ã‚ˆã—ã / AIæ™‚ä»£ã®ã‚­ãƒ£ãƒªã‚¢æˆ¦ç•¥ / SNS6ä¸‡
        "moto_recruit",      # moto / è»¢è·ã¨å‰¯æ¥­ã®ã‹ã‘ç®—è‘—è€… / HIREDä»£è¡¨
        "Kuniyuki119",       # ä»Šæ‘ é‚¦ä¹‹ / ãƒŠã‚¦ãƒ“ãƒ¬ãƒƒã‚¸ä¸Šå ´CEO / æ±äº¬ç§‘å­¦å¤§è¬›å¸«
        "norihiko_sasaki",   # ä½ã€…æœ¨ç´€å½¦ / PIVOT CEO
        "koji_gp",           # å±±æœ¬åº·äºŒ / å…‰é€šä¿¡å¸¸å‹™å‡ºèº« / ã‚¢ãƒªãƒãƒãƒãƒ¼ã‚±è¨­ç«‹
        "tsubasamizuguch",   # æ°´å£ç¿¼ / fonfun CEO / è‡ªå·±è³‡é‡‘ã§TOB / æ™‚ä¾¡ç·é¡10å„„â†’100å„„
        "shunkurosaki",      # é»’å´ä¿Š / PLEX CEO / 700å
        "ozawa_group",       # å°æ¾¤è¾°çŸ¢ / ä»¤å’Œã®è™ / æ—¥æœ¬ä¸€ã®å…ç«¥é¤Šè­·æ–½è¨­ç›®æ¨™
        "ShusukeTerada",     # å¯ºç”°ä¿®è¼” / Dual Bridge Capital / å…ƒç±³ç³»ã‚¢ãƒŠãƒªã‚¹ãƒˆ
        "naotomatsushita",   # æ¾ä¸‹ç›´äºº / ECçµŒå–¶æ”¯æ´æ©Ÿæ§‹ / Yahooèªå®šãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼
        "m_kumagai",         # ç†Šè°·æ­£å¯¿ / GMOã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‚°ãƒ«ãƒ¼ãƒ—ä»£è¡¨
        "hmikitani",         # ä¸‰æœ¨è°·æµ©å² / æ¥½å¤©ã‚°ãƒ«ãƒ¼ãƒ—CEO
        "takoratta",         # åŠå·å“ä¹Ÿ / Tably / GHOVC Founding Partner
        "takahashi_ntu",     # é«˜æ©‹å¼˜æ¨¹ / ReHacQ ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼ / tonari CEO
        "yuji_daisuki1",     # ãƒ ã‚µã‚µãƒ“ / JTCæ–°è¦äº‹æ¥­ãƒ»å•†å“ä¼ç”»
        "daigo_3_8",         # Daigo Yokota / StandBy / physical context is all you need
        "Jumpei_Mitsui",     # ä¸‰äº•æ·³å¹³ / ãƒ¬ã‚´èªå®šãƒ—ãƒ­ãƒ“ãƒ«ãƒ€ãƒ¼ä¸–ç•Œ24äºº / ç˜â†’æ±å¤§â†’è—å¤§
        "keyplayers",        # é«˜é‡ç§€æ• / ã‚­ãƒ¼ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚º / æŠ•è³‡å®Ÿç¸¾80ç¤¾è¶…
        "damadama777",       # é»’ç”°çœŸè¡Œ / ãƒ«ãƒ¼ã‚»ãƒ³ãƒˆãƒ‰ã‚¢ãƒ¼ã‚º / ãƒªã‚¯ãƒŠãƒ“NEXTç·¨é›†é•·
        "K_Ishi_AI",         # K.Ishi / EPFLå’ CSå°‚æ”» / ã‚­ãƒ£ãƒ¡ãƒ«ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼CTO
        "yuusaku_buddica",   # ä¸­é‡å„ªä½œ / BUDDICAä»£è¡¨ / ã€Œæˆé•·ä»¥å¤–å…¨ã¦æ­»ã€
        "Leon_hongo",        # æœ¬éƒ·ãƒ¬ã‚ªãƒ³ / ä¸Šå ´ä¼æ¥­æ¡ç”¨é¢æ¥2000äºº / è»¢è·
        "snakajima",         # Satoshi Nakajima / MulmoCast / ms-japanãƒã‚¤ã‚¯ãƒ­ã‚½ãƒ•ãƒˆå…ƒç¤¾é•·
        "moritaeiichi",      # ã‚‚ã‚Šã£ã—ãƒ¼ / çµ„ç¹”é–‹ç™ºé¡§å• / 25å¹´1000ç¤¾ / HRã‚¢ãƒ¯ãƒ¼ãƒ‰æœ€å„ªç§€
        "MasanoriKanda",     # ç¥ç”°æ˜Œå…¸ / çµŒå–¶ã‚³ãƒ³ã‚µãƒ« / éå¸¸è­˜ãªæˆåŠŸæ³•å‰‡è‘—è€…
        "Money_Massa",       # ãƒãƒƒã‚µ / æŠ•è³‡ã‚³ãƒ¼ãƒ / ç±³å›½å€‹åˆ¥æ ª2å€
        "suh_sunaneko",      # ã™ã… / PM & PdM / ã‚¢ã‚¯ã‚»ãƒ³ãƒãƒ¥ã‚¢å‡ºèº« / æ”¯æ´ä¼šç¤¾çµŒå–¶
        # â”€â”€ æŠ•è³‡ãƒ»é‡‘è â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "cissan_9984",       # cis / è³‡ç”£430å„„å††æ ªæŠ•è³‡å®¶
        "alljon12",          # ãƒã‚µãƒ‹ãƒ¼ / ç´”é‡‘èè³‡ç”£40å„„ãƒ‹ãƒ¼ãƒˆ / æˆé‡‘ç”Ÿæ´»
        "hakureifarm",       # äº”æœˆ / 250å„„å††æŠ•è³‡å®¶ / ãƒ˜ãƒƒã‚¸ãƒ•ã‚¡ãƒ³ãƒ‰ / ç«¶èµ°é¦¬ç”Ÿç”£ç‰§å ´
        "teslafan1201",      # ãƒ†ã‚¹ãƒ©è³‡æœ¬å®¶Plaid / TSLAÃ—PLTR / å†…ç§‘åŒ»å‰¯æ¥­
        "Yoshi0Mura",        # æ‘ä¸Šä¸–å½° / æ‘ä¸Šè²¡å›£ / ã€Œç”Ÿæ¶¯æŠ•è³‡å®¶ã€è‘—è€…
        "TakayamaJoe",       # Joe Takayama / ç±³å›½æ ªÃ—æš—å·è³‡ç”£Ã—ãƒã‚¯ãƒ­ / Backpack BD
        "Masa_Aug2020",      # Masa / å…ƒå¤–è³‡ç³»IBå½¹å“¡ / å†ã‚¨ãƒÃ—ä¸å‹•ç”£Ã—é‡‘è
        "nicosokufx",        # ã«ã“ãã / FX / é‡‘èå¸‚å ´å®Ÿæ³
        "ishiharajun",       # çŸ³åŸé †ï¼ˆè¥¿å±±å­å››éƒï¼‰/ FXãƒ»ãƒãƒ¼ã‚±ãƒƒãƒˆ
        "Market_Letter_",    # ç±³å›½å¸‚å ´ã“ã‚Œèª­ã‚“ã©ã‘ãƒ¡ãƒ¢
        "Barchart",          # Barchart / é‡‘èå¸‚å ´ãƒ„ãƒ¼ãƒ« / Stocksãƒ»Optionsãƒ»Futures
        "kiyohara_stock",    # æ¸…åŸæŠ•è³‡è¡“ç ”ç©¶æ‰€ / ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¯”ç‡æŠ•è³‡
        "DAIBAKUTO",         # DAIBAKUTO / 43å¹´å¤–è³‡ç³»é‡‘èâ†’FIRE / é«˜é…å½“æ ª
        "entry20210104",     # æ ªGPT / AIÃ—æŠ•è³‡ / æ±ºç®—åˆ†æã‚·ã‚¹ãƒ†ãƒ é–‹ç™º
        "toushi_kenshou",    # ã½ã“ãŸã‚“ / AIÃ—æŠ•è³‡å®¶ / è³‡ç”£5000ä¸‡
        "paurooteri",        # ãƒ‘ã‚¦ãƒ­ / ç”ŸæˆAI Ã— åŠå°ä½“ãƒ†ãƒƒã‚¯ä¼æ¥­note
        "hukugyootaku",      # å‰¯æ¥­ã‚ªã‚¿ã‚¯ã«ã‚ƒãµ / æœˆå500ã€œ1000ä¸‡
        "ASTS_SpaceMob",     # $ASTS SpaceMobile æƒ…å ±ãƒãƒ– / Since 2020
        "ASTS_Investors",    # AST Spacemobile investors
        "AST_SpaceMobile",   # AST SpaceMobile å…¬å¼ / å®‡å®™åŸºåœ°æºå¸¯é€šä¿¡
        "Defiantclient2",    # Kevin Chen / $ASTS $QS / economics theology
        "YasuNomu1",         # é‡æ‘æ³°ç´€ / UCãƒãƒ¼ã‚¯ãƒ¬ãƒ¼ç†è«–ç‰©ç†å­¦è€…
        "kenn",              # Kenn Ejima / Gista.js / Admit AI / ex-Quora Head JP
        # â”€â”€ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãƒ¡ãƒ‡ã‚£ã‚¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "BloombergJapan",    # Bloomberg Japan æ—¥æœ¬èªå…¬å¼
        "Bank_of_Japan_j",   # æ—¥æœ¬éŠ€è¡Œ å…¬å¼
        "TrumpPostsJA",      # ãƒˆãƒ©ãƒ³ãƒ—æ°ç™ºè¨€é€Ÿå ± / Truth Socialæœ€é€Ÿ
        "TrumpTrackerJP",    # ãƒˆãƒ©ãƒ³ãƒ—å¤§çµ±é ˜ãƒ‹ãƒ¥ãƒ¼ã‚¹ / ãƒˆãƒ©ãƒ³ãƒ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼
        "sputnik_jp",        # Sputnik æ—¥æœ¬ / å›½éš›ãƒ‹ãƒ¥ãƒ¼ã‚¹
        "turningpointjpn",   # TotalNewsWorld / ä¸–ç•Œã®æƒ…å ±
        "tkzwgrs",           # æ»æ²¢ã‚¬ãƒ¬ã‚½ / Twitterã®ä»Šã¾ã¨ã‚
        "ZARASOKU",          # ã–ã‚‰é€Ÿ / æ ªãƒ»ä»®æƒ³é€šè²¨ãƒ‹ãƒ¥ãƒ¼ã‚¹é€Ÿå ±
        "NazologyInfo",      # ãƒŠã‚¾ãƒ­ã‚¸ãƒ¼ / ç§‘å­¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ¡ãƒ‡ã‚£ã‚¢ / ç”Ÿãç‰©ãƒ»å®‡å®™
        "NIKKEIxTREND",      # æ—¥çµŒã‚¯ãƒ­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ / ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°
        "matchan_jp",        # æ¾å³¶ å€«æ˜ / WIREDæ—¥æœ¬ç‰ˆç·¨é›†é•·
        "WIRED",             # WIRED å…¬å¼ï¼ˆè‹±èªï¼‰
        "sutoroveli_news",   # ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹é€Ÿå ±
        "TechCrunch",        # TechCrunch å…¬å¼
        "VentureBeat",       # VentureBeat / å¤‰é©çš„ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼
        "thenextweb",        # TNW / The Next Web
        "engadget",          # Engadget / ãƒ†ãƒƒã‚¯ç³»ãƒ¡ãƒ‡ã‚£ã‚¢
        "PCMag",             # PCMag / 40å¹´ãƒ†ãƒƒã‚¯ãƒ¬ãƒ“ãƒ¥ãƒ¼
        "ForbesTech",        # Forbes Tech
        "ycombinator",       # Y Combinator å…¬å¼
        "bayareawriter",     # Mary Ann Azevedo / Crunchbaseè¨˜è€…
        "koder_dev",         # Koder / æµ·å¤–Teché€Ÿå ±
        "AInokuhaku",        # AIã®ç©ºç™½ / AIç¨¼ãæ–¹æ¯æ—¥ç™ºä¿¡
        "norihiko_sasaki",   # ä½ã€…æœ¨ç´€å½¦ / PIVOT CEOï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
        "GOROman",           # null-sensei / GOROman
        "EEL_PR",            # ç·¨é›†å·¥å­¦ç ”ç©¶æ‰€
        "isis_es",           # ã‚¤ã‚·ã‚¹ç·¨é›†å­¦æ ¡
        "kenjuman",          # å‰æ‘å …æ¨¹ / ç·¨é›†å·¥å­¦ç ”ç©¶æ‰€
        # â”€â”€ æ”¿æ²»ãƒ»æ³•å¾‹ãƒ»è¡Œæ”¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "haraeiji2",         # åŸè‹±å² / æ”¿ç­–å·¥æˆ¿ä»£è¡¨ / è¦åˆ¶æ”¹é©
        "satsukikatayama",   # ç‰‡å±±ã•ã¤ã / è‡ªæ°‘å…šå‚è­°é™¢è­°å“¡
        "ikegai",            # ç”Ÿè²ç›´äºº / ä¸€æ©‹å¤§æ•™æˆ / æƒ…å ±æ³•ãƒ»AIæ”¿ç­–
        "HiromitsuTakagi",   # é«˜æœ¨æµ©å…‰ / ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç ”ç©¶å“¡
        "Matsuo1984",        # æ¾å°¾å‰›è¡Œ / å¼è­·å£« / ç”ŸæˆAIã®æ³•å¾‹å®Ÿå‹™è‘—è€…
        "IB57185560",        # IBã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚° / ä¼æ¥­é˜²è¡› / å…ƒé‡æ‘è­‰åˆ¸
        "yoshitaka_kitao",   # åŒ—å°¾å‰å­ / SBIãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹ä»£è¡¨
        "noricoco",          # æ–°äº•ç´€å­ / æ±ãƒ­ãƒœ / ã€ŒAI vs. æ•™ç§‘æ›¸ãŒèª­ã‚ãªã„å­ã©ã‚‚ãŸã¡ã€
        "carecon_biz",       # æ£®ç”°æ˜‡ / ãƒªãƒ™ãƒ©ãƒ«ã‚³ãƒ³ã‚µãƒ«ä»£è¡¨ / ã‚­ãƒ£ãƒªã‚³ãƒ³
        "damadama777",       # é»’ç”°çœŸè¡Œ / ãƒ«ãƒ¼ã‚»ãƒ³ãƒˆãƒ‰ã‚¢ãƒ¼ã‚ºï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
        "takano_nara",       # é«˜é‡ã‚ã¤ã— / å…ƒè­¦è¦–åºåˆ‘äº‹ãƒ»å…ƒå¤–äº¤å®˜ / å±æ©Ÿç®¡ç†ä¼šç¤¾
        "cryps1s",           # DANÎ / CISO @OpenAI / ex-CISO @Palantir
        "ssomurice_local",   # å¼“æœˆæµå¤ª / æ”¿æ²»ãƒ»é‡‘èãƒ»ãƒ™ãƒƒã‚»ãƒ³ãƒˆæ¨ã—
        "monozukuritarou",   # ã‚‚ã®ã¥ãã‚Šå¤ªéƒ / è£½é€ æ¥­YouTuber35ä¸‡äºº
        "dennotai",          # å·é‚Šå¥å¤ªéƒ / LINEãƒ¤ãƒ•ãƒ¼ä¼šé•· / AIèµ·æ¥­äºˆå®š
        "narendramodi",      # Narendra Modi / ã‚¤ãƒ³ãƒ‰é¦–ç›¸ï¼ˆå†æ²ï¼‰
        # â”€â”€ ç ”ç©¶ãƒ»å­¦è¡“ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "singularity20xy",   # ã‚ã„ã‚·ãƒ³ã‚®ãƒ¥ãƒ©ãƒªãƒ†ã‚£ / ãƒ†ã‚¹ãƒ©å¼å‘¨æ³¢æ•°
        "namahoge",          # Naruya Kondo / æ±å¤§æ¨è–¦â†’æ¾å°¾ç ”â†’è½åˆç ” / æœªè¸AI
        "daigo_3_8",         # Daigo Yokota / StandBy / physical context is all you needï¼ˆå†æ²ï¼‰
        "TechRacho",         # TechRacho / ç¾å½¹SWEå‘ã‘æŠ€è¡“ãƒ–ãƒ­ã‚°
        "RailsGuidesJP",     # Railsã‚¬ã‚¤ãƒ‰ å…¬å¼
        "RailsTutorialJP",   # Railsãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« å…¬å¼
        "yasulab",           # å®‰å·è¦å¹³ / YassLab CEO / CoderDojo Japan
        # â”€â”€ ãã®ä»– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "1fCB3jDGh651022",   # Mook / æ—¥æœ¬åœ¨ä½éŸ“å›½äºº / Elon Muskå¥½ã
        "summer3919",        # ã²ã‚ãŸã¤ / æœ¬ã‚’èª­ã‚“ã§ç”Ÿãã¦ã„ã‚‹
        "nyanko_movies",     # ãƒ‹ãƒ£ãƒ³ã‚³ / æ˜ ç”»3000æœ¬ / 2.7å„„ã‚¤ãƒ³ãƒ—
        "ib_kiri",           # ğ“ğ“¶ğ“¸ğ“¬ğ“±ğ“²
        "IkawaMototaka",     # äº•å·æ„é«˜ / å¤§ç‹è£½ç´™å…ƒä¼šé•·
        "midorikawa_cyo",    # ãƒŸãƒ‰ãƒªã•ã‚“ / ã‚¢ãƒ©ãƒ•ã‚©ãƒ¼å©šæ´»
        "hebitigo",          # å›°æƒ‘bot
        "yosimuraya",        # å®¶ç³»ã˜ã‚ƒã±ã‚“ / å‰æ‘å®¶å…¬èªã‚¢ãƒ³ãƒã‚µãƒ€ãƒ¼
        "Tsubame33785667",   # Tsubame / ã‚·ãƒ³ã‚®ãƒ¥ãƒ©ãƒªãƒ†ã‚£ãƒ»ã‚«ãƒ¼ãƒ„ãƒ¯ã‚¤ãƒ«
        # â”€â”€ è¿½åŠ æ¼ã‚Œåˆ† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        "tyomateee",         # æœ€å¤šæƒ…å ±å±€ / ä¸–ç•Œã®è©±é¡Œãƒ»ã¾ã¨ã‚
        "slow_developer",    # Haider / together we build an intelligent future
        "kabutociti",        # æº€å·ä¸­å¤®éŠ€è¡Œ / çµŒæ¸ˆæƒ…å ±ã¾ã¨ã‚
        "toshimitsu_sowa",   # æ›½å’Œåˆ©å…‰ / äººæç ”ç©¶æ‰€ä»£è¡¨ / æ¡ç”¨é¢æ¥2ä¸‡äººä»¥ä¸Š
        "m_kac",             # ã‚¨ãƒ ã‚«ã‚¯ / æ›¸ç±è‘—è€…
        "ShinWorkout0207",   # Shin / ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ»ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³
    ],
}

# å‹•çš„ç™ºè¦‹ãƒªã‚¹ãƒˆã®ä¿å­˜å…ˆï¼ˆGrokãŒæ¯æœç™ºè¦‹â†’æ°¸ç¶šä¿å­˜â†’æ¬¡å›ã‹ã‚‰ç›£è¦–ï¼‰
DYNAMIC_WATCHLIST_PATH = "/opt/shared/watchlist_dynamic.json"

# --- Grok X/Twitter search queries (rotate per run) ---
# åç›Šæ•°å­—ã‚’å«ã‚€ã‚¯ã‚¨ãƒªã«çµã‚‹ï¼ˆçœºã‚ã‚‹ã ã‘ã®æœ‰åäººã¯é™¤å¤–ï¼‰
GROK_SEARCH_QUERIES = [
    (
        "Search X/Twitter for posts from the last 48 hours where people "
        "share concrete AI revenue numbers. Use queries like: "
        "(MRR OR ARR OR '$' OR revenue OR 'made money') AND (AI OR SaaS OR agent OR automation). "
        "Find posts with actual dollar amounts, subscriber counts, or client numbers."
    ),
    (
        "Search X/Twitter for posts from the last 48 hours about AI "
        "newsletter creators and content businesses sharing subscriber growth, "
        "revenue, and monetization. Find posts with real numbers like "
        "'hit $X MRR', 'X subscribers', 'earning $X/month'."
    ),
    (
        "Search X/Twitter for posts from solo developers or indie hackers "
        "in the last 48 hours: (launched OR 'just hit' OR 'reached') AND "
        "(MRR OR users OR subscribers OR revenue) AND (AI OR automation OR SaaS). "
        "Find people with real traction and concrete numbers."
    ),
    (
        "Search X/Twitter for posts in the last 48 hours about: "
        "new AI models released, API pricing changes, cost optimization tricks, "
        "or Claude/Gemini/Grok updates that could affect AI automation systems. "
        "Focus on breaking news with technical implications."
    ),
]


# =============================================================================
# API Keys
# =============================================================================
def get_api_key():
    """Get Gemini API key from environment or .env file."""
    key = os.environ.get("GOOGLE_API_KEY")
    if key:
        return key

    env_paths = ["/opt/openclaw/.env", "/opt/shared/.env"]
    for env_path in env_paths:
        if os.path.exists(env_path):
            with open(env_path, "r") as f:
                for line in f:
                    line = line.strip()
                    if line.startswith("GOOGLE_API_KEY="):
                        return line.split("=", 1)[1].strip().strip('"').strip("'")
    return None


def get_xai_api_key():
    """Get xAI API key from environment or .env files."""
    key = os.environ.get("XAI_API_KEY")
    if key:
        return key

    env_paths = [
        "/opt/openclaw/.env", "/opt/shared/.env", "/opt/.env",
        "/opt/claude-code-telegram/.env",
    ]
    for env_path in env_paths:
        if os.path.exists(env_path):
            with open(env_path, "r") as f:
                for line in f:
                    line = line.strip()
                    if line.startswith("XAI_API_KEY="):
                        return line.split("=", 1)[1].strip().strip('"').strip("'")
    return None


def get_telegram_config():
    """Get Telegram bot token and owner chat ID from Neo's .env."""
    token = None
    chat_id = None
    env_path = "/opt/claude-code-telegram/.env"
    if os.path.exists(env_path):
        with open(env_path, "r") as f:
            for line in f:
                line = line.strip()
                if line.startswith("TELEGRAM_BOT_TOKEN="):
                    token = line.split("=", 1)[1].strip().strip('"').strip("'")
                elif line.startswith("ALLOWED_USERS="):
                    chat_id = line.split("=", 1)[1].strip().strip('"').strip("'")
    return token, chat_id


# =============================================================================
# Dynamic Watchlist â€” è‡ªå‹•ç™ºè¦‹ã—ãŸã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’æ°¸ç¶šä¿å­˜
# =============================================================================
def load_dynamic_watchlist():
    """Load dynamically discovered accounts from JSON file.
    Returns a list of username strings (deduped with base list in caller).
    """
    if not os.path.exists(DYNAMIC_WATCHLIST_PATH):
        return []
    try:
        with open(DYNAMIC_WATCHLIST_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
        accounts = [e["username"] for e in data.get("discovered", []) if e.get("username")]
        print(f"  Dynamic watchlist: {len(accounts)} accounts loaded")
        return accounts
    except Exception as e:
        print(f"  Dynamic watchlist load error: {e}")
        return []


def save_dynamic_watchlist(new_discoveries):
    """Append newly discovered accounts to the persistent JSON file.
    Returns the number of new accounts actually added (deduped).
    """
    existing = []
    if os.path.exists(DYNAMIC_WATCHLIST_PATH):
        try:
            with open(DYNAMIC_WATCHLIST_PATH, "r", encoding="utf-8") as f:
                data = json.load(f)
            existing = data.get("discovered", [])
        except Exception:
            existing = []

    existing_usernames = {e["username"].lower() for e in existing}
    added = 0
    for d in new_discoveries:
        if d.get("username") and d["username"].lower() not in existing_usernames:
            existing.append(d)
            existing_usernames.add(d["username"].lower())
            added += 1

    # Keep newest 300 entries
    existing = existing[-300:]

    os.makedirs(os.path.dirname(DYNAMIC_WATCHLIST_PATH), exist_ok=True)
    with open(DYNAMIC_WATCHLIST_PATH, "w", encoding="utf-8") as f:
        json.dump(
            {"discovered": existing, "total": len(existing),
             "last_updated": datetime.now(JST).strftime("%Y-%m-%d %H:%M")},
            f, indent=2, ensure_ascii=False,
        )
    print(f"  Dynamic watchlist: +{added} new accounts saved (total: {len(existing)})")
    return added


def get_all_watchlist_accounts():
    """Return deduplicated list of all accounts (base + dynamic)."""
    all_accounts = []
    seen = set()
    for accounts in X_WATCHLIST.values():
        for a in accounts:
            if a.lower() not in seen:
                all_accounts.append(a)
                seen.add(a.lower())
    for a in load_dynamic_watchlist():
        if a.lower() not in seen:
            all_accounts.append(a)
            seen.add(a.lower())
    return all_accounts


# =============================================================================
# Source 1: Reddit
# =============================================================================
def fetch_reddit(subreddit, limit=10):
    """Fetch hot posts from a subreddit via JSON API."""
    url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit={limit}"
    req = urllib.request.Request(
        url,
        headers={
            "User-Agent": (
                "HeyLoopIntelligence/3.0 (daily research; non-commercial)"
            )
        },
    )
    try:
        with urllib.request.urlopen(req, timeout=15) as resp:
            data = json.loads(resp.read().decode("utf-8"))
            posts = []
            for child in data.get("data", {}).get("children", []):
                d = child["data"]
                if d.get("stickied"):
                    continue
                posts.append({
                    "title": d.get("title", ""),
                    "score": d.get("score", 0),
                    "comments": d.get("num_comments", 0),
                    "url": d.get("url", ""),
                    "permalink": "https://reddit.com" + d.get("permalink", ""),
                    "selftext": (d.get("selftext") or "")[:500],
                    "created": d.get("created_utc", 0),
                    "subreddit": subreddit,
                })
            return posts
    except Exception as e:
        print(f"  Reddit r/{subreddit} error: {e}")
        return []


def collect_reddit_data(run_number):
    """Collect posts from rotating subset of both infra and revenue subs."""
    day = datetime.now(JST).timetuple().tm_yday

    # Each run picks different subreddits: 3 infra + 3 revenue
    infra_start = ((day * 4 + run_number) * 3) % len(INFRA_SUBREDDITS)
    rev_start = ((day * 4 + run_number) * 3) % len(REVENUE_SUBREDDITS)

    todays_infra = [
        INFRA_SUBREDDITS[(infra_start + i) % len(INFRA_SUBREDDITS)]
        for i in range(3)
    ]
    todays_revenue = [
        REVENUE_SUBREDDITS[(rev_start + i) % len(REVENUE_SUBREDDITS)]
        for i in range(3)
    ]

    all_subs = todays_infra + todays_revenue
    print(f"  Infra subs: {', '.join(todays_infra)}")
    print(f"  Revenue subs: {', '.join(todays_revenue)}")

    all_posts = {}
    for sub in all_subs:
        posts = fetch_reddit(sub, limit=8)
        if posts:
            posts.sort(key=lambda p: p["score"], reverse=True)
            all_posts[sub] = posts[:5]
            print(f"  r/{sub}: {len(posts)} posts fetched")
        time.sleep(1.5)  # Respect rate limit

    return all_posts


# =============================================================================
# Source 2: Hacker News
# =============================================================================
def fetch_hn_top_stories(limit=30):
    """Fetch top stories from Hacker News, filtered by relevance."""
    url = "https://hacker-news.firebaseio.com/v0/topstories.json"
    try:
        with urllib.request.urlopen(url, timeout=15) as resp:
            story_ids = json.loads(resp.read().decode("utf-8"))[:limit]
    except Exception as e:
        print(f"  HN top stories error: {e}")
        return []

    relevant = []
    for sid in story_ids:
        try:
            item_url = f"https://hacker-news.firebaseio.com/v0/item/{sid}.json"
            with urllib.request.urlopen(item_url, timeout=10) as resp:
                item = json.loads(resp.read().decode("utf-8"))
                if not item:
                    continue
                title = (item.get("title") or "").lower()
                for kw in HN_KEYWORDS:
                    if kw in title:
                        relevant.append({
                            "title": item.get("title", ""),
                            "url": item.get("url", ""),
                            "score": item.get("score", 0),
                            "comments": item.get("descendants", 0),
                            "hn_url": (
                                f"https://news.ycombinator.com/item?id={sid}"
                            ),
                            "matched_keyword": kw,
                        })
                        break
        except Exception:
            continue
        time.sleep(0.1)

    relevant.sort(key=lambda x: x["score"], reverse=True)
    print(f"  HN: {len(relevant)} relevant stories from top {limit}")
    return relevant[:10]


# =============================================================================
# Source 3: GitHub Releases
# =============================================================================
def fetch_github_updates(run_number):
    """Check latest releases. Full check on run 1, quick check on others."""
    # Full repo check on morning run (1), subset on other runs
    if run_number == 1:
        repos = ALL_GITHUB_REPOS
    else:
        # Alternate between infra and revenue repos
        repos = (
            INFRA_GITHUB_REPOS if run_number % 2 == 0
            else REVENUE_GITHUB_REPOS
        )

    updates = []
    for repo in repos:
        url = f"https://api.github.com/repos/{repo}/releases/latest"
        req = urllib.request.Request(
            url,
            headers={
                "User-Agent": "HeyLoopIntelligence/3.0",
                "Accept": "application/vnd.github+json",
            },
        )
        try:
            with urllib.request.urlopen(req, timeout=10) as resp:
                data = json.loads(resp.read().decode("utf-8"))
                updates.append({
                    "repo": repo,
                    "tag": data.get("tag_name", ""),
                    "name": data.get("name", ""),
                    "published": data.get("published_at", ""),
                    "body": (data.get("body") or "")[:800],
                    "url": data.get("html_url", ""),
                })
                print(f"  GitHub {repo}: {data.get('tag_name', 'no release')}")
        except urllib.error.HTTPError as e:
            if e.code == 404:
                print(f"  GitHub {repo}: no releases")
            else:
                print(f"  GitHub {repo}: HTTP {e.code}")
        except Exception as e:
            print(f"  GitHub {repo}: {e}")
        time.sleep(0.5)

    return updates


# =============================================================================
# Source 4: Gemini + Google Search grounding + Dynamic Discovery
# =============================================================================
def query_gemini_with_search(api_key, search_topic, context_data):
    """Query Gemini with Google Search grounding for real-time analysis."""
    url = f"{GEMINI_API_URL}?key={api_key}"

    prompt = f"""You are a technology + business intelligence analyst for
the "Hey Loop" project â€” an AI system that uses AI to generate returns
far exceeding token costs.

**Topic**: {search_topic['area']}
**Category**: {search_topic['category']}
**Search focus**: {search_topic['search_query']}

Raw data collected from Reddit and Hacker News:

{context_data}

Provide analysis in this format:

## Key Findings (from web search)
- Real news, releases, announcements from the past 7 days
- Include source URLs for EVERY claim
- For revenue topics: include specific dollar amounts, subscriber counts, growth rates

## Reddit & HN Highlights
- Most important discussions from the data above
- Include original post URLs
- Flag any revenue/business model discussions

## Revenue Signals
- Any information about people making money in this space
- Business models that are working RIGHT NOW
- Pricing data, revenue numbers, growth metrics
- Who is succeeding and how

## Actionable for Our System
- Specific things we should implement, change, or investigate
- For each action: estimated effort and potential revenue impact
- Be concrete: "Do X because Y, expected result Z"

## Newly Discovered Sources
- 2-3 NEW people, blogs, accounts, or newsletters relevant to this topic
  that are actively sharing valuable insights RIGHT NOW
- Include their URLs and what makes them worth following
- Prefer people who share revenue numbers publicly

## Warnings
- Breaking changes, deprecations, security issues
- Market shifts that could affect our strategy

Keep it factual. Every claim needs a source URL. No speculation."""

    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "tools": [{"google_search": {}}],
        "generationConfig": {"maxOutputTokens": 8192, "temperature": 0.3},
    }

    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(
        url, data=data,
        headers={"Content-Type": "application/json"},
        method="POST",
    )

    try:
        with urllib.request.urlopen(req, timeout=120) as resp:
            result = json.loads(resp.read().decode("utf-8"))
            parts = result["candidates"][0]["content"]["parts"]
            text_parts = [p["text"] for p in parts if "text" in p]
            return "\n".join(text_parts)
    except urllib.error.HTTPError as e:
        error_body = e.read().decode("utf-8") if e.fp else "No details"
        print(f"  Gemini Search API error {e.code}: {error_body[:300]}")
        return None
    except Exception as e:
        print(f"  Gemini Search error: {e}")
        return None


# =============================================================================
# Source 5: Grok/xAI â€” X/Twitter real-time search
# =============================================================================
def search_x_via_grok(xai_key, run_number):
    """Use Grok API to search X/Twitter for real-time intelligence."""
    query = GROK_SEARCH_QUERIES[run_number % len(GROK_SEARCH_QUERIES)]

    payload = {
        "model": "grok-3",
        "messages": [
            {
                "role": "system",
                "content": (
                    "You are an X/Twitter intelligence analyst. Search for "
                    "recent posts and threads. Always include the @username, "
                    "post content summary, engagement metrics if visible, "
                    "and the post URL. Focus on posts with real numbers "
                    "(revenue, subscribers, growth rates)."
                ),
            },
            {
                "role": "user",
                "content": (
                    f"{query}\n\n"
                    "Return the top 5 most relevant and recent posts/threads. "
                    "Format each as:\n"
                    "- @username: [summary] (likes/retweets if visible)\n"
                    "  URL: [post URL]\n"
                    "  Revenue signal: [any concrete numbers mentioned]\n"
                ),
            },
        ],
        "temperature": 0.3,
        "max_tokens": 2048,
    }

    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(
        GROK_API_URL,
        data=data,
        headers={
            "Content-Type": "application/json",
            "Authorization": f"Bearer {xai_key}",
        },
        method="POST",
    )

    try:
        with urllib.request.urlopen(req, timeout=60) as resp:
            result = json.loads(resp.read().decode("utf-8"))
            content = result["choices"][0]["message"]["content"]
            print(f"  Grok X search: {len(content)} chars")
            return content
    except urllib.error.HTTPError as e:
        error_body = e.read().decode("utf-8") if e.fp else "No details"
        print(f"  Grok API error {e.code}: {error_body[:300]}")
        return None
    except Exception as e:
        print(f"  Grok X search error: {e}")
        return None


# =============================================================================
# Source 5b: X Watchlist â€” ãƒ™ãƒ¼ã‚¹+å‹•çš„ãƒªã‚¹ãƒˆå…¨ä»¶ã‚’æ¯æœç›£è¦–
# =============================================================================
def search_x_watchlist_via_grok(xai_key):
    """Search all watchlist accounts (base list + dynamically discovered).

    Combines X_WATCHLIST (static) + watchlist_dynamic.json (auto-discovered).
    No X API subscription needed â€” Grok's internal X access handles it.
    Grokã«æ¸¡ã›ã‚‹from:ã‚¯ã‚¨ãƒªã¯é•·ã•åˆ¶é™ãŒã‚ã‚‹ãŸã‚æœ€å¤§100ä»¶ã‚’ãƒãƒƒãƒå‡¦ç†ã€‚
    """
    all_accounts = get_all_watchlist_accounts()

    # from: ã‚¯ã‚¨ãƒªãŒé•·ã™ãã‚‹ã¨GrokãŒåˆ‡ã‚Šæ¨ã¦ã‚‹ã®ã§æœ€å¤§100ä»¶ã«åˆ†å‰²
    batch_size = 100
    batches = [all_accounts[i:i + batch_size]
               for i in range(0, len(all_accounts), batch_size)]

    combined_results = []
    for batch_num, batch in enumerate(batches):
        from_query = " OR ".join(f"from:{a}" for a in batch)
        payload = {
            "model": "grok-3",
            "messages": [
                {
                    "role": "system",
                    "content": (
                        "You are an X/Twitter intelligence analyst monitoring a "
                        "curated list of AI builders, researchers, economists, "
                        "and market strategists (Japan + Global). "
                        "Surface the highest-signal posts only. "
                        "Always include @username, summary, and post URL."
                    ),
                },
                {
                    "role": "user",
                    "content": (
                        f"Search X/Twitter for the most insightful posts from "
                        f"the last 48 hours by these accounts:\n{from_query}\n\n"
                        "Prioritize:\n"
                        "1. AI revenue milestones (MRR, ARR, user numbers)\n"
                        "2. Geopolitical/macro analysis (Japan, Asia, US, global)\n"
                        "3. AI model releases or API changes with real impact\n"
                        "4. Original insights â€” NOT retweets of others' content\n\n"
                        "Return the top 8 most valuable posts:\n"
                        "- @username: [one-line summary]\n"
                        "  URL: [post URL]\n"
                        "  Signal: [concrete number or key insight]\n"
                    ),
                },
            ],
            "temperature": 0.3,
            "max_tokens": 2048,
        }
        data = json.dumps(payload).encode("utf-8")
        req = urllib.request.Request(
            GROK_API_URL, data=data,
            headers={"Content-Type": "application/json",
                     "Authorization": f"Bearer {xai_key}"},
            method="POST",
        )
        try:
            with urllib.request.urlopen(req, timeout=60) as resp:
                result = json.loads(resp.read().decode("utf-8"))
                content = result["choices"][0]["message"]["content"]
                print(
                    f"  Grok Watchlist batch {batch_num + 1}/{len(batches)}: "
                    f"{len(content)} chars ({len(batch)} accounts)"
                )
                combined_results.append(content)
        except urllib.error.HTTPError as e:
            error_body = e.read().decode("utf-8") if e.fp else "No details"
            print(f"  Grok Watchlist batch {batch_num + 1} error {e.code}: "
                  f"{error_body[:200]}")
        except Exception as e:
            print(f"  Grok Watchlist batch {batch_num + 1} error: {e}")
        if batch_num < len(batches) - 1:
            time.sleep(2)  # Rate limiting between batches

    if not combined_results:
        return None

    total = len(all_accounts)
    header = (
        f"[Watchlist: {total} accounts monitored "
        f"(base:{sum(len(v) for v in X_WATCHLIST.values())} "
        f"+ dynamic:{total - sum(len(v) for v in X_WATCHLIST.values())})]\n\n"
    )
    return header + "\n\n---\n\n".join(combined_results)


# =============================================================================
# Source 5c: X è‡ªå‹•ç™ºè¦‹ â€” æ¯æœæ–°ã—ã„é«˜ã‚·ã‚°ãƒŠãƒ«ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ç™ºè¦‹ã—ã¦ä¿å­˜
# =============================================================================
def discover_new_x_accounts_via_grok(xai_key, current_accounts):
    """Ask Grok to suggest new X accounts worth monitoring.

    Runs once per morning (Run 1). Parses @username lines and saves to
    watchlist_dynamic.json. Next morning these accounts are automatically
    included in search_x_watchlist_via_grok().
    """
    # Show first 50 of current list to avoid prompt bloat
    existing_sample = " ".join(f"@{a}" for a in current_accounts[:50])

    payload = {
        "model": "grok-3",
        "messages": [
            {
                "role": "system",
                "content": (
                    "You are building a curated X/Twitter intelligence watchlist. "
                    "Discover accounts actively posting high-signal content about "
                    "AI business revenue, geopolitics, and macroeconomics. "
                    "Focus on accounts that share concrete data, not just opinions."
                ),
            },
            {
                "role": "user",
                "content": (
                    f"Already monitoring (partial list): {existing_sample}\n\n"
                    "Based on X/Twitter activity in the last 7 days, suggest "
                    "10 NEW accounts I should monitor. Strict criteria:\n"
                    "1. AI builders publicly sharing MRR/ARR/revenue or product "
                    "launch traction\n"
                    "2. Geopolitics/macro analysts (Asia focus preferred) posting "
                    "original structural analysis\n"
                    "3. Emerging voices: under 300K followers but consistently "
                    "high-signal\n"
                    "4. Japanese AI/business accounts not widely known outside Japan\n\n"
                    "IMPORTANT: Respond in EXACTLY this format, one per line:\n"
                    "@username | Display Name | category | reason (one sentence)\n"
                    "(category: ai_builder / geopolitics / ai_research / jp_media / macro)\n"
                    "No preamble. No explanations outside this format."
                ),
            },
        ],
        "temperature": 0.6,
        "max_tokens": 1024,
    }
    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(
        GROK_API_URL, data=data,
        headers={"Content-Type": "application/json",
                 "Authorization": f"Bearer {xai_key}"},
        method="POST",
    )
    try:
        with urllib.request.urlopen(req, timeout=60) as resp:
            result = json.loads(resp.read().decode("utf-8"))
            content = result["choices"][0]["message"]["content"]
            print(f"  Grok Discovery raw: {len(content)} chars")
    except urllib.error.HTTPError as e:
        error_body = e.read().decode("utf-8") if e.fp else "No details"
        print(f"  Grok Discovery API error {e.code}: {error_body[:200]}")
        return []
    except Exception as e:
        print(f"  Grok Discovery error: {e}")
        return []

    # Parse "@username | Display Name | category | reason" lines
    discoveries = []
    today = datetime.now(JST).strftime("%Y-%m-%d")
    for line in content.split("\n"):
        line = line.strip()
        m = re.match(
            r"@(\w+)\s*\|\s*([^|]+)\s*\|\s*(\w[\w_]*)\s*\|\s*(.+)",
            line,
        )
        if m:
            discoveries.append({
                "username": m.group(1),
                "display_name": m.group(2).strip(),
                "category": m.group(3).strip(),
                "reason": m.group(4).strip(),
                "added_date": today,
                "source": "grok_discovery",
            })
    print(f"  Grok Discovery: parsed {len(discoveries)} new accounts")
    return discoveries


# =============================================================================
# Report Formatting
# =============================================================================
def _load_polymarket_data():
    """Load Polymarket snapshot + alerts prepared by polymarket_monitor.py.

    The monitor runs 5 min before each Hey Loop via cron.
    Files: /opt/shared/polymarket/latest_snapshot.json, alerts.json
    """
    snapshot_path = "/opt/shared/polymarket/latest_snapshot.json"
    alerts_path = "/opt/shared/polymarket/alerts.json"

    snapshot = {}
    alerts = []

    try:
        if os.path.exists(snapshot_path):
            with open(snapshot_path, "r", encoding="utf-8") as f:
                snapshot = json.load(f)
    except (json.JSONDecodeError, IOError):
        pass

    try:
        if os.path.exists(alerts_path):
            with open(alerts_path, "r", encoding="utf-8") as f:
                alerts = json.load(f)
    except (json.JSONDecodeError, IOError):
        pass

    if not snapshot:
        return None

    lines = [
        "### Polymarket Prediction Markets",
        f"Active markets tracked: {len(snapshot)}",
        "",
    ]

    if alerts:
        lines.append(f"Significant odds movements: {len(alerts)}")
        for a in alerts[:5]:
            if a.get("type") == "movement":
                lines.append(
                    f"  - {a.get('question', '?')[:60]}: "
                    f"{a.get('outcome', '?')} "
                    f"{a.get('prev_prob', 0)*100:.0f}% -> "
                    f"{a.get('curr_prob', 0)*100:.0f}% "
                    f"({a.get('delta', 0)*100:+.1f}%)"
                )
        lines.append("")

    # Top 10 by volume
    lines.append("Top markets by volume:")
    items = sorted(snapshot.values(), key=lambda x: x.get("volume", 0), reverse=True)
    for item in items[:10]:
        title = item.get("title", "?")[:55]
        vol = item.get("volume", 0)
        genres = [g.get("name_en", "") for g in item.get("genres", [])]

        markets = item.get("markets", {})
        if markets:
            first_m = next(iter(markets.values()))
            prices = first_m.get("prices", {})
            odds_str = " | ".join(
                f"{k}={v*100:.0f}%" for k, v in list(prices.items())[:3]
            )
        else:
            odds_str = ""

        lines.append(f"  ${vol/1e6:.1f}M | {title}")
        if odds_str:
            lines.append(f"         {odds_str}")
        if genres:
            lines.append(f"         [{', '.join(genres)}]")

    return "\n".join(lines)


def format_raw_data(reddit_data, hn_data, github_data, x_data):
    """Format raw collected data into readable context for Gemini."""
    sections = []

    if reddit_data:
        sections.append("### Reddit Posts")
        for sub, posts in reddit_data.items():
            is_revenue = sub in REVENUE_SUBREDDITS
            tag = "[REVENUE]" if is_revenue else "[INFRA]"
            for p in posts:
                sections.append(
                    f"- {tag} r/{sub}: \"{p['title']}\" "
                    f"(score:{p['score']}, comments:{p['comments']})"
                )
                sections.append(f"  URL: {p['permalink']}")
                if p["selftext"]:
                    sections.append(f"  Summary: {p['selftext'][:200]}")

    if hn_data:
        sections.append("\n### Hacker News Top Stories")
        for s in hn_data:
            sections.append(
                f"- \"{s['title']}\" (score:{s['score']}, "
                f"comments:{s['comments']}) [{s['matched_keyword']}]"
            )
            sections.append(f"  Article: {s['url']}")
            sections.append(f"  Discussion: {s['hn_url']}")

    if github_data:
        sections.append("\n### GitHub Releases")
        for g in github_data:
            if g["tag"]:
                sections.append(
                    f"- {g['repo']} -> {g['tag']} ({g['published'][:10]})"
                )
                sections.append(f"  URL: {g['url']}")
                if g["body"]:
                    sections.append(f"  Notes: {g['body'][:300]}")

    if x_data:
        sections.append("\n### X/Twitter Intelligence (via Grok)")
        sections.append(x_data)

    # Polymarket prediction markets (data prepared by polymarket_monitor.py cron)
    polymarket_text = _load_polymarket_data()
    if polymarket_text:
        sections.append("\n" + polymarket_text)

    return "\n".join(sections) if sections else "(No raw data collected)"


# =============================================================================
# Report Saving
# =============================================================================
def save_report(run_number, topic, raw_data_text, analysis, reddit_data,
                hn_data, github_data, x_data, date_str):
    """Save the intelligence report with run number."""
    os.makedirs(LEARNING_DIR, exist_ok=True)

    area_slug = topic["area"].lower().replace(" ", "-").replace("&", "and")
    filename = f"{date_str}_run{run_number}_{area_slug}.md"
    filepath = os.path.join(LEARNING_DIR, filename)

    run_label = RUN_LABELS.get(run_number, f"Run {run_number}")

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(f"# Hey Loop Intelligence: {topic['area']}\n")
        f.write(f"**Date**: {date_str} | **Run**: #{run_number} ({run_label})\n")
        f.write(f"**Category**: {topic['category'].upper()}\n")
        f.write(
            f"**Sources**: Reddit, Hacker News, GitHub, Gemini+Google Search"
        )
        if x_data:
            f.write(", Grok+X/Twitter")
        f.write(f"\n**Search focus**: {topic['search_query']}\n\n")
        f.write("---\n\n")

        # Analysis
        f.write("## Analysis (Gemini + Google Search grounding)\n\n")
        if analysis:
            f.write(analysis)
        else:
            f.write("*Gemini analysis failed. Review raw data below.*\n")

        f.write("\n\n---\n\n")

        # X/Twitter data
        if x_data:
            f.write("## X/Twitter Intelligence (Grok)\n\n")
            f.write(x_data)
            f.write("\n\n---\n\n")

        # Raw data
        f.write("## Raw Data Collected\n\n")
        f.write(raw_data_text)

        f.write("\n\n---\n\n")

        # Stats
        reddit_count = sum(
            len(v) for v in reddit_data.values()
        ) if reddit_data else 0
        f.write("## Collection Stats\n")
        f.write(f"- Reddit posts: {reddit_count}\n")
        f.write(f"- HN stories: {len(hn_data)}\n")
        f.write(f"- GitHub repos: {len(github_data)}\n")
        f.write(f"- Gemini web search: {'Yes' if analysis else 'Failed'}\n")
        f.write(f"- X/Twitter (Grok): {'Yes' if x_data else 'N/A'}\n")

    print(f"Saved: {filepath}")
    return filepath


def update_dashboard(run_number, topic, date_str, filepath, stats):
    """Update the learning dashboard."""
    dashboard_path = os.path.join(LEARNING_DIR, "DASHBOARD.md")

    entries = []
    if os.path.exists(dashboard_path):
        with open(dashboard_path, "r", encoding="utf-8") as f:
            content = f.read()
            for line in content.split("\n"):
                if line.startswith("| 2"):
                    entries.append(line)

    new_entry = (
        f"| {date_str} #{run_number} | {topic['area']} "
        f"[{topic['category']}] | "
        f"R:{stats['reddit']} HN:{stats['hn']} GH:{stats['github']} "
        f"X:{'Y' if stats.get('x') else 'N'} | "
        f"Pending | {os.path.basename(filepath)} |"
    )
    entries.insert(0, new_entry)
    entries = entries[:60]  # Keep 60 entries (15 days at 4x/day)

    with open(dashboard_path, "w", encoding="utf-8") as f:
        f.write("# Hey Loop Intelligence Dashboard\n\n")
        f.write("> 4x daily intelligence | Infrastructure + Revenue\n\n")
        f.write("| Date/Run | Topic [Category] | Sources | Status | File |\n")
        f.write("|----------|------------------|---------|--------|------|\n")
        for entry in entries:
            f.write(entry + "\n")
        f.write(f"\n\n*Last updated: {date_str} run #{run_number}*\n")

    print(f"Dashboard updated: {dashboard_path}")


# =============================================================================
# Telegram: Enhanced proposals with URLs + monetization ideas
# =============================================================================
def generate_proposals(api_key, run_number, topic, analysis, github_data,
                       x_data, stats):
    """Generate owner-facing Telegram report in Japanese with monetization."""
    url = f"{GEMINI_API_URL}?key={api_key}"

    run_label = RUN_LABELS.get(run_number, f"Run {run_number}")

    github_summary = ""
    for g in github_data:
        if g.get("tag"):
            github_summary += f"- {g['repo']}: {g['tag']}\n"

    x_summary = ""
    if x_data:
        x_summary = f"\n## X/Twitteræƒ…å ±:\n{x_data[:2000]}\n"

    prompt = f"""ã‚ãªãŸã¯Hey Loopãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®çµŒæ¸ˆã‚¢ãƒŠãƒªã‚¹ãƒˆå…¼ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚
ä»¥ä¸‹ã®ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ã€ã‚ªãƒ¼ãƒŠãƒ¼ï¼ˆéã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ï¼‰ã¸ã®å ±å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## ãƒ¬ãƒãƒ¼ãƒˆæƒ…å ±
- Run: #{run_number} ({run_label})
- ãƒˆãƒ”ãƒƒã‚¯: {topic['area']} [{topic['category']}]
- åé›†: Reddit {stats['reddit']}ä»¶, HN {stats['hn']}ä»¶, GitHub {stats['github']}ä»¶

## åˆ†æçµæœï¼ˆæŠœç²‹ï¼‰:
{(analysis or 'åˆ†æå¤±æ•—')[:3000]}

## ä¾å­˜é–¢ä¿‚ãƒªãƒªãƒ¼ã‚¹:
{github_summary}
{x_summary}

## å ±å‘Šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå¿…ãšã“ã®é †ç•ªã§æ›¸ãï¼‰:

[æ³¨ç›®ãƒ‹ãƒ¥ãƒ¼ã‚¹] (æœ€å¤§3ã¤ã€URLå¿…é ˆ)
1. ã‚¿ã‚¤ãƒˆãƒ«
   URL: è¨˜äº‹ã®URL
   è¦ç´„: 1è¡Œã§ä½•ãŒé‡è¦ã‹
   åç›ŠåŒ–: ã“ã®æƒ…å ±ã‚’ã©ã†ãŠé‡‘ã«å¤‰ãˆã‚‰ã‚Œã‚‹ã‹

[ã‚¤ãƒ³ãƒ•ãƒ©æ›´æ–°] (ã‚ã‚Œã°)
- ä¾å­˜é–¢ä¿‚ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è­¦å‘Š

[æ–°ç™ºè¦‹ã®æƒ…å ±æº] (æœ€å¤§2ã¤)
- æ–°ã—ãè¦‹ã¤ã‘ãŸäººç‰©/ãƒ–ãƒ­ã‚°/ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ + URL + ãªãœãƒ•ã‚©ãƒ­ãƒ¼ã™ã¹ãã‹

[ææ¡ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³] (æœ€å¤§3ã¤)
1. ä½•ã‚’ã™ã¹ãã‹ â†’ ãªãœ â†’ æ¨å®šåŠ¹æœ
   æ‰¿èªãªã‚‰ã€Œã‚„ã£ã¦ã€ã¨è¿”ä¿¡

## ãƒ«ãƒ¼ãƒ«:
1. å…¨ä½“3500æ–‡å­—ä»¥å†…
2. å°‚é–€ç”¨èªã¯ï¼ˆï¼‰ã§èª¬æ˜
3. URLã¯å¿…ãšå«ã‚ã‚‹ï¼ˆURLãŒãªã„æƒ…å ±ã¯çœç•¥ï¼‰
4. åç›Šã«é–¢ã™ã‚‹è©±ã¯æœ€å„ªå…ˆã§è¨˜è¼‰
5. ã€Œæƒ…å ±ã‚’è¦‹ã¦ã©ã†ç¨¼ãã‹ã€ã®è¦–ç‚¹ã‚’å¿…ãšå…¥ã‚Œã‚‹
6. ææ¡ˆãŒãªã„å ´åˆã¯ã€Œä»Šå›ã¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ä¸è¦ã€ã¨æ›¸ã
7. ã‚ªãƒ¼ãƒŠãƒ¼ãŒèª­ã‚“ã§5åˆ†ã§åˆ¤æ–­ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã«è½ã¨ã™"""

    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"maxOutputTokens": 8192, "temperature": 0.3},
    }

    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(
        url, data=data,
        headers={"Content-Type": "application/json"},
        method="POST",
    )

    try:
        with urllib.request.urlopen(req, timeout=60) as resp:
            result = json.loads(resp.read().decode("utf-8"))
            return result["candidates"][0]["content"]["parts"][0]["text"]
    except Exception as e:
        print(f"  Proposal generation error: {e}")
        return None


def send_telegram(token, chat_id, message):
    """Send message(s) to owner via Telegram. Splits if > 4000 chars."""
    # Strip markdown (Telegram strict parser causes 400 errors)
    clean = message.replace("**", "").replace("*", "").replace("_", "")

    # Split into chunks at line boundaries
    chunks = []
    current = ""
    for line in clean.split("\n"):
        if len(current) + len(line) + 1 > 4000:
            if current:
                chunks.append(current)
            current = line
        else:
            current += ("\n" if current else "") + line
    if current:
        chunks.append(current)

    success = True
    for i, chunk in enumerate(chunks):
        url = TELEGRAM_API_URL.format(token=token)
        payload = {"chat_id": chat_id, "text": chunk}
        data = json.dumps(payload).encode("utf-8")
        req = urllib.request.Request(
            url, data=data,
            headers={"Content-Type": "application/json"},
            method="POST",
        )
        try:
            with urllib.request.urlopen(req, timeout=15) as resp:
                result = json.loads(resp.read().decode("utf-8"))
                if result.get("ok"):
                    print(
                        f"  Telegram message {i + 1}/{len(chunks)} sent"
                    )
                else:
                    print(f"  Telegram error: {result}")
                    success = False
        except Exception as e:
            print(f"  Telegram send error: {e}")
            success = False
        if i < len(chunks) - 1:
            time.sleep(1)  # Avoid rate limiting between chunks

    return success


# =============================================================================
# Run Number Detection
# =============================================================================
def detect_run_number():
    """Auto-detect run number from current JST hour."""
    jst_hour = datetime.now(JST).hour
    return jst_hour // 6  # 0-5=0, 6-11=1, 12-17=2, 18-23=3


def parse_args():
    """Parse command-line arguments."""
    run_number = None
    force = False

    args = sys.argv[1:]
    i = 0
    while i < len(args):
        if args[i] == "--run" and i + 1 < len(args):
            run_number = int(args[i + 1])
            i += 2
        elif args[i] == "--force":
            force = True
            i += 1
        else:
            i += 1

    if run_number is None:
        run_number = detect_run_number()

    return run_number, force


# =============================================================================
# Main
# =============================================================================
def main():
    run_number, force = parse_args()
    date_str = datetime.now(JST).strftime("%Y-%m-%d")
    jst_time = datetime.now(JST).strftime("%H:%M")
    run_label = RUN_LABELS.get(run_number, f"Run {run_number}")

    print(f"=== Hey Loop Intelligence v3 ===")
    print(f"Date: {date_str} | Time: {jst_time} JST")
    print(f"Run: #{run_number} ({run_label})")
    print()

    # Get API keys
    api_key = get_api_key()
    if not api_key:
        print("ERROR: No GOOGLE_API_KEY found.")
        sys.exit(1)

    xai_key = get_xai_api_key()
    if xai_key:
        print("xAI API key: found (X/Twitter search enabled)")
    else:
        print("xAI API key: not found (X/Twitter search disabled)")

    # Pick today's deep-research topic
    day = datetime.now(JST).timetuple().tm_yday
    topic_index = (day * 4 + run_number) % len(DEEP_TOPICS)
    topic = DEEP_TOPICS[topic_index]
    print(f"Topic: {topic['area']} [{topic['category']}]")

    # Duplicate check
    if not force:
        area_slug = (
            topic["area"].lower().replace(" ", "-").replace("&", "and")
        )
        expected = os.path.join(
            LEARNING_DIR, f"{date_str}_run{run_number}_{area_slug}.md"
        )
        if os.path.exists(expected):
            print(f"Already ran. File: {expected}")
            print("Use --force to re-run.")
            return

    # Phase 1: Collect from all sources
    print("\n--- Phase 1: Collecting real-world data ---")

    print("\n[Reddit]")
    reddit_data = collect_reddit_data(run_number)

    print("\n[Hacker News]")
    hn_data = fetch_hn_top_stories(limit=50)

    print("\n[GitHub]")
    github_data = fetch_github_updates(run_number)

    # Phase 2: X/Twitter via Grok (morning run only to conserve $5 credit)
    x_data = None
    if xai_key and run_number == 1:
        all_accounts = get_all_watchlist_accounts()
        print(
            f"\n[X/Twitter via Grok â€” Watchlist ({len(all_accounts)} accounts: "
            f"base {sum(len(v) for v in X_WATCHLIST.values())} "
            f"+ dynamic {len(all_accounts) - sum(len(v) for v in X_WATCHLIST.values())})]"
        )
        watchlist_data = search_x_watchlist_via_grok(xai_key)
        if watchlist_data:
            x_data = "### X Watchlist\n" + watchlist_data

        print("\n[X/Twitter via Grok â€” General revenue search]")
        general_data = search_x_via_grok(xai_key, run_number)
        if general_data:
            x_data = (x_data + "\n\n" if x_data else "") + "### X General\n" + general_data

        # è‡ªå‹•ç™ºè¦‹: æ–°ã—ã„é«˜ã‚·ã‚°ãƒŠãƒ«ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ææ¡ˆã•ã›ã¦ä¿å­˜
        print("\n[X/Twitter via Grok â€” Auto-discover new accounts]")
        new_discoveries = discover_new_x_accounts_via_grok(xai_key, all_accounts)
        if new_discoveries:
            added_count = save_dynamic_watchlist(new_discoveries)
            if added_count > 0 and x_data:
                names = ", ".join(
                    f"@{d['username']}" for d in new_discoveries[:5]
                )
                x_data += f"\n\n### New Accounts Discovered\nAdded {added_count}: {names}"

    elif xai_key:
        print("\n[X/Twitter] Skipped (Grok runs on morning briefing only)")
    else:
        print("\n[X/Twitter] Skipped (no xAI API key)")

    # Phase 3: Format raw data for Gemini
    raw_data_text = format_raw_data(reddit_data, hn_data, github_data, x_data)

    # Phase 4: Gemini analysis with Google Search grounding
    print("\n--- Phase 2: Gemini analysis with web search ---")
    analysis = query_gemini_with_search(api_key, topic, raw_data_text)

    if analysis:
        print(f"  Analysis received: {len(analysis)} chars")
    else:
        print("  WARNING: Gemini analysis failed, saving raw data only")

    # Phase 5: Save report
    print("\n--- Phase 3: Saving report ---")
    stats = {
        "reddit": sum(
            len(v) for v in reddit_data.values()
        ) if reddit_data else 0,
        "hn": len(hn_data),
        "github": len(github_data),
        "x": bool(x_data),
    }
    filepath = save_report(
        run_number, topic, raw_data_text, analysis,
        reddit_data, hn_data, github_data, x_data, date_str,
    )
    update_dashboard(run_number, topic, date_str, filepath, stats)

    # Phase 6: Telegram report with URLs + monetization proposals
    print("\n--- Phase 4: Sending Telegram report ---")
    tg_token, tg_chat_id = get_telegram_config()
    if tg_token and tg_chat_id:
        proposals = generate_proposals(
            api_key, run_number, topic, analysis, github_data, x_data, stats,
        )
        if proposals:
            header = (
                f"[Hey Loop #{run_number}] {run_label}\n"
                f"{date_str} {jst_time} JST\n"
                f"Topic: {topic['area']} [{topic['category']}]\n"
                f"---\n\n"
            )
            message = header + proposals
            send_telegram(tg_token, tg_chat_id, message)
        else:
            print("  WARNING: Could not generate proposals")
    else:
        print("  WARNING: Telegram config not found")

    # Summary
    print(f"\n=== Done ===")
    print(f"Report: {filepath}")
    watchlist_note = " + X Watchlist(25)" if (x_data and "X Watchlist" in x_data) else ""
    print(
        f"Sources: Reddit({stats['reddit']}) + HN({stats['hn']}) + "
        f"GitHub({stats['github']}) + Gemini Search"
        + (f" + Grok/X{watchlist_note}" if x_data else "")
    )


if __name__ == "__main__":
    main()
